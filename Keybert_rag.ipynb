{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a78e93b3c7b64b63a6f66f17505ca46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26c2bb03d19f4702a5f6b6b85c8d122e",
              "IPY_MODEL_90e895cd1a2748d5bd8831dcde7f1737",
              "IPY_MODEL_225171e61b5e4efb8505b7bc6dd95499"
            ],
            "layout": "IPY_MODEL_067cf328e3b049259a630e0c20f587cf"
          }
        },
        "26c2bb03d19f4702a5f6b6b85c8d122e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feeb6eafeff645219e76a8d9e8dd45ad",
            "placeholder": "​",
            "style": "IPY_MODEL_2eee3f201c874848b0e2e03203a2d446",
            "value": "modules.json: 100%"
          }
        },
        "90e895cd1a2748d5bd8831dcde7f1737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1ea0ccf80bc419dbedaa1f1b35f3a78",
            "max": 387,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88591dc9d85543f3b99020874461ff7a",
            "value": 387
          }
        },
        "225171e61b5e4efb8505b7bc6dd95499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847c8e7040e8432782b223dd5c031dea",
            "placeholder": "​",
            "style": "IPY_MODEL_e9d9285dfade43549e82a480e84ad717",
            "value": " 387/387 [00:00&lt;00:00, 23.9kB/s]"
          }
        },
        "067cf328e3b049259a630e0c20f587cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feeb6eafeff645219e76a8d9e8dd45ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eee3f201c874848b0e2e03203a2d446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1ea0ccf80bc419dbedaa1f1b35f3a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88591dc9d85543f3b99020874461ff7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "847c8e7040e8432782b223dd5c031dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d9285dfade43549e82a480e84ad717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0d3559f9164b62bdefe4ffaa4bd05d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89b9b9cf7c79431c94d222dfbfbf1264",
              "IPY_MODEL_ac7609ff4d4d4edfa7b7559c5e6a9019",
              "IPY_MODEL_fb162fcda71c47339cb7c48109247d58"
            ],
            "layout": "IPY_MODEL_8f8f304ba5f44178a5c9cffd112e0f13"
          }
        },
        "89b9b9cf7c79431c94d222dfbfbf1264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c66a6656334893ab866321ac7f2008",
            "placeholder": "​",
            "style": "IPY_MODEL_fe7dda54d45f4e3db168e4314a7ff5c9",
            "value": "README.md: 100%"
          }
        },
        "ac7609ff4d4d4edfa7b7559c5e6a9019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165d9721a69947d38e761ba35dc53d6f",
            "max": 179345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f727c90d57d407b8b97e3ae7b317e44",
            "value": 179345
          }
        },
        "fb162fcda71c47339cb7c48109247d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dbfea0ff9548d5ace949e4939f395b",
            "placeholder": "​",
            "style": "IPY_MODEL_838dbfd276dc4a88b08d5c03699f75be",
            "value": " 179k/179k [00:00&lt;00:00, 3.25MB/s]"
          }
        },
        "8f8f304ba5f44178a5c9cffd112e0f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c66a6656334893ab866321ac7f2008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7dda54d45f4e3db168e4314a7ff5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "165d9721a69947d38e761ba35dc53d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f727c90d57d407b8b97e3ae7b317e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95dbfea0ff9548d5ace949e4939f395b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "838dbfd276dc4a88b08d5c03699f75be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86d228acdade469fa8fb50a953db0202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_805f94bd86d94d66ba76c58c0675c9b5",
              "IPY_MODEL_c83f5295d0124e9c899123077fcd9405",
              "IPY_MODEL_87d4365c5af84b24a39b93dac471ae5e"
            ],
            "layout": "IPY_MODEL_3832adb3587e491abee11d8c87df82df"
          }
        },
        "805f94bd86d94d66ba76c58c0675c9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b41ea1e840564d6aa58cfb763ee62458",
            "placeholder": "​",
            "style": "IPY_MODEL_7914c4b998504c30a38f6e0ff08139ce",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "c83f5295d0124e9c899123077fcd9405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c282351870b42e098191481fb8052e8",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ffb57764b9c4e52bc20ba241620d9fb",
            "value": 57
          }
        },
        "87d4365c5af84b24a39b93dac471ae5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7009f909a63d4415a23aa685c4af58e0",
            "placeholder": "​",
            "style": "IPY_MODEL_9f0041ec4cba4037827b0db784e8b69d",
            "value": " 57.0/57.0 [00:00&lt;00:00, 3.34kB/s]"
          }
        },
        "3832adb3587e491abee11d8c87df82df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b41ea1e840564d6aa58cfb763ee62458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7914c4b998504c30a38f6e0ff08139ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c282351870b42e098191481fb8052e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ffb57764b9c4e52bc20ba241620d9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7009f909a63d4415a23aa685c4af58e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0041ec4cba4037827b0db784e8b69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae5c0e34dbf4761a2952751eefcedda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fca83735d9245818ca0667ed4a8c2c3",
              "IPY_MODEL_0a769f8fd7c648a1bcc948771190bc74",
              "IPY_MODEL_258f1370587f43c38e52d31fede43aaa"
            ],
            "layout": "IPY_MODEL_bf0aac089d1140eea7eda5e791b0ebfc"
          }
        },
        "2fca83735d9245818ca0667ed4a8c2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e50b463505244dfb793298db2480c7b",
            "placeholder": "​",
            "style": "IPY_MODEL_d70229893cb7491c9b644f1e510caf2e",
            "value": "config.json: 100%"
          }
        },
        "0a769f8fd7c648a1bcc948771190bc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f76faeb4d4f444d29c9a41d5962634ab",
            "max": 694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d69cd22e9eee4463be67bed661df01e3",
            "value": 694
          }
        },
        "258f1370587f43c38e52d31fede43aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60892f9c58864821b8a7bb998a104330",
            "placeholder": "​",
            "style": "IPY_MODEL_7167e1daca9541318ba9d2da1455ab00",
            "value": " 694/694 [00:00&lt;00:00, 47.6kB/s]"
          }
        },
        "bf0aac089d1140eea7eda5e791b0ebfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e50b463505244dfb793298db2480c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70229893cb7491c9b644f1e510caf2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f76faeb4d4f444d29c9a41d5962634ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d69cd22e9eee4463be67bed661df01e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60892f9c58864821b8a7bb998a104330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7167e1daca9541318ba9d2da1455ab00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b05d336975c432e87c5ced4f1a33603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1caeb03cd7ea438cbbe469cbde3c047f",
              "IPY_MODEL_58123594b46445ceb8e34757bdf51053",
              "IPY_MODEL_f6464caf0c3f4c8fbc0e995cb82cfa03"
            ],
            "layout": "IPY_MODEL_82b732b5b6394f0d9ebdc08491297cbd"
          }
        },
        "1caeb03cd7ea438cbbe469cbde3c047f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaf80afeab6a4865ba1c1fb018b5898c",
            "placeholder": "​",
            "style": "IPY_MODEL_f5a7dee7405449929ddd1f7caf007207",
            "value": "model.safetensors: 100%"
          }
        },
        "58123594b46445ceb8e34757bdf51053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efed2fc7caa542daaca532da0711c200",
            "max": 1112201288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f71b814e55c45b39d784a9522b77ee0",
            "value": 1112201288
          }
        },
        "f6464caf0c3f4c8fbc0e995cb82cfa03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c154df78364c4e7e98b5368cc46a25c2",
            "placeholder": "​",
            "style": "IPY_MODEL_4c07c49975394d068f86200a59e2ed4c",
            "value": " 1.11G/1.11G [00:11&lt;00:00, 189MB/s]"
          }
        },
        "82b732b5b6394f0d9ebdc08491297cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf80afeab6a4865ba1c1fb018b5898c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a7dee7405449929ddd1f7caf007207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efed2fc7caa542daaca532da0711c200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f71b814e55c45b39d784a9522b77ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c154df78364c4e7e98b5368cc46a25c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c07c49975394d068f86200a59e2ed4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acb35a85a030413e83f74ae852dbc613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc23398a16a149b6bad45652216c7d43",
              "IPY_MODEL_cc1e64c6b1254a1a9b1d14d125c9b856",
              "IPY_MODEL_d90822b242ee45619a604ca533f6ca48"
            ],
            "layout": "IPY_MODEL_1dfe7dff95b7474aa4c8bcf1da90ae0d"
          }
        },
        "cc23398a16a149b6bad45652216c7d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8cff6c8555e453781d7ff5f6177b607",
            "placeholder": "​",
            "style": "IPY_MODEL_63ba6309b7804a7e8934acb951ba9d3a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "cc1e64c6b1254a1a9b1d14d125c9b856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569f332c2eb2424ba03f8276bb5ecf6e",
            "max": 418,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fafe70019654c4880bdd4429b018faf",
            "value": 418
          }
        },
        "d90822b242ee45619a604ca533f6ca48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588421d1072441a4928ede51f6e6d1ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f50c241a327943838c8a93836963bbbe",
            "value": " 418/418 [00:00&lt;00:00, 24.3kB/s]"
          }
        },
        "1dfe7dff95b7474aa4c8bcf1da90ae0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cff6c8555e453781d7ff5f6177b607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ba6309b7804a7e8934acb951ba9d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "569f332c2eb2424ba03f8276bb5ecf6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fafe70019654c4880bdd4429b018faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "588421d1072441a4928ede51f6e6d1ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50c241a327943838c8a93836963bbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fae035525a8e4f4badbc03e70dadb71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cc387d97c2d424f9f4139d895c91021",
              "IPY_MODEL_2ac446761b164e998157b25a1ad72781",
              "IPY_MODEL_6bf355c2e21a44ae9a3803df2284b298"
            ],
            "layout": "IPY_MODEL_8100014d324f4b3f9c4612420a6fffb6"
          }
        },
        "1cc387d97c2d424f9f4139d895c91021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f67959a5c240aaadc38c295d1fc211",
            "placeholder": "​",
            "style": "IPY_MODEL_35a2f465fe424d5898012384af844034",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "2ac446761b164e998157b25a1ad72781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033d9778f2a4410b9fc87f1a41ed8a0f",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a4d76c789064d04b4508acdc0a5bbcf",
            "value": 5069051
          }
        },
        "6bf355c2e21a44ae9a3803df2284b298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe3391801d2542aa9462136f616f05e9",
            "placeholder": "​",
            "style": "IPY_MODEL_248c11ed1d6c4380808c7e9eb48b1cfb",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 97.9MB/s]"
          }
        },
        "8100014d324f4b3f9c4612420a6fffb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f67959a5c240aaadc38c295d1fc211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a2f465fe424d5898012384af844034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "033d9778f2a4410b9fc87f1a41ed8a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4d76c789064d04b4508acdc0a5bbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe3391801d2542aa9462136f616f05e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248c11ed1d6c4380808c7e9eb48b1cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85846aa9a7d14f52ba6b818e336a109e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ad20001651f47efbc8ff0962f278aab",
              "IPY_MODEL_acfb3607ae654592ab5403dd5a7fc374",
              "IPY_MODEL_b4683a6959954731a0663b3c5df1b2b4"
            ],
            "layout": "IPY_MODEL_a11a4fb54d7143f7bf22edfcb6326229"
          }
        },
        "6ad20001651f47efbc8ff0962f278aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae317cdd75f473ab515376e24ece7ec",
            "placeholder": "​",
            "style": "IPY_MODEL_0b48f9ecf7f6473192f359697cdd92f1",
            "value": "tokenizer.json: 100%"
          }
        },
        "acfb3607ae654592ab5403dd5a7fc374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1781f26fcf9a41be846599916d45ff15",
            "max": 17082660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b125c851153a435bacc611c945e244fc",
            "value": 17082660
          }
        },
        "b4683a6959954731a0663b3c5df1b2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_672d6b386b574c4c9facf4a9e3d06d8e",
            "placeholder": "​",
            "style": "IPY_MODEL_4c7b972fa3b04d429ef38c28da766a78",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 175MB/s]"
          }
        },
        "a11a4fb54d7143f7bf22edfcb6326229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae317cdd75f473ab515376e24ece7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b48f9ecf7f6473192f359697cdd92f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1781f26fcf9a41be846599916d45ff15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b125c851153a435bacc611c945e244fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "672d6b386b574c4c9facf4a9e3d06d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c7b972fa3b04d429ef38c28da766a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ec38966df8944698ed1578f3d207d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12751c30d0b24e44825d1050a359a960",
              "IPY_MODEL_f0cc5fdf6f7a439e869dc426fdb3be85",
              "IPY_MODEL_55c439e9c18d47ee96efdf40d741dca4"
            ],
            "layout": "IPY_MODEL_2afca914252a4446b748a6a4cacc51a3"
          }
        },
        "12751c30d0b24e44825d1050a359a960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a41f81285c438da2e1871608ea6a15",
            "placeholder": "​",
            "style": "IPY_MODEL_7a5311f22fbe433db1e3bd7ac5d4b5a1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f0cc5fdf6f7a439e869dc426fdb3be85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eba7e1151314e46839a5811be721111",
            "max": 280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6f56ba420084a83bce06d4813a4278f",
            "value": 280
          }
        },
        "55c439e9c18d47ee96efdf40d741dca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_630ce26ebb2d4e36b6e1bce0a1d3d5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_e946f25153f64c339db5aa3cce827e00",
            "value": " 280/280 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "2afca914252a4446b748a6a4cacc51a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a41f81285c438da2e1871608ea6a15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5311f22fbe433db1e3bd7ac5d4b5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eba7e1151314e46839a5811be721111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f56ba420084a83bce06d4813a4278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "630ce26ebb2d4e36b6e1bce0a1d3d5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e946f25153f64c339db5aa3cce827e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0c83808795943c6a0e2e55c91997db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31e315e8666a475385a5c4ef46a84a12",
              "IPY_MODEL_1c9089a114eb409d99ad26eeaef4ba1f",
              "IPY_MODEL_7bbe4c6e25bf4d04964025e13d9be2da"
            ],
            "layout": "IPY_MODEL_6c3ea7ddddf1494d8a3b8c1a42dbd277"
          }
        },
        "31e315e8666a475385a5c4ef46a84a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8741e348de874eeab6ee90de3eceae24",
            "placeholder": "​",
            "style": "IPY_MODEL_c821c448ccd449459701dfb6120cb89a",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "1c9089a114eb409d99ad26eeaef4ba1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cad9341e8a4435eb9c73868997045e7",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2e418f9d80b4db99aa272b331bd3fb3",
            "value": 200
          }
        },
        "7bbe4c6e25bf4d04964025e13d9be2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02fc34e2ddee4fc698d586bbfa423069",
            "placeholder": "​",
            "style": "IPY_MODEL_1c1b94aee1b143878280905e0d51d111",
            "value": " 200/200 [00:00&lt;00:00, 5.47kB/s]"
          }
        },
        "6c3ea7ddddf1494d8a3b8c1a42dbd277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8741e348de874eeab6ee90de3eceae24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c821c448ccd449459701dfb6120cb89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cad9341e8a4435eb9c73868997045e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e418f9d80b4db99aa272b331bd3fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02fc34e2ddee4fc698d586bbfa423069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1b94aee1b143878280905e0d51d111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3b2271b41b54b0daeb0cba932d7dbf5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c15e861395e84f1bb45e5c473ba2a6b0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  98%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[32m82/84 \u001b[0m [ \u001b[33m0:00:10\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m8 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  98%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━</span> <span style=\"color: #008000; text-decoration-color: #008000\">82/84 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:10</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">8 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c15e861395e84f1bb45e5c473ba2a6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yadJEYQyOR5A",
        "outputId": "704cecf1-bcee-456d-95b3-36f901c86d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n",
            "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.117-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.117-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface, tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, httpx, dataclasses-json, langsmith, sentence_transformers, langchain-core, langchain-text-splitters, keybert, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 huggingface-0.0.1 jsonpatch-1.33 jsonpointer-3.0.0 keybert-0.8.5 langchain-0.2.16 langchain-core-0.2.38 langchain-text-splitters-0.2.4 langchain_community-0.2.16 langsmith-0.1.117 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 sentence_transformers-3.0.1 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain keybert transformers langchain_community sentence_transformers tiktoken huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.rich import trange,tqdm\n",
        "from rich import console\n",
        "from rich.panel import Panel\n",
        "from rich.progress import track\n",
        "from rich.markdown import Markdown\n",
        "from rich.text import Text\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import datetime\n",
        "from rich.console import Console\n"
      ],
      "metadata": {
        "id": "GIGh-tj9Ospg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console = Console(width=10)\n"
      ],
      "metadata": {
        "id": "EmpvD_zZPLcZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline,AutoTokenizer,AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import torch\n",
        "import keybert\n"
      ],
      "metadata": {
        "id": "5Odso0rOPQEs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = keybert.KeyBERT(model=\"intfloat/multilingual-e5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "a78e93b3c7b64b63a6f66f17505ca46a",
            "26c2bb03d19f4702a5f6b6b85c8d122e",
            "90e895cd1a2748d5bd8831dcde7f1737",
            "225171e61b5e4efb8505b7bc6dd95499",
            "067cf328e3b049259a630e0c20f587cf",
            "feeb6eafeff645219e76a8d9e8dd45ad",
            "2eee3f201c874848b0e2e03203a2d446",
            "f1ea0ccf80bc419dbedaa1f1b35f3a78",
            "88591dc9d85543f3b99020874461ff7a",
            "847c8e7040e8432782b223dd5c031dea",
            "e9d9285dfade43549e82a480e84ad717",
            "df0d3559f9164b62bdefe4ffaa4bd05d",
            "89b9b9cf7c79431c94d222dfbfbf1264",
            "ac7609ff4d4d4edfa7b7559c5e6a9019",
            "fb162fcda71c47339cb7c48109247d58",
            "8f8f304ba5f44178a5c9cffd112e0f13",
            "d4c66a6656334893ab866321ac7f2008",
            "fe7dda54d45f4e3db168e4314a7ff5c9",
            "165d9721a69947d38e761ba35dc53d6f",
            "4f727c90d57d407b8b97e3ae7b317e44",
            "95dbfea0ff9548d5ace949e4939f395b",
            "838dbfd276dc4a88b08d5c03699f75be",
            "86d228acdade469fa8fb50a953db0202",
            "805f94bd86d94d66ba76c58c0675c9b5",
            "c83f5295d0124e9c899123077fcd9405",
            "87d4365c5af84b24a39b93dac471ae5e",
            "3832adb3587e491abee11d8c87df82df",
            "b41ea1e840564d6aa58cfb763ee62458",
            "7914c4b998504c30a38f6e0ff08139ce",
            "1c282351870b42e098191481fb8052e8",
            "0ffb57764b9c4e52bc20ba241620d9fb",
            "7009f909a63d4415a23aa685c4af58e0",
            "9f0041ec4cba4037827b0db784e8b69d",
            "4ae5c0e34dbf4761a2952751eefcedda",
            "2fca83735d9245818ca0667ed4a8c2c3",
            "0a769f8fd7c648a1bcc948771190bc74",
            "258f1370587f43c38e52d31fede43aaa",
            "bf0aac089d1140eea7eda5e791b0ebfc",
            "4e50b463505244dfb793298db2480c7b",
            "d70229893cb7491c9b644f1e510caf2e",
            "f76faeb4d4f444d29c9a41d5962634ab",
            "d69cd22e9eee4463be67bed661df01e3",
            "60892f9c58864821b8a7bb998a104330",
            "7167e1daca9541318ba9d2da1455ab00",
            "8b05d336975c432e87c5ced4f1a33603",
            "1caeb03cd7ea438cbbe469cbde3c047f",
            "58123594b46445ceb8e34757bdf51053",
            "f6464caf0c3f4c8fbc0e995cb82cfa03",
            "82b732b5b6394f0d9ebdc08491297cbd",
            "eaf80afeab6a4865ba1c1fb018b5898c",
            "f5a7dee7405449929ddd1f7caf007207",
            "efed2fc7caa542daaca532da0711c200",
            "7f71b814e55c45b39d784a9522b77ee0",
            "c154df78364c4e7e98b5368cc46a25c2",
            "4c07c49975394d068f86200a59e2ed4c",
            "acb35a85a030413e83f74ae852dbc613",
            "cc23398a16a149b6bad45652216c7d43",
            "cc1e64c6b1254a1a9b1d14d125c9b856",
            "d90822b242ee45619a604ca533f6ca48",
            "1dfe7dff95b7474aa4c8bcf1da90ae0d",
            "c8cff6c8555e453781d7ff5f6177b607",
            "63ba6309b7804a7e8934acb951ba9d3a",
            "569f332c2eb2424ba03f8276bb5ecf6e",
            "6fafe70019654c4880bdd4429b018faf",
            "588421d1072441a4928ede51f6e6d1ae",
            "f50c241a327943838c8a93836963bbbe",
            "fae035525a8e4f4badbc03e70dadb71b",
            "1cc387d97c2d424f9f4139d895c91021",
            "2ac446761b164e998157b25a1ad72781",
            "6bf355c2e21a44ae9a3803df2284b298",
            "8100014d324f4b3f9c4612420a6fffb6",
            "64f67959a5c240aaadc38c295d1fc211",
            "35a2f465fe424d5898012384af844034",
            "033d9778f2a4410b9fc87f1a41ed8a0f",
            "8a4d76c789064d04b4508acdc0a5bbcf",
            "fe3391801d2542aa9462136f616f05e9",
            "248c11ed1d6c4380808c7e9eb48b1cfb",
            "85846aa9a7d14f52ba6b818e336a109e",
            "6ad20001651f47efbc8ff0962f278aab",
            "acfb3607ae654592ab5403dd5a7fc374",
            "b4683a6959954731a0663b3c5df1b2b4",
            "a11a4fb54d7143f7bf22edfcb6326229",
            "8ae317cdd75f473ab515376e24ece7ec",
            "0b48f9ecf7f6473192f359697cdd92f1",
            "1781f26fcf9a41be846599916d45ff15",
            "b125c851153a435bacc611c945e244fc",
            "672d6b386b574c4c9facf4a9e3d06d8e",
            "4c7b972fa3b04d429ef38c28da766a78",
            "9ec38966df8944698ed1578f3d207d25",
            "12751c30d0b24e44825d1050a359a960",
            "f0cc5fdf6f7a439e869dc426fdb3be85",
            "55c439e9c18d47ee96efdf40d741dca4",
            "2afca914252a4446b748a6a4cacc51a3",
            "60a41f81285c438da2e1871608ea6a15",
            "7a5311f22fbe433db1e3bd7ac5d4b5a1",
            "7eba7e1151314e46839a5811be721111",
            "b6f56ba420084a83bce06d4813a4278f",
            "630ce26ebb2d4e36b6e1bce0a1d3d5d2",
            "e946f25153f64c339db5aa3cce827e00",
            "f0c83808795943c6a0e2e55c91997db8",
            "31e315e8666a475385a5c4ef46a84a12",
            "1c9089a114eb409d99ad26eeaef4ba1f",
            "7bbe4c6e25bf4d04964025e13d9be2da",
            "6c3ea7ddddf1494d8a3b8c1a42dbd277",
            "8741e348de874eeab6ee90de3eceae24",
            "c821c448ccd449459701dfb6120cb89a",
            "4cad9341e8a4435eb9c73868997045e7",
            "b2e418f9d80b4db99aa272b331bd3fb3",
            "02fc34e2ddee4fc698d586bbfa423069",
            "1c1b94aee1b143878280905e0d51d111"
          ]
        },
        "id": "2RDx4I0RPai8",
        "outputId": "0d6f0522-c9d6-4264-9214-36edb6cf2505"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a78e93b3c7b64b63a6f66f17505ca46a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/179k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df0d3559f9164b62bdefe4ffaa4bd05d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86d228acdade469fa8fb50a953db0202"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ae5c0e34dbf4761a2952751eefcedda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b05d336975c432e87c5ced4f1a33603"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acb35a85a030413e83f74ae852dbc613"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fae035525a8e4f4badbc03e70dadb71b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85846aa9a7d14f52ba6b818e336a109e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ec38966df8944698ed1578f3d207d25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0c83808795943c6a0e2e55c91997db8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logfile = 'keybert-log.txt'"
      ],
      "metadata": {
        "id": "Yc_vG-rIPycZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writehistory(text):\n",
        "  with open(logfile,'a') as f:\n",
        "    f.write(text)\n",
        "    f.write('\\n')\n",
        "\n",
        "  f.close()\n",
        ""
      ],
      "metadata": {
        "id": "IC06eu8gP8tR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keys(text,ngram,dvsity):\n",
        "  import random,datetime\n",
        "  a = kw_model.extract_keywords(text,keyphrase_ngram_range=(1,ngram),stop_words='english',use_mmr=True,diversity=dvsity,highlight = True)\n",
        "  tags = []\n",
        "  for kw in a:\n",
        "    tags.append(kw[0])\n",
        "\n",
        "  timestamped = datetime.datetime.now()\n",
        "  logging_text = f\"Logged on {str(timestamped)}\\n Metadata {str(tags)} \\n settings : keyphrase ngram : {str(ngram)},Divresity:{str(dvsity)}\"\n",
        "  writehistory(logging_text)\n",
        "  return tags\n",
        "\n"
      ],
      "metadata": {
        "id": "xmV_FAxnP8wM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Title: MagicCoder :Source Code is All you need\n",
        "published on paperwithcode and arxiv\n",
        "Author : Yuxiang Wei 1 Zhe Wang 2 † Jiawei Liu 1 Yifeng Ding 1 Lingming Zhang 1\n",
        "\n",
        "URL :https://arxiv.org/pdf/2312.02120\n",
        "We introduce Magicoder, a series of fully opensource (code, weights, and data) Large Language\n",
        "Models (LLMs) for code that significantly closes\n",
        "the gap with top code models while having no\n",
        "more than 7B parameters. Magicoder models are\n",
        "trained on 75K synthetic instruction data using\n",
        "OSS-INSTRUCT, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main\n",
        "motivation is to mitigate the inherent bias of the\n",
        "synthetic data generated by LLMs through the\n",
        "wealth of open-source references for the production of more realistic and controllable data. The\n",
        "orthogonality of OSS-INSTRUCT and other data\n",
        "generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both\n",
        "Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or\n",
        "even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on\n",
        "CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).\n",
        "Overall, OSS-INSTRUCT opens a new direction\n",
        "for crafting diverse synthetic instruction data for\n",
        "code using abundant open-source references.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ST1UB4oYP8zT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_keywords = extract_keys(text,1,0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "8fW0VrMmP82t",
        "outputId": "3d74eb89-bde2-48f9-c905-b7d65a9f11ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Title MagicCoder Source \u001b[30;48;2;255;255;0mCode\u001b[0m is All you need published on paperwithcode and arxiv Author Yuxiang Wei Zhe Wang \n",
              "Jiawei Liu Yifeng Ding Lingming Zhang URL https arxiv org pdf 2312 02120 We introduce \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series of fully \n",
              "opensource \u001b[30;48;2;255;255;0mcode\u001b[0m weights and data Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for \u001b[30;48;2;255;255;0mcode\u001b[0m that significantly closes the gap with top \n",
              "\u001b[30;48;2;255;255;0mcode\u001b[0m models while having no more than 7B parameters \u001b[30;48;2;255;255;0mMagicoder\u001b[0m models are trained on \u001b[30;48;2;255;255;0m75K\u001b[0m synthetic instruction data \n",
              "using OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m novel approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source \u001b[30;48;2;255;255;0mcode\u001b[0m snippets to generate diverse \n",
              "instruction data for \u001b[30;48;2;255;255;0mcode\u001b[0m Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
              "\u001b[30;48;2;255;255;0mLLMs\u001b[0m through the wealth of open source references for the production of more realistic and controllable data The \n",
              "orthogonality of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and other data generation methods like Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m further enables us to build an \n",
              "enhanced MagicoderS Both \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS substantially outperform state of the art \u001b[30;48;2;255;255;0mcode\u001b[0m models with \n",
              "similar or even larger sizes on wide range of coding benchmarks Notably MagicoderS CL 7B based on CODELLAMA even \n",
              "surpasses the prominent ChatGPT on HumanEval 66 vs 65 in pass Overall OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m opens new direction for crafting\n",
              "diverse synthetic instruction data for \u001b[30;48;2;255;255;0mcode\u001b[0m using abundant open source references\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Title MagicCoder Source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> is All you need published on paperwithcode and arxiv Author Yuxiang Wei Zhe Wang \n",
              "Jiawei Liu Yifeng Ding Lingming Zhang URL https arxiv org pdf 2312 02120 We introduce <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series of fully \n",
              "opensource <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> weights and data Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> that significantly closes the gap with top \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> models while having no more than 7B parameters <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> models are trained on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">75K</span> synthetic instruction data \n",
              "using OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> novel approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> snippets to generate diverse \n",
              "instruction data for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> through the wealth of open source references for the production of more realistic and controllable data The \n",
              "orthogonality of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and other data generation methods like Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> further enables us to build an \n",
              "enhanced MagicoderS Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS substantially outperform state of the art <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> models with \n",
              "similar or even larger sizes on wide range of coding benchmarks Notably MagicoderS CL 7B based on CODELLAMA even \n",
              "surpasses the prominent ChatGPT on HumanEval 66 vs 65 in pass Overall OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> opens new direction for crafting\n",
              "diverse synthetic instruction data for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> using abundant open source references\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/magicoder.txt.txt\""
      ],
      "metadata": {
        "id": "yzj2pLUQVRzL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename,encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "y6mL1KnqWI-6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "YzOs31OHWOw1",
        "outputId": "3cd7f92d-f731-445b-d8b2-c59e57cfde1f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We introduce Magicoder, a series of fully open\\x02source (code, weights, and data) Large Language\\nModels (LLMs) for code that significantly closes\\nthe gap with top code models while having no\\nmore than 7B parameters. Magicoder models are\\ntrained on 75K synthetic instruction data using\\nOSS-INSTRUCT, a novel approach to enlighten\\x02ing LLMs with open-source code snippets to gen\\x02erate diverse instruction data for code. Our main\\nmotivation is to mitigate the inherent bias of the\\nsynthetic data generated by LLMs through the\\nwealth of open-source references for the produc\\x02tion of more realistic and controllable data. The\\northogonality of OSS-INSTRUCT and other data\\ngeneration methods like Evol-Instruct further en\\x02ables us to build an enhanced MagicoderS. Both\\nMagicoder and MagicoderS substantially outper\\x02form state-of-the-art code models with similar or\\neven larger sizes on a wide range of coding bench\\x02marks. Notably, MagicoderS-CL-7B based on\\nCODELLAMA even surpasses the prominent Chat\\x02GPT on HumanEval+ (66.5 vs. 65.9 in pass@1).\\nOverall, OSS-INSTRUCT opens a new direction\\nfor crafting diverse synthetic instruction data for\\ncode using abundant open-source references.\\n1. Introduction\\nCode generation, also known as program synthesis (Gul\\x02wani et al., 2017), is a long-standing challenge in com\\x02puter science. In the past few decades, a large body of\\nresearch has been studying symbolic approaches, such as\\nabstraction-based synthesis (Wang et al., 2017; Feng et al.,\\n2018) for general-purpose synthesis problems and program-\\n†The work was done during a remote summer internship at the\\nUniversity of Illinois. 1University of Illinois at Urbana-Champaign,\\nUSA 2Tsinghua University, China. Correspondence to: Yuxiang\\nWei <ywei40@illinois.edu>.\\nProceedings of the 41 st International Conference on Machine\\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\\nthe author(s).\\nming by examples (Cambronero et al., 2023; Liu et al.,\\n2023a) for domain-specific tasks. Until recently, Large Lan\\x02guage Models (LLMs) trained on code (Austin et al., 2021;\\nChen et al., 2021) has shown outstanding breakthroughs in\\ngenerating code that accurately satisfies user intents, and\\nthey are widely deployed to assist real-world software de\\x02velopment (Microsoft, 2023b; Services, 2023).\\nInitially, closed-source models such as GPT-3.5 Turbo (Ope\\x02nAI, 2022) (i.e., ChatGPT) and GPT-4 (OpenAI, 2023) mas\\x02sively dominated various coding benchmarks and leader\\x02boards (Chen et al., 2021; Austin et al., 2021; Liu et al.,\\n2023b; Lai et al., 2022; Xia & Zhang, 2023). To further push\\nthe boundaries of code generation with open source LLMs,\\nSELF-INSTRUCT (Wang et al., 2023a) is adopted to boot\\x02strap the instruction-following ability of LLMs. In the realm\\nof code, practitioners commonly devise synthetic coding\\ninstructions using a stronger teacher model (e.g., ChatGPT\\nand GPT-4) and then finetune a weaker student model (e.g.,\\nCODELLAMA (Roziere et al. ` , 2023)) with the generated data\\nto distill the knowledge from the teacher (Taori et al., 2023;\\nChaudhary, 2023). For example, Code Alpaca (Chaudhary,\\n2023) consists of 20K automatically generated code instruc\\x02tions by applying SELF-INSTRUCT on ChatGPT using 21\\nseed tasks. To further enhance the coding abilities of LLMs,\\nLuo et al. (2023b) proposes Code Evol-Instruct that employs\\nvarious heuristics to increase the complexity of seed code\\ninstructions (Code Alpaca in this case), achieving state-of\\x02the-art (SOTA) results among open-source models.\\nWhile these data generation methods can effectively im\\x02prove the instruction-following capability of an LLM, they\\nrely on a narrow range of predefined tasks or heuristics\\nunder the hood. For example, on the one hand, Code Al\\x02paca that adopts SELF-INSTRUCT only relies on 21 seed\\ntasks to generate new code instructions using an identical\\nprompt template. On the other hand, Code Evol-Instruct\\ntakes Code Alpaca as seeds and merely depends on 5 heuris\\x02tics to evolve the dataset. As partly suggested by Yu et al.\\n(2023) and Wang et al. (2023a), such approaches may sig\\x02nificantly inherit the system bias inherent in the LLMs as\\nwell as the predefined tasks.\\nTherefore, in this paper, we propose OSS-INSTRUCT to\\nmitigate the inherent bias of LLMs and to unleash their\\npotential to craft diverse and creative code instructions via\\n1\\narXiv:2312.02120v2 [cs.CL] 7 Jun 2024\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nYou are working on a natural language processing (NLP) \\nproject and need to create a program to preprocess and \\nclassify movie reviews...\\n...\\nYour program should be able to preprocess new movie \\nreviews, train the model, and classify new reviews accurately.\\nQ Generated problem (details omitted)\\nDPosNeg.py\\nDLog.cpp\\nDGrantInfo.ts\\nDProgram.cs\\nDStrength.swift\\nD…\\nOpen-source codebase\\n learn_model(\\n tf_idfSVM, tf_idfNB, target)\\ndef get_clean_review(raw_review):\\n letters_only = re.sub(\\n \"[^a-zA-Z]\", \" \", raw_review)\\nDSeed code snippet\\nOSS-INSTRUCT\\nPlease gain inspiration from the \\ncode snippet to create a high\\x02quality programming problem…\\ns Prompt (details omitted)\\nfrom sklearn.feature_extraction.text import TfidfVectorizer ...\\ndef get_clean_review(raw_review): ...\\ndef train_model(tf_idfSVM, tf_idfNB, reviews, labels): ...\\ndef classify_review(clean_review, tf_idfSVM, tf_idfNB): ... ...\\ntrain_model(tf_idfSVM, tf_idfNB, reviews, labels)\\ncleaned_review = get_clean_review(...)...\\nL Generated solution (details omitted)\\nsLanguage\\nModel\\nFigure 1: Overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval (+)\\ndirect learning from the open source. As shown in Figure 1,\\nOSS-INSTRUCT leverages a powerful LLM to automati\\x02cally generate new coding problems by drawing inspira\\x02tion from any random code snippets collected from the\\nopen source. In this example, the LLM gets inspired by\\ntwo incomplete code fragments from different functions\\nand manages to relate them and craft a realistic machine\\nlearning problem. Thanks to the “infinite” real-world open\\x02source code, OSS-INSTRUCT can directly produce diverse,\\nrealistic, and controllable code instructions by providing\\ndistinct seed code snippets. In the end, we generate 75K\\nsynthetic data to finetune CODELLAMA-PYTHON-7B, re\\x02sulting in Magicoder-CL. While being simple and effective,\\nOSS-INSTRUCT is orthogonal to existing data generation\\nmethods, and they can be combined to further boost the\\nmodels’ coding capabilities. Therefore, we continually fine\\x02tune Magicoder-CL on an open-source Evol-Instruct dataset\\nwith 110K entries, producing MagicoderS-CL.\\nWe evaluate Magicoder and MagicoderS on a wide range\\nof coding tasks, including HumanEval (Chen et al., 2021)\\nand MBPP (Austin et al., 2021) for Python text-to-code gen\\x02eration, MultiPL-E (Cassano et al., 2022) for multilingual\\ncode completion, and DS-1000 (Lai et al., 2022) for solving\\ndata science problems. We further adopt EvalPlus (Liu et al.,\\n2023b), which includes the augmented HumanEval+ and\\nMBPP+ datasets for more rigorous model evaluation. Both\\nMagicoder-CL and MagicoderS-CL substantially boost the\\nbase CODELLAMA-PYTHON-7B. Additionally, Magicoder\\x02CL even outperforms WizardCoder-CL-7B, WizardCoder\\x02SC-15B, and all studied SOTA LLMs with less than or equal\\nto 16B parameters on all the benchmarks we tested. Also,\\nthe pass@1 result of the enhanced MagicoderS-CL is on\\npar with ChatGPT on HumanEval (70.7 vs. 72.6) and sur\\x02passes it on the more rigorous HumanEval+ (66.5 vs. 65.9),\\nindicating that MagicoderS-CL can generate more robust\\ncode. It also achieves SOTA results among all code models\\nat the same scale.\\nAdditionally, we notice a very recent advancement in the\\ndevelopment of the DeepSeek-Coder series (Guo et al.,\\n2024) which has shown exceptional coding performance.\\nHowever, due to the limited technical details disclosed,\\nwe only briefly discuss them in §3.4. Despite this, we\\napplied OSS-INSTRUCT on DeepSeek-Coder-Base 6.7B,\\nresulting in the creation of Magicoder-DS and MagicoderS\\x02DS. In addition to the consistent findings on the previous\\nresults with CODELLAMA-PYTHON-7B as the base model,\\nMagicoder-DS and MagicoderS-DS benefit from the more\\npowerful DeepSeek-Coder-Base-6.7B. This advantage is\\ndemonstrated by MagicoderS-DS, which achieves a remark\\x02able 76.8 pass@1 on HumanEval. MagicoderS-DS also out\\x02performs DeepSeek-Coder-Instruct-6.7B on HumanEval (+)\\nand MBPP (+) with 8× less finetuning tokens.\\nTo justify the design of OSS-INSTRUCT, i.e., generating\\n2\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\ninstruction-tuning data from open-source references rather\\nthan using the references directly, we demonstrate that fine\\x02tuning the base models with semantically relevant comment\\x02function pairs extracted from open-source projects even\\nnegatively impacts the model performance (§4.2).\\nIn general, we make the following contributions:\\n• We introduce OSS-INSTRUCT, a pioneering approach to\\nenlightening LLMs with open-source code snippets to\\ngenerate more diverse, realistic, and controllable coding\\ninstruction data, which can be leveraged to substantially\\nboost the performance of various LLMs via instruction\\ntuning. It opens a new dimension for creating low-bias\\nand diverse instruction-tuning data from the abundance of\\nopen-source references.\\n• We build the Magicoder series trained with OSS\\x02INSTRUCT and MagicoderS series trained on a combi\\x02nation of OSS-INSTRUCT and Evol-Instruct. Our eval\\x02uation across 6 benchmarks shows that all Magicoders\\nsignificantly improve the base LLMs. Notably, both\\nMagicoderS-CL and MagicoderS-DS outperform Chat\\x02GPT on HumanEval+ with only 7B parameters.\\n• We fully open source the model weights, training data, and\\nsource code at https://github.com/ise-uiuc/\\nmagicoder to facilitate future research.\\n2. OSS-INSTRUCT: Instruction Tuning from\\nOpen Source\\nIn this section, we elaborate on our OSS-INSTRUCT ap\\x02proach. From a high level, as shown in Figure 1, OSS\\x02INSTRUCT works by prompting an LLM (e.g., ChatGPT)\\nto generate a coding problem and its solution according to\\nsome seed code snippet collected from the wild (e.g., from\\nGitHub). The seed snippet offers controllability of the gen\\x02eration and encourages the LLM to create diverse coding\\nproblems that can reflect real-world programming scenarios.\\n2.1. Generating Coding Problems\\nOSS-INSTRUCT is powered by seed code snippets that can\\nbe easily collected from open source. In this work, we\\ndirectly adopt starcoderdata as our seed corpus, a fil\\x02tered version of The Stack (Kocetkov et al., 2022) dataset\\nthat StarCoder is trained on, containing permissively li\\x02censed source code documents in various programming lan\\x02guages. We chose starcoderdata because it is widely\\nadopted, includes massive high-quality code snippets, and\\nis even post-processed for data decontamination (Li et al.,\\n2023; Allal et al., 2023). For each code document from\\nthe corpus, we randomly extract 1–15 consecutive lines\\nas the seed snippet for the model to gain inspiration from\\nand produce coding problems. In total, we collected 80K\\ninitial seed snippets from 80K code documents, 40K from\\nPython, and 5K from each of C++, Java, TypeScript, Shell,\\nC#, Rust, PHP, and Swift respectively. Then, each collected\\nseed code snippet is applied to the prompt template shown\\nin Appendix A.1, which a teacher model takes as input and\\noutputs both a coding problem and its solution.\\n2.2. Data Cleaning and Decontamination\\nWe perform data cleaning by excluding samples that are\\nidentical or share the same seed code snippet. While there\\nexist other sorts of noisiness (e.g., the solution is incom\\x02plete) in the generated data, inspired by Honovich et al.\\n(2023), they are not removed as we believe they still con\\x02tain valuable information for LLMs to learn. More experi\\x02mental details can be found in Appendix C.3. Finally, we\\napply the same logic as StarCoder Li et al. (2023) to decon\\x02taminate our training data by removing coding problems\\nthat contain docstrings or solutions from HumanEval (Chen\\net al., 2021) and MBPP (Austin et al., 2021), docstrings\\nfrom APPS (Hendrycks et al., 2021), prompts from DS\\x021000 (Lai et al., 2022), or questions from GSM8K (Cobbe\\net al., 2021). As part of our analysis, the decontamination\\nprocedure only filters out 9 additional samples. Since the\\nseed corpus starcoderdata has already gone through\\nrigorous data decontamination, this observation suggests\\nthat OSS-INSTRUCT is unlikely to introduce additional data\\nleakage beyond the seeds. The eventual OSS-INSTRUCT\\ndataset contains about 75K entries. An overview of the\\ndataset statistics can be found in Appendix A.3.\\n2.3. Qualitative Examples of OSS-INSTRUCT\\nFigure 2 shows some qualitative examples of how OSS\\x02INSTRUCT can help LLM get inspiration from a seed code\\nsnippet to create new coding problems and solutions. For\\nexample, the shell script example shows how an LLM crafts\\na Python coding problem with just one line of shell script.\\nThe library imports example demonstrates how an LLM\\ncan create a realistic machine learning problem using just\\na few import statements. Meanwhile, the class signature\\ninstance illustrates the ability of LLM to draw inspiration\\nfrom an incomplete class definition featuring annotations\\nlike SpringBootApplication and keywords such as\\nbank. From this, the LLM generates a problem that re\\x02quires implementing a complete banking system based on\\nSpring Boot. Overall, OSS-INSTRUCT can inspire an LLM\\nwith distinct code structures and semantics to create diverse\\ncoding tasks, including algorithmic challenges, realistic\\nissues, single-function code generation, library-based pro\\x02gram completion, whole-program development, and even\\nwhole-application construction.\\n3\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD Seed: shell script\\npython3 makeErrorFile.py data/test_dataset_14 14\\nQ Problem\\nCreate a Python program that generates an error file\\nbased on a given dataset...\\nL Code\\ndef generate_error_file(dataset_file, ...):\\n error_lines = []\\n with open(dataset_file, \\'r\\') as file:\\n for line in file:\\n ...\\n with open(error_file_name, \\'w\\') as error_file:\\n for error_line in error_lines:\\n error_file.write(error_line + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n if len(sys.argv) != 3:\\n print(\"Usage: ...\")\\n else:\\n dataset_file = sys.argv[1]\\n dataset_number = sys.argv[2]\\n generate_error_file(...)\\nD Seed: library imports\\nimport numpy as np\\nimport gym_electric_motor as gem\\nimport matplotlib.pyplot as plt\\nQ Problem\\nCreate a reinforcement learning agent to control an \\nelectric motor using the OpenAI Gym environment...\\nL Code\\nimport numpy as np\\nimport gym_electric_motor as gem\\n...\\nenv = gem.make(\"DcSeriesCont-v1\")\\nclass DQNAgent:\\n def __init__(self, state_dim, action_dim): ...\\n def build_model(self): ...\\n def act(self, state): ...\\n def train(self, state, action, reward, ...): ...\\n...\\nfor episode in range(episodes):\\n state = env.reset()\\n state = np.reshape(state, [1, state_dim])\\n ...\\nD Seed: class signature\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {\\n public static void main(String[] args) {\\nQ Problem\\nCreate a simple Java Spring Boot application\\nfor a banking system...\\nL Code\\nimport org.axonframework.commandhandling...\\nimport org.axonframework.config...\\n...\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {...}\\npublic class BankAccount {...}\\npublic class CreateAccountCommand {...}\\npublic class DepositFundsCommand {...}\\npublic class WithdrawFundsCommand {...}\\npublic class AccountCreatedEvent {...}\\npublic class FundsDepositedEvent {...}\\npublic class FundsWithdrawnEvent {...}\\nFigure 2: Examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed\\nproblem requirements, implementations, and explanations are omitted for brevity. More examples can be found in\\nAppendix A.2.\\n0.0 0.1 0.2 0.3 0.4 0.5\\nCosine Similarity Score\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\n0.12\\n0.14\\nPercentage\\nSelf-Instruct; Avg Score: 0.169\\nEvol-Instruct; Avg Score: 0.131\\nOSS-Instruct; Avg Score: 0.105\\nFigure 3: Cosine similarities between HumanEval and syn\\x02thetic data generated by different methods.\\nSimilarity with HumanEval To study whether our data\\ngeneration process produces more HumanEval-like prob\\x02lems or solutions that contribute to high performance, we\\npair each sample from our 75K dataset with each of the\\n164 HumanEval (Chen et al., 2021) samples and compute\\ntheir cosine similarity using TF-IDF (SPARCK JONES,\\n1972) embeddings. We then associate each OSS-INSTRUCT\\nsample with a HumanEval sample with the highest simi\\x02larity score. We also compare our dataset against Code\\nAlpaca, a 20K dataset applying SELF-INSTRUCT to code,\\nand evol-codealpaca-v1 (theblackcat102, 2023), an\\nopen-source reproduction of Evol-Instruct containing 110K\\ncoding instructions. We resort to the open-source implemen\\x02tation because the official Code Evol-Instruct (Luo et al.,\\n2023b) dataset is not released. We decontaminate all the\\ndatasets beforehand using the same way discussed in §2.2.\\nFigure 3 shows that OSS-INSTRUCT exhibits the lowest\\naverage similarity among all the studied data generation\\ntechniques while SELF-INSTRUCT shows the highest aver\\x02age similarity. This result indicates that the improvements\\nfrom OSS-INSTRUCT are not merely due to including data\\nfrom the same distribution.\\n3. Evaluation\\nWe choose CODELLAMA-PYTHON-7B and DeepSeek\\x02Coder-Base 6.7B as the base LLMs. To derive Magicoder\\nseries, we first finetune them on 75K synthetic data\\ngenerated through OSS-INSTRUCT. We then obtain\\nMagicoderS by continuing finetuning Magicoder with the\\nevol-codealpaca-v1 dataset, an open-source Evol\\x02Instruct implementation containing about 110K samples.\\nMore implementation details and additional evaluation re\\x02sults are listed in Appendices B and C. We also present\\ninteresting use cases that reflect the effectiveness of instruc\\x02tion tuning in Appendix D and demonstrate Magicoder’s\\ncapability to generate complex programs in Appendix E.\\n3.1. Python Text-to-Code Generation\\nHumanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n2021) are two of the most widely used benchmarks for code\\ngeneration. Each task in these benchmarks includes a task\\ndescription (e.g., docstring) as the prompt, where LLMs\\ngenerate corresponding code whose correctness is checked\\nby a handful of test cases. Because tests in these benchmarks\\ncan be insufficient, for more rigorous evaluation, we use\\nHumanEval+ and MBPP+, both powered by the EvalPlus\\nframework (Liu et al., 2023b) to obtain 80×/35× more tests.\\nFollowing prior work (Liu et al., 2023b; Chen et al., 2023),\\nfor each task and LLM we use greedy decoding to generate\\n4\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 1: Pass@1 (%) results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding. The\\nabbreviations “CL” and “SC” refer to the base models CODELLAMA-PYTHON and StarCoder, respectively. We report the\\nresults consistently from the EvalPlus (Liu et al., 2023b) Leaderboard.\\nModel Release Date Size\\nBenchmark Open-Source\\nHumanEval (+) MBPP (+) Weight Data\\nGPT-3.5 Turbo Nov 2023 - 72.6 (65.9) 81.7 (69.4) # #\\nGPT-4 Turbo Nov 2023 - 85.4 (81.7) 83.0 (70.7) # #\\nCODELLAMA-PYTHON Aug 2023 34B 51.8 (42.7) 67.2 (52.9) #\\nWizardCoder-CL Sep 2023 34B 73.2 (64.6) 73.2 (59.9) #\\nCodeT5+ May 2023 16B 31.7 (26.2) 54.6 (44.4) \\nCodeGen-Mono Mar 2022 16B 32.9 (27.4) 52.6 (43.6) \\nStarCoder May 2023 15B 34.1 (29.3) 55.1 (46.1) \\nCODELLAMA-PYTHON Aug 2023 13B 42.7 (36.6) 61.2 (50.9) #\\nWizardCoder-SC Sep 2023 15B 51.9 (45.1) 61.9 (50.6) #\\nStarCoder May 2023 7B 24.4 (20.7) 33.1 (28.8) \\nMistral Oct 2023 7B 28.7 (23.2) 50.1 (40.9) #\\nCodeT5+ May 2023 6B 29.3 (23.8) 51.9 (40.9) \\nCodeGen-Mono Mar 2022 6B 29.3 (25.6) 49.9 (42.1) \\nCODELLAMA-PYTHON Aug 2023 7B 37.8 (34.1) 57.6 (45.4) #\\nWizardCoder-CL Sep 2023 7B 48.2 (40.9) 56.6 (47.1) #\\nMagicoder-CL Dec 2023 7B 60.4 (55.5) 64.2 (52.6) \\nMagicoderS-CL Dec 2023 7B 70.7 (66.5) 68.4 (56.6) \\none sample and focus on comparing the pass@1 metric.\\nWe consider a wide range of baseline models, including\\nCODELLAMA-PYTHON (Roziere et al. ` , 2023), Wizard\\x02Coder (Luo et al., 2023b), GPT-3.5 Turbo (OpenAI, 2022),\\nGPT-4 Turbo (OpenAI, 2023), StarCoder (Li et al., 2023),\\nCodeT5+ (Wang et al., 2023b), CodeGen-Mono (Nijkamp\\net al., 2023), and Mistral (Jiang et al., 2023a). All the re\\x02sults are consistently reported from the EvalPlus (Liu et al.,\\n2023b) leaderboard (EvalPlus hash: 1895d2f).\\nTable 1 shows the pass@1 results of different LLMs on\\nthese benchmarks. From the results, we can first observe\\nthat Magicoder-CL has a clear improvement over the base\\nCODELLAMA-PYTHON-7B, and outperforms all studied\\nopen-source models except CODELLAMA-PYTHON-34B\\nand WizardCoder-CL-34B. Notably, Magicoder-CL sur\\x02passes WizardCoder-SC-15B and has a substantial improve\\x02ment on HumanEval and HumanEval+ over CODELLAMA\\x02PYTHON-34B. MagicoderS-CL demonstrates further im\\x02provements by being trained with the orthogonal Evol\\x02Instruct method. MagicoderS-CL outperforms ChatGPT\\nand all other open-source models on HumanEval+. More\\x02over, although it scores slightly lower than WizardCoder\\x02CL-34B and ChatGPT on HumanEval, it surpasses both of\\nthem on the more rigorous HumanEval+ dataset, indicating\\nthat MagicoderS-CL may produce more robust code.\\n3.2. Multilingual Code Generation\\nIn addition to Python, as shown in Table 2, we perform\\nan extensive evaluation on 6 widely used programming\\nlanguages, i.e., Java, JavaScript, C++, PHP, Swift, and\\nRust, using the MultiPL-E benchmark (Cassano et al.,\\n2022). We report available results from the WizardCoder pa\\x02per (Luo et al., 2023b) and evaluate our models consistently\\nthrough bigcode-evaluation-harness (Ben Allal\\net al., 2022). We skip proprietary models such as Chat\\x02GPT and GPT-4 as they are not supported by the frame\\x02work. Due to a significant inference latency when running\\nWizardCoder-CL-7B using the harness in our environment,\\nwe choose not to include it in our analysis.\\nThe results indicate that Magicoder-CL improves the base\\nCODELLAMA-PYTHON-7B by a large margin among all\\nthe studied programming languages. Moreover, Magicoder\\x02CL also achieves better results than the SOTA 15B\\nWizardCoder-SC among half of the programming lan\\x02guages. Additionally, MagicoderS-CL demonstrates fur\\x02ther improvement over Magicoder-CL on all program\\x02ming languages, achieving comparable performance against\\nWizardCoder-CL-34B with only 7B parameters. It is worth\\nnoting that Magicoder-CL is only trained with very limited\\nmultilingual data but still outperforms other LLMs with\\nsimilar or even larger sizes. Also, although the harness\\n5\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nevaluates models in completion formats which are for base\\nmodels, Magicoders still show significant improvements de\\x02spite being only instruction-tuned. This implies that LLMs\\ncan learn knowledge from the data beyond its format.\\n3.3. Code Generation for Data Science\\nThe DS-1000 dataset (Lai et al., 2022) contains 1K distinct\\ndata science coding issues ranging from 7 popular data sci\\x02ence libraries in Python. It evaluates the realistic and practi\\x02cal use case of an LLM and offers unit tests for validating\\neach problem. DS-1000 has both completion and insertion\\nmodes, but here we only evaluate completion because the\\nbase CODELLAMA-PYTHON does not support infilling. Ta\\x02ble 3 shows the evaluation results where we include the\\nrecent INCODER (Fried et al., 2023), CodeGen (Nijkamp\\net al., 2023), Code-Cushman-001 (Microsoft, 2023a), Star\\x02Coder (Li et al., 2023), CODELLAMA-PYTHON (Roziere `\\net al., 2023), and WizardCoder (Luo et al., 2023b). We\\ncan see from the table that Magicoder-CL-7B already out\\x02performs all the baselines we evaluate, including state\\x02of-the-art WizardCoder-CL-7B and WizardCoder-SC-15B.\\nMagicoderS-CL-7B further breaks the limit by introduc\\x02ing an 8.3 percentage point absolute improvement over\\nWizardCoder-SC-15B.\\n3.4. Comparison with DeepSeek-Coder\\nDeepSeek-Coder (Guo et al., 2024) is a series of models\\nreleased concurrently to our work and they demonstrate su\\x02perior coding performance. We only briefly discuss it in\\nthis section because its data and instruction tuning details\\nare not publicly available at the time of writing. We apply\\nthe same finetuning strategy on DeepSeek-Coder-Base-6.7B\\nas we performed on CODELLAMA-PYTHON-7B, leading\\nto Magicoder-DS and MagicoderS-DS. Table 4 shows a\\nsimilar trend as Table 1 that the base model can be sig\\x02nificantly improved after applying OSS-INSTRUCT. Re\\x02markably, the MagicoderS-DS variant surpasses DeepSeek\\x02Coder-Instruct-6.7B on all the benchmarks with ×8 fewer\\ntraining tokens, and it also closely matches DeepSeek\\x02Coder-Instruct-33B on these datasets.\\n4. Ablations of Data Source\\n4.1. Impact of the Language Distribution\\nTo understand the correlation between the programming lan\\x02guages appearing in the training data and the downstream\\nperformance of different languages, we conduct an addi\\x02tional ablation study about the training data. We classify the\\n75K training data into approximately 43K Python-only, and\\n32K non-Python data according to whether ‘‘‘python\\nis a substring of the generated data. We do not classify\\nthe data based on the seed code snippet because LLMs per\\x02forming OSS-INSTRUCT may produce code in a different\\nprogramming language than the seed.\\nTable 5 shows the evaluation results, where we consistently\\nfinetune the base CODELLAMA-PYTHON-7B for 2 epochs\\non different data partitions using the same training hyper\\x02parameters explained in Appendix B. From the table, we\\ncan see that, as can be imagined, training on Python or\\nnon-Python data can substantially boost the performance of\\nthe base model in Python or non-Python tasks, respectively.\\nInterestingly, instruction tuning on different programming\\nlanguages can still boost the overall coding performance\\nthat includes out-of-distribution languages. For example,\\nwhen trained on only non-Python data, Magicoder-CL still\\nachieves a 10.4 percentage point improvement over the base\\nmodel in the Python-only evaluation. This implies LLMs\\ncan establish correlations between different programming\\nlanguages and perform transfer learning of deeper code se\\x02mantics. Finally, we observe a more significant boost in\\nPython evaluation when combining data from both sources,\\nwith a slight decrease in multilingual performance compared\\nwith only finetuning on multilingual data. We attribute this\\ndecrease to the dominant amount of Python data (around\\n57%) during instruction tuning.\\n4.2. OSS-INSTRUCT vs. Direct Finetuning\\nThe fact that OSS-INSTRUCT gets an LLM inspired from\\nopen-source code snippets may lead to a natural question:\\nwhy not directly finetuning on these open-source code? To\\nanswer this question, we follow CodeSearchNet (Husain\\net al., 2020) to mine semantically relevant comment-function\\npairs from the same seed document corpus we use to con\\x02struct the 75K OSS-INSTRUCT dataset. We then train the\\nmodel to predict the function bodies from the function signa\\x02tures and comments. We prioritize comment-function pairs\\nthat overlap with our 75K seed snippets, resulting in about\\n11K data points. To align with our 75K samples, we collect\\nthe remaining 64K samples using the whole corpus of 75K\\nseed documents. Eventually, we have the same number of\\ncomment-function pairs with OSS-INSTRUCT data.\\nWe finetune the base CODELLAMA-PYTHON-7B for 2\\nepochs using the paired data, following the same training\\nsetup discussed in Appendix B. From Table 6, we observe\\nthat finetuning on 75K paired comment-function data even\\nworsens the base model, while OSS-INSTRUCT helps to\\nintroduce a substantial boost. We conjecture that the degra\\x02dation is owing to the substantial noise and inconsistency\\nthat exists intrinsically in the data pairs, even though these\\npaired data exhibit very similar format as HumanEval or\\nMultiPL-E problems. This further shows that data factual\\x02ity, rather than the format, is essential to code instruction\\ntuning. It also indicates the superiority of OSS-INSTRUCT\\nwhich can translate these loosely related code fragments\\n6\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 2: Pass@1 results of different LLMs on MultiPL-E (Cassano et al., 2022) following the same hyperparameter\\nsettings as the WizardCoder paper (Luo et al., 2023b): temperature = 0.2, top p = 0.95, max length = 512, and\\nnum samples = 50. We evaluate all 7B models using bigcode-evaluation-harness (Ben Allal et al., 2022) and\\nreport other results from WizardCoder.\\nModel Size\\nProgramming Language\\nJava JavaScript C++ PHP Swift Rust\\nCODELLAMA 34B 40.2 41.7 41.4 40.4 35.3 38.7\\nCODELLAMA-PYTHON 34B 39.5 44.7 39.1 39.8 34.3 39.7\\nCODELLAMA-INSTRUCT 34B 41.5 45.9 41.5 37.0 37.6 39.3\\nWizardCoder-CL 34B 44.9 55.3 47.2 47.2 44.3 46.2\\nStarCoderBase 15B 28.5 31.7 30.6 26.8 16.7 24.5\\nStarCoder 15B 30.2 30.8 31.6 26.1 22.7 21.8\\nWizardCoder-SC 15B 35.8 41.9 39.0 39.3 33.7 27.1\\nCODELLAMA 7B 29.3 31.7 27.0 25.1 25.6 25.5\\nCODELLAMA-PYTHON 7B 29.1 35.7 30.2 29.0 27.1 27.0\\nMagicoder-CL 7B 36.4 45.9 36.5 39.5 33.4 30.6\\nMagicoderS-CL 7B 42.9 57.5 44.4 47.6 44.1 40.3\\nTable 3: Pass@1 results on DS-1000 (completion format) with temperature = 0.2, top p = 0.5, max length =\\n1024, and num samples = 40, following the same hyperparameter setting used in WizardCoder (Luo et al., 2023b). We\\nevaluate all the 7B models with their preferred prompt formats and report other results from WizardCoder.\\nModel Size\\n+ 155\\nMatplotlib\\n+ 220\\nNumPy\\n+ 291\\nPandas\\n+ 68\\nPyTorch\\n+ 106\\nSciPy\\n+ 115\\nSklearn\\n+ 45\\nTensorFlow\\n= 1000\\nOverall\\nINCODER 6.7B 28.3 4.4 3.1 4.4 2.8 2.8 3.8 7.4\\nCodeGen-Mono 16B 31.7 10.9 3.4 7.0 9.0 10.8 15.2 11.7\\nCode-Cushman-001 - 40.7 21.8 7.9 12.4 11.3 18.0 12.2 18.1\\nStarCoder 15B 51.7 29.7 11.4 21.4 20.2 29.5 24.5 26.0\\nWizardCoder-SC 15B 55.2 33.6 16.7 26.2 24.2 24.9 26.7 29.2\\nCODELLAMA-PYTHON 7B 55.3 34.5 16.4 19.9 22.3 17.6 28.5 28.0\\nWizardCoder-CL 7B 53.5 34.4 15.2 25.7 21.0 24.5 28.9 28.4\\nMagicoder-CL 7B 54.6 34.8 19.0 24.7 25.0 22.6 28.9 29.9\\nMagicoderS-CL 7B 55.9 40.6 28.4 40.4 28.8 35.8 37.6 37.5\\nTable 4: Pass@1 (greedy decoding) comparison between Magicoder and DeepSeek-Coder (Guo et al., 2024) on Hu\\x02manEval (+) and MBPP (+). DeepSeek-Coder results are reported from EvalPlus (Liu et al., 2023b) Leaderboard.\\nModel Size Training Tokens\\nBenchmark Open-Source\\nHumanEval (+) MBPP (+) Weight Data\\nDeepSeek-Coder-Base\\n1.3B 2T - 55.4 (46.9) #\\n6.7B 2T 47.6 (39.6) 70.2 (56.6) #\\n33B 2T 51.2 (43.3) - #\\nDeepSeek-Coder Instruct\\n1.3B +2B 64.6 (58.5) 63.7 (53.1) #\\n6.7B +2B 73.8 (70.1) 72.7 (63.4) #\\n33B +2B 78.7 (72.6) 78.7 (66.7) #\\nMagicoder-DS 6.7B +90M 66.5 (60.4) 75.4 (61.9) \\nMagicoderS-DS 6.7B +240M 76.8 (70.7) 75.7 (64.4) \\n7\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 5: Ablation study of using different programming languages as training data. We show the pass@1 results on\\nHumanEval+ (Liu et al., 2023b) for Python and the average pass@1 results on MultiPL-E (Cassano et al., 2022) for the\\nsame set of programming languages used in Table 2 (i.e., Java, JavaScript, C++, PHP, Swift, and Rust). All the variants are\\nfinetuned with 2 epochs and evaluated through greedy-decoding.\\nModel (7B) Finetuning Data Python (HumanEval+) Others (MultiPL-E)\\nCODELLAMA-PYTHON - 34.1 29.6\\nMagicoder-CL Python (43K) 47.6 32.7\\nMagicoder-CL Others (32K) 44.5 38.3\\nMagicoder-CL Both (75K) 55.5 37.8\\nTable 6: Comparison between OSS-INSTRUCT and directly\\nfinetuning on comment-function pairs with CODELLAMA\\x02PYTHON-7B as the base model.\\nFinetuning Data HumanEval+ MultiPL-E\\nBase model w/o finetuning 34.1 29.6\\nComment-function pairs (75K) 34.1 24.1\\nOSS-INSTRUCT (75K) 55.5 37.8\\ninto semantically-consistent instruction-tuning data.\\n4.3. OSS-INSTRUCT with A Less Powerful Teacher\\nIn this section, we explore the factors contributing to the\\neffectiveness of OSS-INSTRUCT beyond just the distillation\\nof the teacher model. We propose two potential key reasons.\\nFirst, since the base model is pretrained with comprehen\\x02sive code data, the distillation process likely activates the\\nmodel’s internal capabilities, leading to improved perfor\\x02mance in coding tasks. Second, OSS-INSTRUCT uses seed\\ncode snippets to generate problem-solution pairs in one shot.\\nThese seed snippets provide valuable context, enabling the\\nmodel to create better solutions than a plain teacher model\\nlacking such seed information. These enhanced solutions\\ncan then be used to train more effective student models. To\\nverify these points, we conduct an additional experiment\\nby generating a subset of 20K OSS-INSTRUCT data using\\nMixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024), a state-of\\x02the-art, general-purpose, open-source LLM.\\nTable 7: Pass@1 on HumanEval+ and MBPP+ when fine\\x02tuning CODELLAMA-PYTHON-7B for 2 epochs on 20K\\nOSS-INSTRUCT data generated by Mixtral-8x7B-Instruct\\x02v0.1 (Jiang et al., 2024).\\nModel HumanEval+ MBPP+\\nMixtral-8x7B-Instruct-v0.1 39.6 47.4\\nCODELLAMA-PYTHON-7B 34.1 45.4\\nMagicoder-CL-Mixtral-7B 55.5 50.4\\nTable 7 indicates that Magicoder-CL-Mixtral-7B not\\nonly significantly improves over the base CODELLAMA\\x02PYTHON, but is also better than Mixtral-8x7B-Instruct-v0.1\\n(i.e., the teacher model) across HumanEval+ and MBPP+.\\nThese results suggest that OSS-INSTRUCT is not simply dis\\x02tilling a teacher model, but also triggering the base model’s\\nown capability and effectively leveraging the information\\nencapsulated in seed code snippets.\\n5. Related Work\\nFoundation models for code Trained over billions of\\nlines of code, LLMs have demonstrated outstanding per\\x02formance in a wide range of software engineering tasks,\\nincluding code generation (Chen et al., 2021; Austin et al.,\\n2021), program repair (Xia & Zhang, 2022; Wei et al.,\\n2023; Xia et al., 2023b; Jiang et al., 2023b; Bouzenia et al.,\\n2024), and software testing (Xia et al., 2023a; Deng et al.,\\n2023; Yuan et al., 2023; Schafer et al. ¨ , 2023; Lemieux et al.,\\n2023). In particular, prominent base models, such as Code\\x02Gen (Nijkamp et al., 2023), CodeT5 (Wang et al., 2021),\\nStarCoder (Li et al., 2023), and CODELLAMA (Roziere `\\net al., 2023), are pre-trained over a huge number of code\\x02base from scratch, establishing the fundamental ability of\\ngeneral code generation and understanding. More recent\\ncode LLMs, such as DeepSeek-Coder (Guo et al., 2024) and\\nStarCoder2 (Lozhkov et al., 2024), additionally organize\\nthe pretraining data at the repository level to enhance the\\nmodel’s contextual understanding capabilities. Furthermore,\\nthese base models are also finetuned (Luo et al., 2023b) or\\nprompted (Chen et al., 2023) to unlock their true potential\\nto specialize in solving domain-specific coding tasks.\\nInstruction tuning with synthetic data Instruction tun\\x02ing aims to improve pretrained LLMs by finetuning them\\nwith a mixture of instructions and corresponding re\\x02sponses (Wei et al., 2022). However, obtaining high\\x02quality instructional data is oftentimes laborious. Hence,\\nresearchers are increasingly focusing on the development\\nof methods to generate synthetic instruction data. Wang\\net al. (2023a) introduces SELF-INSTRUCT, where a founda\\x028\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\ntion LLM (GPT-3 (Brown et al., 2020)) is used to gen\\x02erate synthetic instruction-response pairs with carefully\\ncrafted prompts. The same LLM is then instruction-tuned on\\nthe synthetic data to distill such self-generated knowledge.\\nThis technique has been further extended to create synthetic\\ndata with different LLMs. For example, Alpaca (Taori et al.,\\n2023) and Code Alpaca (Chaudhary, 2023) apply SELF\\x02INSTRUCT to finetune LLAMA with ChatGPT-generated\\ninstructions. To improve SELF-INSTRUCT, WizardLM (Xu\\net al., 2023) and WizardCoder (Luo et al., 2023a) propose\\nEvol-Instruct and Code Evol-Instruct by guiding ChatGPT\\nwith heuristic prompts to make the synthetic data more com\\x02plex and diverse. More recently, Gunasekar et al. (2023)\\nshows that textbook-quality synthetic data alone can help\\nthe model achieve remarkable coding and reasoning capa\\x02bilities. Orthogonal to all existing methods, our proposed\\nOSS-INSTRUCT allows LLMs to get inspired from real\\x02world code snippets for better controllability, quality, and\\ncreativity in coding tasks.\\nEvaluating LLMs for code Most code benchmarks eval\\x02uate LLMs on generating single-function programs from\\nnatural language descriptions. Such benchmarks include\\nHumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),\\nAPPS (Hendrycks et al., 2021), and CodeContests (Li et al.,\\n2022). A handful of manual tests are used to assess the\\nfunctional correctness of LLM-generated solutions. How\\x02ever, insufficient tests can lead to false negatives. Conse\\x02quently, the EvalPlus framework (Liu et al., 2023b) pro\\x02duces HumanEval+ and MBPP+ by extending 80×/35×\\nmore tests. To address dataset contamination issues, re\\x02searchers propose LiveCodeBench (Jain et al., 2024), which\\ncompiles fresh coding problems not included in model\\ntraining, and EvoEval (Xia et al., 2024), which strategi\\x02cally leverages LLMs to evolve existing benchmarks into\\nnew coding tasks. Meanwhile, there are comprehensive\\nbenchmarks evaluating code generation for data science\\n(DS-1000 (Lai et al., 2022)), addressing open-source issues\\n(SWE-bench (Jimenez et al., 2023)), and repository-level\\ncode generation (CROSSCODEEVAL (Ding et al., 2023) and\\nRepoEval (Zhang et al., 2023)).\\n6. Conclusion and Future Work\\nWe propose OSS-INSTRUCT, a novel data generation\\nmethod using Large Language Models to generate diverse\\ncoding challenges from open-source code snippets. This\\napproach enables Magicoder, which significantly improves\\nthe base LLM. Despite having less than 7B parameters, it\\ncan outperform all evaluate LLMs with less than or equal to\\n16B parameters, including the 15B WizardCoder. Combin\\x02ing OSS-INSTRUCT with Evol-Instruct allows us to build\\nthe enhanced MagicoderS models. They achieve remark\\x02able results by rivaling leading models like ChatGPT in\\nHumanEval benchmarks. We fully open source the model\\nweights, training data, and source code, to enable future\\nresearch in LLMs for code. In the near future, we will ap\\x02ply OSS-INSTRUCT to larger base models. We will also\\ncontinue advancing OSS-INSTRUCT by generating higher\\x02quality data with a strategically designed distribution of the\\nseed code snippets and with more advanced teacher LLMs\\nsuch as GPT-4.\\nAcknowledgement\\nWe thank all the reviewers for their insightful comments and\\nsuggestions for our paper. This work was partially supported\\nby NSF grant CCF-2131943, as well as Kwai Inc.\\nImpact Statement\\nThis work is motivated to boost large language models\\nin terms of their code generation and understanding ca\\x02pabilities through instruction tuning. The proposed OSS\\x02INSTRUCT method leverages the abundance of open source\\nto generate diverse and controllable instruction data. We ex\\x02pect this idea to also foster innovative software solutions tai\\x02lored to domain-specific needs, particularly in areas where\\nreal data is private and scarce, by generating extensive syn\\x02thetic data. Additionally, our method reinforces the value\\nof community-driven content and knowledge sharing by\\nincorporating open-source code as references.\\nHowever, it is essential to recognize the potential for misuse,\\nsuch as the deliberate generation of vulnerable code that can\\nbe exploited for malicious purposes. Ultimately, adhering\\nto ethical guidelines is crucial to ensure the responsible use\\nof this technique.\\nReferences\\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Fer\\x02randis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey,\\nM., Umapathi, L. K., Anderson, C. J., Zi, Y., Poirier, J. L.,\\nSchoelkopf, H., Troshin, S., Abulkhanov, D., Romero,\\nM., Lappert, M., Toni, F. D., del R´ıo, B. G., Liu, Q.,\\nBose, S., Bhattacharyya, U., Zhuo, T. Y., Yu, I., Villegas,\\nP., Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H.,\\nContractor, D., Villa, L., Li, J., Bahdanau, D., Jernite, Y.,\\nHughes, S., Fried, D., Guha, A., de Vries, H., and von\\nWerra, L. Santacoder: don’t reach for the stars!, 2023.\\nAustin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,\\nH., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V.,\\nand Sutton, C. Program synthesis with large language\\nmodels. CoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732.\\nBen Allal, L., Muennighoff, N., Kumar Umapathi,\\n9\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nL., Lipkin, B., and von Werra, L. A framework\\nfor the evaluation of code generation models.\\nhttps://github.com/bigcode-project/\\nbigcode-evaluation-harness, 2022.\\nBouzenia, I., Devanbu, P., and Pradel, M. Repairagent: An\\nautonomous, llm-based agent for program repair. arXiv\\npreprint arXiv:2403.17134, 2024.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,\\nRadford, A., Sutskever, I., and Amodei, D. Language\\nmodels are few-shot learners. In Larochelle, H.,\\nRanzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),\\nAdvances in Neural Information Processing Systems,\\nvolume 33, pp. 1877–1901. Curran Associates, Inc.,\\n2020. URL https://proceedings.neurips.\\ncc/paper_files/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\\npdf.\\nCambronero, J., Gulwani, S., Le, V., Perelman, D., Rad\\x02hakrishna, A., Simon, C., and Tiwari, A. Flashfill++:\\nScaling programming by example by cutting to the\\nchase. Proc. ACM Program. Lang., 7(POPL), jan 2023.\\ndoi: 10.1145/3571226. URL https://doi.org/10.\\n1145/3571226.\\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps\\x02Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson,\\nC. J., Feldman, M. Q., Guha, A., Greenberg, M., and\\nJangda, A. Multipl-e: A scalable and extensible approach\\nto benchmarking neural code generation, 2022.\\nChaudhary, S. Code alpaca: An instruction-following llama\\nmodel for code generation. https://github.com/\\nsahil280114/codealpaca, 2023.\\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\\nH. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar\\x02ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\\nPlappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\\nV., Morikawa, E., Radford, A., Knight, M., Brundage,\\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\\nW. Evaluating large language models trained on code,\\n2021.\\nChen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large ¨\\nlanguage models to self-debug, 2023.\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., Hesse, C., and Schulman, J. Training verifiers to solve\\nmath word problems, 2021.\\nDeng, Y., Xia, C. S., Peng, H., Yang, C., and Zhang, L.\\nLarge language models are zero-shot fuzzers: Fuzzing\\ndeep-learning libraries via large language models, 2023.\\nDing, Y., Wang, Z., Ahmad, W. U., Ding, H., Tan, M.,\\nJain, N., Ramanathan, M. K., Nallapati, R., Bhatia, P.,\\nRoth, D., and Xiang, B. Crosscodeeval: A diverse\\nand multilingual benchmark for cross-file code comple\\x02tion. In Thirty-seventh Conference on Neural Informa\\x02tion Processing Systems Datasets and Benchmarks Track,\\n2023. URL https://openreview.net/forum?\\nid=wgDcbBMSfh.\\nFeng, Y., Martins, R., Bastani, O., and Dillig, I. Program\\nsynthesis using conflict-driven learning. SIGPLAN Not.,\\n53(4):420–435, jun 2018. ISSN 0362-1340. doi: 10.\\n1145/3296979.3192382. URL https://doi.org/\\n10.1145/3296979.3192382.\\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace,\\nE., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and\\nLewis, M. Incoder: A generative model for code infilling\\nand synthesis. In The Eleventh International Confer\\x02ence on Learning Representations, 2023. URL https:\\n//openreview.net/forum?id=hQwb-lbM6EL.\\nGulwani, S., Polozov, O., and Singh, R. Program syn\\x02thesis. Foundations and Trends® in Programming Lan\\x02guages, 4(1-2):1–119, 2017. ISSN 2325-1107. doi:\\n10.1561/2500000010. URL http://dx.doi.org/\\n10.1561/2500000010.\\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T.,\\nGiorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P.,\\nde Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl,\\nH. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee,\\nY. T., and Li, Y. Textbooks are all you need, 2023.\\nGuo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W.,\\nChen, G., Bi, X., Wu, Y., Li, Y. K., Luo, F., Xiong, Y.,\\nand Liang, W. Deepseek-coder: When the large language\\nmodel meets programming – the rise of code intelligence,\\n2024.\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\\nSteinhardt, J. Measuring coding challenge competence\\nwith apps, 2021.\\n10\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnat\\x02ural instructions: Tuning language models with (almost)\\nno human labor. In Rogers, A., Boyd-Graber, J., and\\nOkazaki, N. (eds.), Proceedings of the 61st Annual Meet\\x02ing of the Association for Computational Linguistics (Vol\\x02ume 1: Long Papers), pp. 14409–14428, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2023.acl-long.806. URL https:\\n//aclanthology.org/2023.acl-long.806.\\nHugging Face. Hugging face: The ai community build\\x02ing the future. https://huggingface.co/, 2023.\\nAccessed: 2023-12-01.\\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and\\nBrockschmidt, M. Codesearchnet challenge: Evaluat\\x02ing the state of semantic code search, 2020.\\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T.,\\nWang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Live\\x02codebench: Holistic and contamination free evaluation of\\nlarge language models for code, 2024.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\\nT., and Sayed, W. E. Mistral 7b, 2023a.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna,\\nE. B., Bressand, F., Lengyel, G., Bour, G., Lample, G.,\\nLavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P.,\\nSubramanian, S., Yang, S., Antoniak, S., Scao, T. L.,\\nGervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed,\\nW. E. Mixtral of experts, 2024.\\nJiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code\\nlanguage models on automated program repair, 2023b.\\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press,\\nO., and Narasimhan, K. Swe-bench: Can language mod\\x02els resolve real-world github issues?, 2023.\\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis,\\nC. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T.,\\nBahdanau, D., von Werra, L., and de Vries, H. The stack:\\n3 tb of permissively licensed source code, 2022.\\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer,\\nL., tau Yih, S. W., Fried, D., Wang, S., and Yu, T. Ds\\x021000: A natural and reliable benchmark for data science\\ncode generation, 2022.\\nLemieux, C., Inala, J. P., Lahiri, S. K., and Sen, S. Co\\x02damosa: Escaping coverage plateaus in test genera\\x02tion with pre-trained large language models. In 2023\\nIEEE/ACM 45th International Conference on Software\\nEngineering (ICSE), pp. 919–931. IEEE, 2023.\\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D.,\\nMou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q.,\\nZheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O.,\\nDavaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko,\\nO., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Uma\\x02pathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang,\\nZ., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov,\\nD., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhat\\x02tacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas,\\nP., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor,\\nN., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J.,\\nDao, T., Mishra, M., Gu, A., Robinson, J., Anderson,\\nC. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried,\\nD., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes,\\nS., Wolf, T., Guha, A., von Werra, L., and de Vries, H.\\nStarcoder: may the source be with you!, 2023.\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser,\\nJ., Leblond, R., Eccles, T., Keeling, J., Gimeno, F.,\\nDal Lago, A., Hubert, T., Choy, P., de Masson d’Autume,\\nC., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J.,\\nGowal, S., Cherepanov, A., Molloy, J., Mankowitz,\\nD. J., Sutherland Robson, E., Kohli, P., de Freitas,\\nN., Kavukcuoglu, K., and Vinyals, O. Competition\\x02level code generation with alphacode. Science, 378\\n(6624):1092–1097, December 2022. ISSN 1095-9203.\\ndoi: 10.1126/science.abq1158. URL http://dx.doi.\\norg/10.1126/science.abq1158.\\nLiu, J., Peng, J., Wang, Y., and Zhang, L. Neuri: Di\\x02versifying dnn generation via inductive rule inference.\\nIn Proceedings of the 31st ACM Joint European Soft\\x02ware Engineering Conference and Symposium on the\\nFoundations of Software Engineering, ESEC/FSE 2023,\\npp. 657–669, New York, NY, USA, 2023a. Associa\\x02tion for Computing Machinery. ISBN 9798400703270.\\ndoi: 10.1145/3611643.3616337. URL https://doi.\\norg/10.1145/3611643.3616337.\\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code\\ngenerated by chatGPT really correct? rigorous evaluation\\nof large language models for code generation. In Thirty\\x02seventh Conference on Neural Information Processing\\nSystems, 2023b. URL https://openreview.net/\\nforum?id=1qvx610Cu7.\\nLozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier,\\nJ., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T.,\\nTian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang,\\nZ., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.-D.,\\nRisdal, M., Li, J., Zhu, J., Zhuo, T. Y., Zheltonozhskii,\\nE., Dade, N. O. O., Yu, W., Krauß, L., Jain, N., Su, Y.,\\nHe, X., Dey, M., Abati, E., Chai, Y., Muennighoff, N.,\\nTang, X., Oblokulov, M., Akiki, C., Marone, M., Mou,\\nC., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A.,\\nDehaene, O., Patry, N., Xu, C., McAuley, J., Hu, H.,\\n11\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nScholak, T., Paquet, S., Robinson, J., Anderson, C. J.,\\nChapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y.,\\nFerrandis, C. M., Zhang, L., Hughes, S., Wolf, T., Guha,\\nA., von Werra, L., and de Vries, H. Starcoder 2 and the\\nstack v2: The next generation, 2024.\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C.,\\nMa, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering\\ncode large language models with evol-instruct. arXiv\\npreprint arXiv:2306.08568, 2023a.\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C.,\\nMa, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering\\ncode large language models with evol-instruct, 2023b.\\nMicrosoft. Azure openai service models. https:\\n//learn.microsoft.com/en-us/azure/\\ncognitive-services/openai/concepts/\\nmodels, 2023a.\\nMicrosoft. GitHub Copilot – Your AI pair pro\\x02grammer. https://github.com/features/\\ncopilot, 2023b.\\nMuennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B.,\\nZhuo, T. Y., Singh, S., Tang, X., von Werra, L., and\\nLongpre, S. Octopack: Instruction tuning code large\\nlanguage models, 2023.\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H.,\\nZhou, Y., Savarese, S., and Xiong, C. Codegen: An\\nopen large language model for code with multi-turn pro\\x02gram synthesis. In The Eleventh International Confer\\x02ence on Learning Representations, 2023. URL https:\\n//openreview.net/forum?id=iaYcJKpY2B_.\\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and\\nSolar-Lezama, A. Is self-repair a silver bullet for code\\ngeneration? In The Twelfth International Conference\\non Learning Representations, 2024. URL https://\\nopenreview.net/forum?id=y0GJXRungR.\\nOpenAI. Chatgpt: Optimizing language models for dialogue.\\nhttps://openai.com/blog/chatgpt/, 2022.\\nOpenAI. Gpt-4 technical report, 2023.\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, `\\nX. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,\\nA., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C.,\\nGrattafiori, A., Xiong, W., Defossez, A., Copet, J., Azhar, ´\\nF., Touvron, H., Martin, L., Usunier, N., Scialom, T., and\\nSynnaeve, G. Code llama: Open foundation models for\\ncode, 2023.\\nSchafer, M., Nadi, S., Eghbali, A., and Tip, F. An empirical ¨\\nevaluation of using large language models for automated\\nunit test generation. IEEE Transactions on Software En\\x02gineering, 2023.\\nServices, A. W. AI Code Generator - Amazon Code\\x02Whisperer - AWS. https://aws.amazon.com/\\ncodewhisperer/, 2023.\\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\\nrates with sublinear memory cost, 2018.\\nSPARCK JONES, K. A statistical interpretation of term\\nspecificity and its application in retrieval. 28(1):11–21,\\n2023/11/30 1972. doi: 10.1108/eb026526. URL https:\\n//doi.org/10.1108/eb026526.\\nSu, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Ostendorf,\\nM., Yih, W.-t., Smith, N. A., Zettlemoyer, L., and Yu, T.\\nOne embedder, any task: Instruction-finetuned text em\\x02beddings. 2022. URL https://arxiv.org/abs/\\n2212.09741.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\\nStanford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca, 2023.\\ntheblackcat102. The evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/\\ntheblackcat102/evol-codealpaca-v1,\\n2023.\\nWang, X., Dillig, I., and Singh, R. Program synthesis using\\nabstraction refinement. Proc. ACM Program. Lang., 2\\n(POPL), dec 2017. doi: 10.1145/3158151. URL https:\\n//doi.org/10.1145/3158151.\\nWang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5:\\nIdentifier-aware unified pre-trained encoder-decoder mod\\x02els for code understanding and generation. In Moens,\\nM.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Pro\\x02ceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 8696–8708, On\\x02line and Punta Cana, Dominican Republic, November\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.emnlp-main.685. URL https://\\naclanthology.org/2021.emnlp-main.685.\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\\nKhashabi, D., and Hajishirzi, H. Self-instruct: Align\\x02ing language models with self-generated instructions. In\\nRogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Pro\\x02ceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\npp. 13484–13508, Toronto, Canada, July 2023a. Associ\\x02ation for Computational Linguistics. doi: 10.18653/v1/\\n2023.acl-long.754. URL https://aclanthology.\\norg/2023.acl-long.754.\\n12\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nWang, Y., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and\\nHoi, S. C. H. Codet5+: Open code large language models\\nfor code understanding and generation, 2023b.\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\\nmodels are zero-shot learners, 2022.\\nWei, Y., Xia, C. S., and Zhang, L. Copiloting the copilots:\\nFusing large language models with completion engines\\nfor automated program repair, 2023.\\nXia, C. S. and Zhang, L. Less training, more repairing\\nplease: Revisiting automated program repair via zero\\x02shot learning, 2022.\\nXia, C. S. and Zhang, L. Keep the conversation going:\\nFixing 162 out of 337 bugs for $0.42 each using chatgpt.\\narXiv preprint arXiv:2304.00385, 2023.\\nXia, C. S., Paltenghi, M., Tian, J. L., Pradel, M., and Zhang,\\nL. Universal fuzzing via large language models, 2023a.\\nXia, C. S., Wei, Y., and Zhang, L. Automated program\\nrepair in the era of large pre-trained language models.\\nIn 2023 IEEE/ACM 45th International Conference on\\nSoftware Engineering (ICSE), pp. 1482–1494, 2023b. doi:\\n10.1109/ICSE48619.2023.00129.\\nXia, C. S., Deng, Y., and Zhang, L. Top leaderboard ranking\\n= top coding proficiency, always? evoeval: Evolving\\ncoding benchmarks via llm, 2024.\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\\nC., and Jiang, D. Wizardlm: Empowering large language\\nmodels to follow complex instructions. arXiv preprint\\narXiv:2304.12244, 2023.\\nYu, Y., Zhuang, Y., Zhang, J., Meng, Y., Ratner, A., Krishna,\\nR., Shen, J., and Zhang, C. Large language model as\\nattributed training data generator: A tale of diversity and\\nbias, 2023.\\nYuan, Z., Lou, Y., Liu, M., Ding, S., Wang, K., Chen, Y.,\\nand Peng, X. No more manual tests? evaluating and\\nimproving chatgpt for unit test generation. arXiv preprint\\narXiv:2305.04207, 2023.\\nZhang, F., Chen, B., Zhang, Y., Keung, J., Liu, J., Zan,\\nD., Mao, Y., Lou, J.-G., and Chen, W. Repocoder:\\nRepository-level code completion through iterative re\\x02trieval and generation, 2023.\\n13\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nYou are exceptionally skilled at crafting high-quality programming problems and \\noffering precise solutions.\\nPlease gain inspiration from the following random code snippet to create a \\nhigh-quality programming problem. Present your output in two distinct sections: \\n[Problem Description] and [Solution].\\nCode snippet for inspiration:\\n```\\n{code}\\n```\\nGuidelines for each section:\\n1. [Problem Description]: This should be **completely self-contained**, providing \\nall the contextual information one needs to understand and solve the problem. \\nAssume common programming knowledge, but ensure that any specific context, \\nvariables, or code snippets pertinent to this problem are explicitly included.\\n2. [Solution]: Offer a comprehensive, **correct** solution that accurately \\naddresses the [Problem Description] you provided.\\nFigure 4: The detailed prompt design for OSS-INSTRUCT\\nA. More Details of OSS-INSTRUCT\\nA.1. Prompt Design\\nFigure 4 illustrates the prompt template of OSS-INSTRUCT, where the first section presents a high-level description of the\\ntask, the second section incorporates the code snippet, and the third section offers guidelines on the response.\\nA.2. Qualitative Examples\\nFigure 5 extends Figure 2 and shows more qualitative examples of OSS-INSTRUCT generation, including outputs that are\\ninspired by method definitions, shell scripts, library imports, class signatures, code statements, and code comments.\\nA.3. Breakdown of OSS-INSTRUCT Dataset\\nCategories To study the categories of OSS-INSTRUCT-generated data, we use INSTRUCTOR (Su et al., 2022), which is\\none of the SOTA embedding models and can generate different text embeddings according to a task instruction. Inspired by\\nOctoPack (Muennighoff et al., 2023) and the topic tags on GitHub, we manually designed 10 categories specific to coding.\\nAs shown in Figure 6, we calculate the cosine similarity between the embeddings of each sample in OSS-INSTRUCT and\\nthe 10 categories to obtain the category breakdown. Overall, OSS-INSTRUCT exhibits diversity and balance across different\\ncategories.\\nLength distribution We depict the length distribution for both generated problems and solutions in Figure 7. The x-axis\\nrepresents the number of tokens in each problem/solution, while the y-axis shows the correspondent number of samples.\\nB. Implementation Details\\nB.1. Data Generation\\nWe use gpt-3.5-turbo-1106 as the foundation model to do OSS-INSTRUCT due to its high cost-effectiveness. We\\nrandomly extract 1–15 lines from each selected code document from starcoderdata and let gpt-3.5-turbo-1106\\nimagine a self-contained coding problem and a correct solution. Given the numerous seed code snippets, we perform greedy\\ndecoding to maximize the consistency between the generated problems and solutions.\\n14\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD Seed: method definition\\nrender() {\\nQ Problem\\nYour task is to complete the `render` method to \\ngenerate the rendered shape as a string...\\nL Code\\nclass ShapeRenderer {\\n constructor(vertices) {\\n this.vertices = vertices;\\n }\\n render() {\\n let renderedShape = \"\";\\n for (let i = 0; i < this.vertices.length; i++) {\\n const vertex = this.vertices[i];\\n renderedShape += `(${vertex.x}, ${vertex.y})`;\\n if (i < this.vertices.length - 1) {\\n renderedShape += \" - \";\\n }\\n }\\n return renderedShape;\\n }\\n}\\nD Seed: library imports\\nimport numpy as np\\nimport gym_electric_motor as gem\\nimport matplotlib.pyplot as plt\\nQ Problem\\nCreate a reinforcement learning agent to control an \\nelectric motor using the OpenAI Gym environment...\\nL Code\\nimport numpy as np\\nimport gym_electric_motor as gem\\n...\\nenv = gem.make(\"DcSeriesCont-v1\")\\nclass DQNAgent:\\n def __init__(self, state_dim, action_dim): ...\\n def build_model(self): ...\\n def act(self, state): ...\\n def train(self, state, action, reward, ...): ...\\n...\\nfor episode in range(episodes):\\n state = env.reset()\\n state = np.reshape(state, [1, state_dim])\\n ...\\nD Seed: class signature\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {\\n public static void main(String[] args) {\\nQ Problem\\nCreate a simple Java Spring Boot application for a \\nbanking system...\\nL Code\\nimport org.axonframework.commandhandling.CommandBus;\\nimport org.axonframework.config.Configuration;\\n...\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {...}\\npublic class BankAccount {...}\\npublic class CreateAccountCommand {...}\\npublic class DepositFundsCommand {...}\\npublic class WithdrawFundsCommand {...}\\npublic class AccountCreatedEvent {...}\\npublic class FundsDepositedEvent {...}\\npublic class FundsWithdrawnEvent {...}\\nD Seed: code statements\\ncutoff_range = np.ptp(cutoffs)\\nif cutoff_range == 0: cutoff_range = 1\\ncutoff_colors = plt.get_cmap(\\'plasma\\')(\\n (cutoffs - np.min(cutoffs)) / cutoff_range\\n)\\nQ Problem\\nImplement a function that calculates the color values \\nfor a given set of cutof values based on a specified \\ncolor map...\\nL Code\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef calculate_cutoff_colors(cutoffs, cmap_name):\\n cutoff_range = np.ptp(cutoffs)\\n if cutoff_range == 0:\\n cutoff_range = 1\\n cmap = plt.get_cmap(cmap_name)\\n normalized_cutoffs = ...\\n cutoff_colors = ...\\n return cutoff_colors\\nD Seed: shell script\\npython3 makeErrorFile.py data/test_dataset_14 14\\nQ Problem\\nCreate a Python program that generates an error file\\nbased on a given dataset...\\nL Code\\ndef generate_error_file(dataset_file, ...):\\n error_lines = []\\n with open(dataset_file, \\'r\\') as file:\\n for line in file:\\n ...\\n with open(error_file_name, \\'w\\') as error_file:\\n for error_line in error_lines:\\n error_file.write(error_line + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n if len(sys.argv) != 3:\\n print(\"Usage: ...\")\\n else:\\n dataset_file = sys.argv[1]\\n dataset_number = sys.argv[2]\\n generate_error_file(...)\\nD Seed: comments\\n# Set degrees\\nQ Problem\\nImplement a Python class that represents a \\ntemperature in degrees...\\nL Code\\nclass TemperatureConverter:\\n def __init__(self): ...\\n def set_celsius(self, degrees): ...\\n def set_fahrenheit(self, degrees): ...\\n def set_kelvin(self, degrees): ...\\n def get_celsius(self): ...\\n def get_fahrenheit(self): ...\\n def get_kelvin(self): ...\\n def convert_to(self, unit):\\n if unit == \\'C\\':\\n return self.get_celsius()\\n elif unit == \\'F\\':\\n return self.get_fahrenheit()\\n elif unit == \\'K\\':\\n return self.get_kelvin()\\n ...\\nFigure 5: More examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed\\nproblem requirements, implementations, and explanations are omitted for brevity.\\n15\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nFigure 6: The category constitution of OSS-INSTRUCT\\n0 100 200 300 400 500 600 700\\nNumber of Tokens\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n#Count (Thousand)\\nproblem\\nsolution\\nFigure 7: Token count distribution of OSS-INSTRUCT-generated problems and solutions\\nB.2. Data Decontamination\\nWe apply data decontamination before training our Magicoder and MagicoderS models. Following Li et al. (2023), we\\ndecontaminate both our 75K OSS-INSTRUCT dataset and the evol-codealpaca-v1 (theblackcat102, 2023) dataset,\\nan open-source reproduction of Evol-Instruct generated by GPT-4 (OpenAI, 2023), by removing exact matches from\\nHumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), DS-1000 (Lai et al., 2022), and GSM8K (Cobbe et al., 2021).\\nEventually, we filtered out 9 problems for OSS-INSTRUCT dataset and 89 for evol-codealpaca-v1.\\nB.3. Training\\nWe employ CODELLAMA-PYTHON-7B and DeepSeek-Coder-Base 6.7B as the base LLMs. To obtain Magicoder series, we\\nfirst finetune the base models on about 75K synthetic data generated through OSS-INSTRUCT using the transformers\\nlibrary from Hugging Face (Hugging Face, 2023). We finetune the base models for 2 epochs using two NVIDIA A100-80GB\\nGPUs through the Distributed Data Parallel (DDP) module from PyTorch. We set the initial learning rate at 5e-5 with 15\\nwarmup steps and a linear scheduler. We use Adafactor (Shazeer & Stern, 2018) as our optimizer and choose a batch size of\\n512 with a sequence truncation length of 1216. To obtain MagicoderS, we continue to finetune Magicoder models with the\\nevol-codealpaca-v1 dataset, an open-source Evol-Instruct implementation containing about 110K samples. We use\\nthe same hyperparameters except for 15 warmup steps and a 1024 maximum sequence length.\\nC. More Evaluation Results\\nC.1. Evaluation on APPS for Competitive Programming\\nWe additionally evaluate Magicoder on APPS (Hendrycks et al., 2021), a benchmark suite of competitive programming\\nproblems. Following Olausson et al. (2024), we select a subset of 300 problems from the APPS test set. From Table 8, we can\\nobserve that the CODELLAMA-PYTHON-based Magicoder-CL significantly outperforms the base model and WizardCoder\\x02CL. MagicoderS-CL-7B is even better than WizardCoder-SC-15B despite having less than half the number of parameters.\\n16\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nMeanwhile, DeepSeek-Coder-based MagicoderS-DS achieves the best result among all the evaluated baselines, substantially\\noutperforming the instruction-tuned DeepSeek-Coder-6.7B-Instruct.\\nTable 8: Pass@1 results on APPS evaluated using greedy decoding in a zero-shot setting.\\nModel Introductory (60) Interview (180) Competition (60) Overall (300)\\nWizardCoder-SC-15B 21.7 6.1 1.7 8.3\\nCODELLAMA-PYTHON-7B 3.3 2.8 0.0 2.3\\nWizardCoder-CL-7B 10.0 3.9 1.7 4.7\\nMagicoder-CL-7B 18.3 5.6 1.7 7.3\\nMagicoderS-CL-7B 23.3 6.1 1.7 8.7\\nDeepSeek-Coder-6.7B-Base 16.7 7.2 0.0 7.7\\nDeepSeek-Coder-6.7B-Instruct 23.3 9.4 0.0 10.3\\nMagicoder-DS-6.7B 20.0 8.9 1.7 9.7\\nMagicoderS-DS-6.7B 28.3 11.7 3.3 13.3\\nC.2. Fill-in-the-Middle Evaluation on DS-1000\\nTable 9 shows the evaluation results of Magicoder-DS and MagicoderS-DS on DS-1000 (Lai et al., 2022) (Insertion format),\\nassessing a model’s fill-in-the-middle capability. In this experiment, we use DeepSeek-Coder as the base model and exclude\\nCODELLAMA-PYTHON-based results, as CODELLAMA-PYTHON does not support the fill-in-the-middle format. The results\\nhighlight Magicoder’s superior performance in fill-in-the-middle tasks compared to all other evaluated baselines. This\\noutstanding capability suggests that Magicoder can serve as a valuable copilot for developers.\\nTable 9: Pass@1 results on DS-1000 (Insertion format) with temperature = 0.2, top p = 0.5, max length = 1024,\\nand num samples = 40.\\nModel NumPy Pandas PyTorch SciPy Sklearn TensorFlow Overall\\nWizardCoder-SC-15B 35.1 20.4 30.4 28.9 32.3 37.8 28.6\\nDeepSeek-Coder-6.7B-Base 36.3 28.6 15.8 19.3 32.8 35.1 29.3\\nDeepSeek-Coder-6.7B-Instruct 44.1 27.3 38.2 30.8 38.4 29.6 34.6\\nMagicoder-DS-6.7B 39.7 31.2 27.4 23.7 44.6 30.2 33.9\\nMagicoderS-DS-6.7B 43.3 29.5 39.2 26.2 44.5 36.2 35.9\\nC.3. Impact of Removing Noisy Data\\nIn §2.2, we highlight the benefits of preserving certain types of noise in instruction-tuning data. To support this argument, we\\nremoved data samples with partially implemented code, such as those containing pass or NotImplemented tokens in\\nPython problems. Table 10 shows the pass@1 performance of CODELLAMA-PYTHON-7B on HumanEval+ when finetuned\\nfor 2 epochs on the raw OSS-INSTRUCT dataset compared to the same dataset with noisy samples removed. We can observe\\nthat retaining the noisy samples results in slightly better performance.\\nTable 10: Pass@1 on HumanEval+ when finetuning CODELLAMA-PYTHON-7B with and without noisy samples.\\nSetting Dataset Size HumanEval+\\nWith noisy data 75K 55.5\\nWithout noisy data 68K 54.9\\n17\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD. Cases where OSS-INSTRUCT Improves Code Understanding\\nIn this section, we compare the base model, DeepSeek-Coder-6.7B-Base, and its OSS-INSTRUCT-tuned version, Magicoder\\x02DS-6.7B, in both in-domain and out-of-domain scenarios and demonstrate how OSS-INSTRUCT helps improve the base\\nmodel’s instruction following capability. We provide the base model with an appropriate prompt prefix to make sure it can\\nproduce a plausible output format.\\nD.1. More Precise Understanding of Requirements\\nThe following example shows that after instruction tuning, the model can understand the requirement “Check whether the\\ninput is valid” more precisely. DeepSeek-Coder-6.7B-Base only considers the integer case, while Magicoder-DS-6.7B\\neffectively checks if the parameter is of the correct type.\\nPrompt: Write a function that takes an integer a and returns True if this integer is a cube of some integer number. Note:\\nyou should check whether the input is valid.\\ndef is_cube(a):\\nif a <= 0:\\nreturn False\\nx = int(a ** (1/3))\\nreturn x**3 == a\\nDeepSeek-Coder-6.7B-Base\\ndef is_cube(a):\\nif not isinstance(a, int) or a < 0:\\nreturn False # Invalid input\\n# Check if the number is a perfect cube\\nx = int(a ** (1/3))\\nreturn x**3 == a\\nMagicoder-DS-6.7B\\nD.2. Handling Corner Cases\\nThe example below indicates that Magicoder-DS-6.7B can correctly handle the corner case, where the input string has less\\nthan or equal to 2 characters, even if this is not explicitly stated in the prompt. This is also an out-of-domain example since\\nthe Scala programming language is not included in OSS-INSTRUCT dataset.\\nPrompt: Implement a Scala function that takes a string as input and returns a new string where all the characters are\\nreversed except the first and last characters. Reply only with the function.\\ndef reverseExceptFirstLast(str: String):\\nString = {\\nval chars = str.toCharArray\\nval first = chars(0)\\nval last = chars(chars.length - 1)\\nval middle = chars.slice(1, chars.length\\n- 1).reverse\\nfirst + middle + last\\n}\\nDeepSeek-Coder-6.7B-Base\\ndef reverseExceptFirstLast(input: String):\\nString = {\\nif (input.length <= 2) {\\ninput\\n} else {\\nval reversedMiddle = input.substring(1,\\ninput.length - 1).reverse\\ninput.charAt(0) + reversedMiddle +\\ninput.charAt(input.length - 1)\\n}\\n}\\nMagicoder-DS-6.7B\\nD.3. Generalizing to Unseen Tasks\\nWe also show that after instruction tuning, Magicoder-DS-6.7B can respond plausibly to unseen requests during training.\\nThis example asks the model to create an easy Python coding exercise, which the base model cannot interpret. Such kinds of\\ntasks are also not incorporated in the OSS-INSTRUCT dataset.\\n18\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nPrompt: Create an easy Python coding exercise\\ndef add_numbers(a, b):\\nreturn a + b\\nDeepSeek-Coder-6.7B-Base\\n# Given a list of integers, return the sum\\nof all the positive integers in the\\nlist.\\ndef sum_positive_integers(nums):\\nreturn sum(num for num in nums if num >\\n0)\\n# Test the function\\nnumbers = [1, -2, 3, -4, 5, -6, 7, -8, 9,\\n-10]\\nprint(sum_positive_integers(numbers)) #\\nOutput: 25\\nMagicoder-DS-6.7B\\nE. Generating Complex Programs\\nIn this section, we present the cases where the best performing MagicoderS-DS-6.7B is capable of generating real-world,\\ncomplex programs.\\nE.1. Implementing Snake Game in Python\\nPrompt: Write a snake game in Python using the turtle library (the game is created by Magicoder).\\nResponse: Here is a simple snake game in Python using the turtle library. This game is a basic version and does not\\ninclude many features that a full-fledged snake game would have.\\nimport turtle\\nimport time\\nimport random\\ndelay = 0.1\\n# Score\\nscore = 0\\nhigh_score = 0\\n# Set up the screen\\nwn = turtle.Screen()\\nwn.title(\"Snake Game by Magicoder\")\\nwn.bgcolor(\"green\")\\nwn.setup(width=600, height=600)\\nwn.tracer(0) # Turns off the screen updates\\n# Snake head\\nhead = turtle.Turtle()\\nhead.speed(0)\\nhead.shape(\"square\")\\nhead.color(\"black\")\\nhead.penup()\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Snake food\\nfood = turtle.Turtle()\\nfood.speed(0)\\nfood.shape(\"circle\")\\nfood.color(\"red\")\\nfood.penup()\\nfood.goto(0, 100)\\nsegments = []\\n# Pen\\npen = turtle.Turtle()\\npen.speed(0)\\npen.shape(\"square\")\\n19\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\npen.color(\"white\")\\npen.penup()\\npen.hideturtle()\\npen.goto(0, 260)\\npen.write(\"Score: 0 High Score: 0\", align=\"center\", font=(\"Courier\", 24, \"normal\"))\\n# Functions\\ndef go_up():\\nif head.direction!= \"down\":\\nhead.direction = \"up\"\\ndef go_down():\\nif head.direction!= \"up\":\\nhead.direction = \"down\"\\ndef go_left():\\nif head.direction!= \"right\":\\nhead.direction = \"left\"\\ndef go_right():\\nif head.direction!= \"left\":\\nhead.direction = \"right\"\\ndef move():\\nif head.direction == \"up\":\\ny = head.ycor()\\nhead.sety(y + 20)\\nif head.direction == \"down\":\\ny = head.ycor()\\nhead.sety(y - 20)\\nif head.direction == \"left\":\\nx = head.xcor()\\nhead.setx(x - 20)\\nif head.direction == \"right\":\\nx = head.xcor()\\nhead.setx(x + 20)\\n# Keyboard bindings\\nwn.listen()\\nwn.onkeypress(go_up, \"w\")\\nwn.onkeypress(go_down, \"s\")\\nwn.onkeypress(go_left, \"a\")\\nwn.onkeypress(go_right, \"d\")\\n# Main game loop\\nwhile True:\\nwn.update()\\n# Check for a collision with the border\\nif head.xcor() > 290 or head.xcor() < -290 or head.ycor() > 290 or head.ycor() < -290:\\ntime.sleep(1)\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Hide the segments\\nfor segment in segments:\\nsegment.goto(1000, 1000)\\n# Clear the segments list\\nsegments.clear()\\n# Reset the score\\nscore = 0\\n# Reset the delay\\ndelay = 0.1\\n# Update the score display\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24, \"\\n20\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nnormal\"))\\n# Check for a collision with the food\\nif head.distance(food) < 20:\\n# Move the food to a random spot\\nx = random.randint(-280, 280)\\ny = random.randint(-280, 280)\\nfood.goto(x, y)\\n# Add a segment\\nnew_segment = turtle.Turtle()\\nnew_segment.speed(0)\\nnew_segment.shape(\"square\")\\nnew_segment.color(\"grey\")\\nnew_segment.penup()\\nsegments.append(new_segment)\\n# Shorten the delay\\ndelay -= 0.001\\n# Increase the score\\nscore += 10\\nif score > high_score:\\nhigh_score = score\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24, \"\\nnormal\"))\\n# Move the end segments first in reverse order\\nfor index in range(len(segments) - 1, 0, -1):\\nx = segments[index - 1].xcor()\\ny = segments[index - 1].ycor()\\nsegments[index].goto(x, y)\\n# Move segment 0 to where the head is\\nif len(segments) > 0:\\nx = head.xcor()\\ny = head.ycor()\\nsegments[0].goto(x, y)\\nmove()\\n# Check for head collision with the body segments\\nfor segment in segments:\\nif segment.distance(head) < 20:\\ntime.sleep(1)\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Hide the segments\\nfor segment in segments:\\nsegment.goto(1000, 1000)\\n# Clear the segments list\\nsegments.clear()\\n# Reset the score\\nscore = 0\\n# Reset the delay\\ndelay = 0.1\\n# Update the score display\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24, \"\\nnormal\"))\\ntime.sleep(delay)\\nwn.mainloop()\\nThis game is a simple version of the classic snake game. The snake starts with a single segment and grows in size when it\\neats food. The game ends when the snake hits the border or its own body. The score is increased by 10 points for each\\npiece of food eaten. The game speed increases as the snake grows.\\n21\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nE.2. Building Othello Game in Java\\nPrompt: Build a console-based Othello game in Java with row and column numbers shown on the board. The game\\nshould end when there are no more valid moves for either player.\\nResponse: Here is a simple console-based Othello game in Java. It does not include the graphical interface, but it does\\ninclude the row and column numbers on the board.\\nimport java.util.Scanner;\\npublic class Othello {\\nprivate static final int BOARD_SIZE = 8;\\nprivate static final char EMPTY = ’-’;\\nprivate static final char BLACK = ’B’;\\nprivate static final char WHITE = ’W’;\\nprivate static final char[][] board = new char[BOARD_SIZE][BOARD_SIZE];\\nprivate static char currentPlayer = BLACK;\\npublic static void main(String[] args) {\\ninitializeBoard();\\nprintBoard();\\nwhile (true) {\\nSystem.out.println(\"Player \" + currentPlayer + \", enter your move (row column):\");\\nScanner scanner = new Scanner(System.in);\\nint row = scanner.nextInt();\\nint col = scanner.nextInt();\\nif (isValidMove(row, col)) {\\nmakeMove(row, col);\\nprintBoard();\\nif (!hasValidMove()) {\\nSystem.out.println(\"Player \" + currentPlayer + \" has no valid moves. Game over!\");\\nbreak;\\n}\\ncurrentPlayer = (currentPlayer == BLACK)? WHITE : BLACK;\\n} else {\\nSystem.out.println(\"Invalid move. Try again.\");\\n}\\n}\\n}\\nprivate static void initializeBoard() {\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nboard[i][j] = EMPTY;\\n}\\n}\\nboard[3][3] = board[4][4] = BLACK;\\nboard[3][4] = board[4][3] = WHITE;\\n}\\nprivate static void printBoard() {\\nSystem.out.print(\" \");\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nSystem.out.print(i + \" \");\\n}\\nSystem.out.println();\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nSystem.out.print(i + \" \");\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nSystem.out.print(board[i][j] + \" \");\\n}\\nSystem.out.println();\\n}\\n}\\nprivate static boolean isValidMove(int row, int col) {\\nif (board[row][col]!= EMPTY) {\\nreturn false;\\n}\\nfor (int i = -1; i <= 1; i++) {\\nfor (int j = -1; j <= 1; j++) {\\nif (i!= 0 || j!= 0) {\\nif (isValidDirection(row, col, i, j)) {\\nreturn true;\\n}\\n}\\n22\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\n}\\n}\\nreturn false;\\n}\\nprivate static boolean isValidDirection(int row, int col, int rowDir, int colDir) {\\nint newRow = row + rowDir;\\nint newCol = col + colDir;\\nwhile (newRow >= 0 && newRow < BOARD_SIZE && newCol >= 0 && newCol < BOARD_SIZE) {\\nif (board[newRow][newCol] == EMPTY) {\\nreturn false;\\n}\\nif (board[newRow][newCol] == currentPlayer) {\\nreturn true;\\n}\\nnewRow += rowDir;\\nnewCol += colDir;\\n}\\nreturn false;\\n}\\nprivate static void makeMove(int row, int col) {\\nboard[row][col] = currentPlayer;\\nfor (int i = -1; i <= 1; i++) {\\nfor (int j = -1; j <= 1; j++) {\\nif (i!= 0 || j!= 0) {\\nflipDirection(row, col, i, j);\\n}\\n}\\n}\\n}\\nprivate static void flipDirection(int row, int col, int rowDir, int colDir) {\\nint newRow = row + rowDir;\\nint newCol = col + colDir;\\nwhile (newRow >= 0 && newRow < BOARD_SIZE && newCol >= 0 && newCol < BOARD_SIZE) {\\nif (board[newRow][newCol] == EMPTY) {\\nreturn;\\n}\\nif (board[newRow][newCol] == currentPlayer) {\\nwhile (true) {\\nnewRow -= rowDir;\\nnewCol -= colDir;\\nif (newRow == row && newCol == col) {\\nbreak;\\n}\\nboard[newRow][newCol] = currentPlayer;\\n}\\nreturn;\\n}\\nnewRow += rowDir;\\nnewCol += colDir;\\n}\\n}\\nprivate static boolean hasValidMove() {\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nif (isValidMove(i, j)) {\\nreturn true;\\n}\\n}\\n}\\nreturn false;\\n}\\n}\\nThis program starts with a 8x8 board with two pieces in the center of the board, one black and one white. The player who\\ngoes first is black. The game ends when there are no more valid moves for either player. The player enters their move as\\ntwo numbers, the row and the column. If the move is valid, the game makes the move and switches the player. If the\\nmove is not valid, the player is asked to enter a valid move.\\nE.3. Writing Specific Gradio Application\\n23\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nPrompt: Write a gradio (3.48.0) application for the following use case: Take an input image and return a 45 degree\\nclockwise rotated image. You should also add text description under the output showing the rotation degree.\\nResponse: Here is a simple snake game in Python using the turtle library. This game is a basic version and does not\\ninclude many features that a full-fledged snake game would have.\\nimport gradio as gr\\nimport numpy as np\\nfrom PIL import Image\\ndef rotate_image(img):\\nimg = Image.fromarray(img)\\nrotated_img = img.rotate(-45)\\nreturn np.array(rotated_img), \"45 degree clockwise rotation\"\\ndemo = gr.Interface(\\nfn=rotate_image,\\ninputs=gr.inputs.Image(type=\"numpy\"),\\noutputs=[gr.outputs.Image(type=\"numpy\"), \"text\"],\\ntitle=\"Image Rotation\",\\ndescription=\"Upload an image and get a 45 degree clockwise rotated image\",\\n)\\ndemo.launch()\\nThis script defines a function rotate_image that takes an image as input, rotates it 45 degrees clockwise using the PIL\\nlibrary, and returns the rotated image and a text description. The gr.Interface function is then used to create a Gradio\\ninterface for this function.\\nThe gr.inputs.Image(type=\"numpy\") line specifies that the input to the function should be an image, and the\\ngr.outputs.Image(type=\"numpy\") line specifies that the output should be an image. The ”text” output type is\\nused to display the text description.\\nFinally, demo.launch() is used to launch the Gradio interface.\\nE.4. Developing Machine Learning Pipeline with PyTorch\\nPrompt: Build a simple neural network in Python using Pytorch to classify handwritten digits from the MNIST dataset.\\nYou should use CNN as the model structure, train the model for 5 epochs, draw a chart of the training loss, and show the\\nfinal result.\\nResponse: Here is a simple implementation of a Convolutional Neural Network (CNN) using PyTorch to classify\\nhandwritten digits from the MNIST dataset.\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\n# Device configuration\\ndevice = torch.device(’cuda’ if torch.cuda.is_available() else ’cpu’)\\n# Hyper-parameters\\nnum_epochs = 5\\nbatch_size = 100\\nlearning_rate = 0.001\\n# MNIST dataset\\ntrain_dataset = torchvision.datasets.MNIST(root=’../../data’,\\ntrain=True,\\ntransform=transforms.ToTensor(),\\ndownload=True)\\ntest_dataset = torchvision.datasets.MNIST(root=’../../data’,\\ntrain=False,\\ntransform=transforms.ToTensor())\\n# Data loader\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\\nbatch_size=batch_size,\\n24\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nshuffle=True)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\\nbatch_size=batch_size,\\nshuffle=False)\\n# Convolutional neural network (two convolutional layers)\\nclass ConvNet(nn.Module):\\ndef __init__(self):\\nsuper(ConvNet, self).__init__()\\nself.layer1 = nn.Sequential(\\nnn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\\nnn.BatchNorm2d(16),\\nnn.ReLU(),\\nnn.MaxPool2d(kernel_size=2, stride=2))\\nself.layer2 = nn.Sequential(\\nnn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\\nnn.BatchNorm2d(32),\\nnn.ReLU(),\\nnn.MaxPool2d(kernel_size=2, stride=2))\\nself.fc = nn.Linear(7*7*32, 10)\\ndef forward(self, x):\\nout = self.layer1(x)\\nout = self.layer2(out)\\nout = out.reshape(out.size(0), -1)\\nout = self.fc(out)\\nreturn out\\nmodel = ConvNet().to(device)\\n# Loss and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\n# Train the model\\ntotal_step = len(train_loader)\\nloss_list = []\\nfor epoch in range(num_epochs):\\nfor i, (images, labels) in enumerate(train_loader):\\nimages = images.to(device)\\nlabels = labels.to(device)\\n# Forward pass\\noutputs = model(images)\\nloss = criterion(outputs, labels)\\n# Backward and optimize\\noptimizer.zero_grad()\\nloss.backward()\\noptimizer.step()\\nif (i+1) %\\nprint (’Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}’\\n.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\\nloss_list.append(loss.item())\\n# Test the model\\nmodel.eval() # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\\nwith torch.no_grad():\\ncorrect = 0\\ntotal = 0\\nfor images, labels in test_loader:\\nimages = images.to(device)\\nlabels = labels.to(device)\\noutputs = model(images)\\n_, predicted = torch.max(outputs.data, 1)\\ntotal += labels.size(0)\\ncorrect += (predicted == labels).sum().item()\\nprint(’Test Accuracy of the model on the 10000 test images: {} %\\n# Draw a chart of the training loss\\nplt.plot(loss_list)\\nplt.title(’Training Loss’)\\nplt.xlabel(’Epoch’)\\nplt.ylabel(’Loss’)\\nplt.show()\\n25\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nThis script first loads the MNIST dataset and then defines a Convolutional Neural Network (CNN) with two convolutional\\nlayers. The CNN is trained for 5 epochs using the Adam optimizer and the Cross Entropy Loss function. After training,\\nthe model is tested on the test dataset and the accuracy is printed. Finally, a chart of the training loss is drawn.\\nF. Limitations\\nWhile Magicoders prove to be effective by being instruction-tuned with OSS-INSTRUCT and Evol-Instruct generated data,\\nwe apply our techniques to models with no more than 7B parameters due to resource constraints. This limitation hinders our\\nability to observe the method’s impact on larger models. Moreover, the OSS-INSTRUCT prompt is sophisticated and may\\nrequire a relatively strong model to comprehend the correct intent. Weaker models may fail to produce plausible instruction\\ndata. Future research may explore applying OSS-INSTRUCT to models at different capacities in the context of self-training.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Magicoder Source Code Is All You Need\""
      ],
      "metadata": {
        "id": "a_3fgTBAWiA9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/magicoder.txt.txt\"\n",
        "author = \"Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang \"\n",
        "URL =\"https://arxiv.org/pdf/2312.02120\""
      ],
      "metadata": {
        "id": "_bO7ykmHWs_w"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,TokenTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "DKoOGBm8W-g1"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=320, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "mlMMvPZmXGjv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_text = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "1qRJLHmJXMsU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "DEAh2hRcXSla",
        "outputId": "37f04aaa-1ae8-4cdf-c1b2-e244e8f5b6c5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We introduce Magicoder, a series of fully open\\x02source (code, weights, and data) Large Language\\nModels (LLMs) for code that significantly closes\\nthe gap with top code models while having no\\nmore than 7B parameters. Magicoder models are\\ntrained on 75K synthetic instruction data using\\nOSS-INSTRUCT, a novel approach to enlighten\\x02ing LLMs with open-source code snippets to gen\\x02erate diverse instruction data for code. Our main\\nmotivation is to mitigate the inherent bias of the\\nsynthetic data generated by LLMs through the\\nwealth of open-source references for the produc\\x02tion of more realistic and controllable data. The\\northogonality of OSS-INSTRUCT and other data\\ngeneration methods like Evol-Instruct further en\\x02ables us to build an enhanced MagicoderS. Both\\nMagicoder and MagicoderS substantially outper\\x02form state-of-the-art code models with similar or\\neven larger sizes on a wide range of coding bench\\x02marks. Notably, MagicoderS-CL-7B based on\\nCODELLAMA even surpasses the prominent Chat\\x02GPT on HumanEval+ (66.5 vs. 65.9 in pass@1).\\nOverall, OSS-INSTRUCT opens a new direction\\nfor crafting diverse synthetic instruction data for\\ncode using abundant open-source references.\\n1. Introduction\\nCode generation, also known as program synthesis (Gul\\x02wani et al., 2017), is a long-standing challenge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = []\n"
      ],
      "metadata": {
        "id": "V7H83fQcXT03"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in trange(0,len(splitted_text)):\n",
        "  text = splitted_text[i]\n",
        "  keys.append({'document': filename,\n",
        "               'title':title,\n",
        "               'author':author,\n",
        "               'url':URL,\n",
        "               'doc':text,\n",
        "               'keywords': extract_keys(text,1,0.5)}\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a3b2271b41b54b0daeb0cba932d7dbf5",
            "c15e861395e84f1bb45e5c473ba2a6b0"
          ]
        },
        "id": "Lo8GgecIX2V2",
        "outputId": "d80e90b9-f4c5-4e22-8c67-5d26eba81b1f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3b2271b41b54b0daeb0cba932d7dbf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "We introduce Magicoder series of fully open source code weights and data Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for code that \n",
              "significantly closes the gap with top code models while having no more than 7B parameters Magicoder models are \n",
              "trained on \u001b[30;48;2;255;255;0m75K\u001b[0m \u001b[30;48;2;255;255;0msynthetic\u001b[0m instruction data using OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m novel approach to enlighten ing \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source \n",
              "code snippets to gen erate diverse instruction data for code Our main motivation is to mitigate the inherent bias \n",
              "of the \u001b[30;48;2;255;255;0msynthetic\u001b[0m data generated by \u001b[30;48;2;255;255;0mLLMs\u001b[0m through the wealth of open source references for the produc tion of more \n",
              "realistic and controllable data The orthogonality of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and other data generation methods like Evol \n",
              "\u001b[30;48;2;255;255;0mInstruct\u001b[0m further en ables us to build an enhanced \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m Both Magicoder and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m substantially outper \n",
              "form state of the art code models with similar or even larger sizes on wide range of coding bench marks Notably \n",
              "\u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL 7B based on CODELLAMA even surpasses the prominent Chat GPT on HumanEval 66 vs 65 in pass Overall OSS\n",
              "\u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m opens new direction for crafting diverse \u001b[30;48;2;255;255;0msynthetic\u001b[0m instruction data for code using abundant open source \n",
              "references Introduction Code generation also known as program synthesis Gul wani et al 2017 is long standing \n",
              "challenge\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">We introduce Magicoder series of fully open source code weights and data Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for code that \n",
              "significantly closes the gap with top code models while having no more than 7B parameters Magicoder models are \n",
              "trained on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">75K</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> instruction data using OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> novel approach to enlighten ing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source \n",
              "code snippets to gen erate diverse instruction data for code Our main motivation is to mitigate the inherent bias \n",
              "of the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data generated by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> through the wealth of open source references for the produc tion of more \n",
              "realistic and controllable data The orthogonality of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and other data generation methods like Evol \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> further en ables us to build an enhanced <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> Both Magicoder and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> substantially outper \n",
              "form state of the art code models with similar or even larger sizes on wide range of coding bench marks Notably \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL 7B based on CODELLAMA even surpasses the prominent Chat GPT on HumanEval 66 vs 65 in pass Overall OSS\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> opens new direction for crafting diverse <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> instruction data for code using abundant open source \n",
              "references Introduction Code generation also known as program synthesis Gul wani et al 2017 is long standing \n",
              "challenge\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "in com puter science In the past few decades large body of research has been studying symbolic approaches such as \n",
              "abstraction \u001b[30;48;2;255;255;0mbased\u001b[0m synthesis Wang et al 2017 Feng et al 2018 for general purpose synthesis problems and program The \n",
              "work was done during remote summer internship at the University of Illinois 1University of Illinois at Urbana \n",
              "Champaign USA 2Tsinghua University China Correspondence to Yuxiang Wei ywei40 illinois edu Proceedings of the 41 st\n",
              "International Conference on Machine Learning Vienna Austria PMLR 235 2024 Copyright 2024 by the author ming by \n",
              "examples Cambronero et al 2023 Liu et al 2023a for domain specific tasks Until recently Large Lan guage Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m\n",
              "trained on code Austin et al 2021 Chen et al 2021 has shown outstanding breakthroughs in generating code that \n",
              "accurately satisfies user intents and they are widely deployed to assist real world software de velopment Microsoft\n",
              "2023b Services 2023 Initially closed source models such as GPT Turbo Ope nAI \u001b[30;48;2;255;255;0m2022\u001b[0m \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and GPT OpenAI 2023 mas \n",
              "sively dominated various coding \u001b[30;48;2;255;255;0mbenchmarks\u001b[0m and leader boards Chen et al\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">in com puter science In the past few decades large body of research has been studying symbolic approaches such as \n",
              "abstraction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">based</span> synthesis Wang et al 2017 Feng et al 2018 for general purpose synthesis problems and program The \n",
              "work was done during remote summer internship at the University of Illinois 1University of Illinois at Urbana \n",
              "Champaign USA 2Tsinghua University China Correspondence to Yuxiang Wei ywei40 illinois edu Proceedings of the 41 st\n",
              "International Conference on Machine Learning Vienna Austria PMLR 235 2024 Copyright 2024 by the author ming by \n",
              "examples Cambronero et al 2023 Liu et al 2023a for domain specific tasks Until recently Large Lan guage Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span>\n",
              "trained on code Austin et al 2021 Chen et al 2021 has shown outstanding breakthroughs in generating code that \n",
              "accurately satisfies user intents and they are widely deployed to assist real world software de velopment Microsoft\n",
              "2023b Services 2023 Initially closed source models such as GPT Turbo Ope nAI <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and GPT OpenAI 2023 mas \n",
              "sively dominated various coding <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">benchmarks</span> and leader boards Chen et al\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2021 Austin et al 2021 Liu et al 2023b Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m Xia Zhang 2023 To further push the boundaries of code \n",
              "generation with open source LLMs SELF INSTRUCT Wang et al 2023a is adopted to boot strap the \u001b[30;48;2;255;255;0minstruction\u001b[0m following \n",
              "ability of LLMs In the realm of code practitioners commonly devise synthetic coding instructions using stronger \n",
              "teacher model \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and GPT and then finetune weaker student model \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m Roziere et al 2023 with the \n",
              "generated data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For example Code Alpaca \n",
              "Chaudhary 2023 consists of 20K automatically generated code instruc tions by applying SELF INSTRUCT on \u001b[30;48;2;255;255;0mChatGPT\u001b[0m \n",
              "using 21 seed tasks To further enhance the coding abilities of LLMs Luo et al 2023b proposes Code Evol Instruct \n",
              "that employs various heuristics to \u001b[30;48;2;255;255;0mincrease\u001b[0m the complexity of seed code instructions Code Alpaca in this case \n",
              "achieving state of the art SOTA results among open source models While these data generation methods can \n",
              "effectively im prove the \u001b[30;48;2;255;255;0minstruction\u001b[0m following capability of an LLM\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2021 Austin et al 2021 Liu et al 2023b Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> Xia Zhang 2023 To further push the boundaries of code \n",
              "generation with open source LLMs SELF INSTRUCT Wang et al 2023a is adopted to boot strap the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> following \n",
              "ability of LLMs In the realm of code practitioners commonly devise synthetic coding instructions using stronger \n",
              "teacher model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and GPT and then finetune weaker student model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> Roziere et al 2023 with the \n",
              "generated data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For example Code Alpaca \n",
              "Chaudhary 2023 consists of 20K automatically generated code instruc tions by applying SELF INSTRUCT on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> \n",
              "using 21 seed tasks To further enhance the coding abilities of LLMs Luo et al 2023b proposes Code Evol Instruct \n",
              "that employs various heuristics to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">increase</span> the complexity of seed code instructions Code Alpaca in this case \n",
              "achieving state of the art SOTA results among open source models While these data generation methods can \n",
              "effectively im prove the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> following capability of an LLM\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "they rely on narrow range of predefined tasks or heuristics under the hood For example on the one hand Code Al paca\n",
              "that adopts SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m only relies on 21 seed tasks to generate new code instructions using an identical prompt \n",
              "template On the other hand Code Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m takes Code \u001b[30;48;2;255;255;0mAlpaca\u001b[0m as seeds and merely depends on heuris tics to \n",
              "evolve the dataset As partly suggested by Yu et al 2023 and Wang et al 2023a such approaches may sig nificantly \n",
              "inherit the system bias inherent in the \u001b[30;48;2;255;255;0mLLMs\u001b[0m as well as the predefined tasks Therefore in this paper we propose OSS\n",
              "\u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m to mitigate the inherent bias of \u001b[30;48;2;255;255;0mLLMs\u001b[0m and to unleash their potential to craft diverse and creative code \n",
              "instructions via arXiv 2312 02120v2 cs CL Jun 2024 Magicoder Empowering Code Generation with OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m You are \n",
              "working on natural language processing NLP project and need to create program to preprocess and classify movie \n",
              "reviews Your program should be able to preprocess new movie reviews train the model and classify new reviews \n",
              "accurately \u001b[30;48;2;255;255;0mGenerated\u001b[0m problem details omitted \u001b[30;48;2;255;255;0mDPosNeg\u001b[0m py DLog cpp DGrantInfo ts DProgram cs DStrength swift\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">they rely on narrow range of predefined tasks or heuristics under the hood For example on the one hand Code Al paca\n",
              "that adopts SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> only relies on 21 seed tasks to generate new code instructions using an identical prompt \n",
              "template On the other hand Code Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> takes Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> as seeds and merely depends on heuris tics to \n",
              "evolve the dataset As partly suggested by Yu et al 2023 and Wang et al 2023a such approaches may sig nificantly \n",
              "inherit the system bias inherent in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> as well as the predefined tasks Therefore in this paper we propose OSS\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> to mitigate the inherent bias of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> and to unleash their potential to craft diverse and creative code \n",
              "instructions via arXiv 2312 02120v2 cs CL Jun 2024 Magicoder Empowering Code Generation with OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> You are \n",
              "working on natural language processing NLP project and need to create program to preprocess and classify movie \n",
              "reviews Your program should be able to preprocess new movie reviews train the model and classify new reviews \n",
              "accurately <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Generated</span> problem details omitted <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DPosNeg</span> py DLog cpp DGrantInfo ts DProgram cs DStrength swift\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Open source codebase \u001b[30;48;2;255;255;0mlearn_model\u001b[0m \u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB target def get_clean_review raw_review letters_only re sub zA \n",
              "raw_review DSeed code snippet OSS INSTRUCT Please gain inspiration from the code snippet to create high quality \n",
              "programming problem Prompt details omitted from \u001b[30;48;2;255;255;0msklearn\u001b[0m feature_extraction text import TfidfVectorizer def \n",
              "get_clean_review raw_review def train_model \u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB reviews labels def classify_review clean_review \n",
              "\u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB train_model \u001b[30;48;2;255;255;0mtf_idfSVM\u001b[0m tf_idfNB reviews labels \u001b[30;48;2;255;255;0mcleaned_review\u001b[0m get_clean_review Generated solution\n",
              "details omitted \u001b[30;48;2;255;255;0msLanguage\u001b[0m Model Figure Overview of OSS INSTRUCT and the pass results of different LLMs on HumanEval\n",
              "direct learning from the open source As shown in Figure OSS INSTRUCT leverages powerful LLM to automati cally \n",
              "generate new coding problems by drawing inspira tion from any random code snippets collected from the open source \n",
              "In this example the LLM\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Open source codebase <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">learn_model</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB target def get_clean_review raw_review letters_only re sub zA \n",
              "raw_review DSeed code snippet OSS INSTRUCT Please gain inspiration from the code snippet to create high quality \n",
              "programming problem Prompt details omitted from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sklearn</span> feature_extraction text import TfidfVectorizer def \n",
              "get_clean_review raw_review def train_model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB reviews labels def classify_review clean_review \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB train_model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tf_idfSVM</span> tf_idfNB reviews labels <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">cleaned_review</span> get_clean_review Generated solution\n",
              "details omitted <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sLanguage</span> Model Figure Overview of OSS INSTRUCT and the pass results of different LLMs on HumanEval\n",
              "direct learning from the open source As shown in Figure OSS INSTRUCT leverages powerful LLM to automati cally \n",
              "generate new coding problems by drawing inspira tion from any random code snippets collected from the open source \n",
              "In this example the LLM\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gets inspired by two incomplete code fragments from different functions and manages to relate them and craft \n",
              "realistic machine learning problem Thanks to the \u001b[30;48;2;255;255;0minfinite\u001b[0m real world open source code OSS INSTRUCT can directly \n",
              "produce diverse realistic and controllable code instructions by providing distinct seed code snippets In the end we\n",
              "generate 75K synthetic data to finetune CODELLAMA PYTHON 7B re sulting in \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL While being simple and \n",
              "effective OSS INSTRUCT is orthogonal to existing data generation methods and they can be combined to further boost \n",
              "the models coding capabilities Therefore we continually fine tune \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL on an open source Evol Instruct \n",
              "dataset with 110K entries producing MagicoderS CL We evaluate \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS on wide range of coding \n",
              "tasks including HumanEval Chen et al 2021 and \u001b[30;48;2;255;255;0mMBPP\u001b[0m Austin et al 2021 for Python text to code gen eration MultiPL \n",
              "Cassano et al \u001b[30;48;2;255;255;0m2022\u001b[0m for multilingual code completion and DS 1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m for solving data science problems We\n",
              "further adopt \u001b[30;48;2;255;255;0mEvalPlus\u001b[0m Liu et al 2023b which includes the augmented HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m datasets for more rigorous \n",
              "model evaluation Both \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL and\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">gets inspired by two incomplete code fragments from different functions and manages to relate them and craft \n",
              "realistic machine learning problem Thanks to the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">infinite</span> real world open source code OSS INSTRUCT can directly \n",
              "produce diverse realistic and controllable code instructions by providing distinct seed code snippets In the end we\n",
              "generate 75K synthetic data to finetune CODELLAMA PYTHON 7B re sulting in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL While being simple and \n",
              "effective OSS INSTRUCT is orthogonal to existing data generation methods and they can be combined to further boost \n",
              "the models coding capabilities Therefore we continually fine tune <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL on an open source Evol Instruct \n",
              "dataset with 110K entries producing MagicoderS CL We evaluate <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS on wide range of coding \n",
              "tasks including HumanEval Chen et al 2021 and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> Austin et al 2021 for Python text to code gen eration MultiPL \n",
              "Cassano et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> for multilingual code completion and DS 1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> for solving data science problems We\n",
              "further adopt <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvalPlus</span> Liu et al 2023b which includes the augmented HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> datasets for more rigorous \n",
              "model evaluation Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL and\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL substantially boost the base CODELLAMA PYTHON 7B Additionally Magicoder CL even outperforms \n",
              "WizardCoder CL 7B WizardCoder SC 15B and all studied SOTA \u001b[30;48;2;255;255;0mLLMs\u001b[0m with less than or equal to 16B parameters on all the\n",
              "benchmarks we tested Also the pass result of the enhanced \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL is on par with \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 70 vs \n",
              "72 and sur passes it on the more rigorous HumanEval 66 vs 65 indicating that \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL can generate more robust\n",
              "code It also achieves SOTA results among all code models at the same scale Additionally we notice very recent \n",
              "advancement in the development of the \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder series Guo et al 2024 which has shown exceptional \u001b[30;48;2;255;255;0mcoding\u001b[0m \n",
              "performance However due to the limited technical details disclosed we only briefly discuss them in Despite this we \n",
              "applied OSS INSTRUCT on \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Base 7B resulting in the creation of Magicoder DS and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS In \n",
              "addition to the consistent findings on the previous results with CODELLAMA PYTHON 7B as the base model Magicoder DS\n",
              "and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS benefit from the more powerful DeepSe\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL substantially boost the base CODELLAMA PYTHON 7B Additionally Magicoder CL even outperforms \n",
              "WizardCoder CL 7B WizardCoder SC 15B and all studied SOTA <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with less than or equal to 16B parameters on all the\n",
              "benchmarks we tested Also the pass result of the enhanced <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL is on par with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 70 vs \n",
              "72 and sur passes it on the more rigorous HumanEval 66 vs 65 indicating that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL can generate more robust\n",
              "code It also achieves SOTA results among all code models at the same scale Additionally we notice very recent \n",
              "advancement in the development of the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder series Guo et al 2024 which has shown exceptional <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> \n",
              "performance However due to the limited technical details disclosed we only briefly discuss them in Despite this we \n",
              "applied OSS INSTRUCT on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Base 7B resulting in the creation of Magicoder DS and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS In \n",
              "addition to the consistent findings on the previous results with CODELLAMA PYTHON 7B as the base model Magicoder DS\n",
              "and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS benefit from the more powerful DeepSe\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ek Coder Base 7B This advantage is demonstrated by MagicoderS DS which achieves remark able 76 pass on HumanEval \n",
              "MagicoderS DS also out performs DeepSeek Coder Instruct 7B on HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m with less finetuning tokens To \n",
              "justify the design of OSS INSTRUCT generating \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT instruction \n",
              "\u001b[30;48;2;255;255;0mtuning\u001b[0m data from open source references rather than using the references directly we demonstrate that fine \u001b[30;48;2;255;255;0mtuning\u001b[0m \n",
              "the base models with semantically \u001b[30;48;2;255;255;0mrelevant\u001b[0m comment function pairs extracted from open source projects even \n",
              "negatively impacts the model performance In general we make the following contributions We introduce OSS INSTRUCT \n",
              "pioneering approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source code snippets to generate more diverse realistic and \n",
              "controllable coding instruction data which can be leveraged to substantially boost the performance of various \u001b[30;48;2;255;255;0mLLMs\u001b[0m \n",
              "via instruction \u001b[30;48;2;255;255;0mtuning\u001b[0m It opens new dimension for creating low bias and diverse instruction \u001b[30;48;2;255;255;0mtuning\u001b[0m data from the \n",
              "abundance of open source references We build the \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series trained with OSS INSTRUCT and MagicoderS series \n",
              "trained on combi nation of OSS INSTRUCT and Evol\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ek Coder Base 7B This advantage is demonstrated by MagicoderS DS which achieves remark able 76 pass on HumanEval \n",
              "MagicoderS DS also out performs DeepSeek Coder Instruct 7B on HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> with less finetuning tokens To \n",
              "justify the design of OSS INSTRUCT generating <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT instruction \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tuning</span> data from open source references rather than using the references directly we demonstrate that fine <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tuning</span> \n",
              "the base models with semantically <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">relevant</span> comment function pairs extracted from open source projects even \n",
              "negatively impacts the model performance In general we make the following contributions We introduce OSS INSTRUCT \n",
              "pioneering approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source code snippets to generate more diverse realistic and \n",
              "controllable coding instruction data which can be leveraged to substantially boost the performance of various <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> \n",
              "via instruction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tuning</span> It opens new dimension for creating low bias and diverse instruction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tuning</span> data from the \n",
              "abundance of open source references We build the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series trained with OSS INSTRUCT and MagicoderS series \n",
              "trained on combi nation of OSS INSTRUCT and Evol\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mInstruct\u001b[0m Our eval uation across benchmarks shows that all Magicoders significantly improve the base \u001b[30;48;2;255;255;0mLLMs\u001b[0m Notably \n",
              "both MagicoderS CL and MagicoderS DS outperform Chat GPT on HumanEval with only 7B parameters We fully open source \n",
              "the model weights training data and source code at https github com ise uiuc \u001b[30;48;2;255;255;0mmagicoder\u001b[0m to facilitate future \n",
              "research OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m Instruction Tuning from Open Source In this section we elaborate on our OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m ap \n",
              "proach From high level as shown in Figure OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m works by prompting an LLM \u001b[30;48;2;255;255;0mChatGPT\u001b[0m to generate coding problem\n",
              "and its solution according to some \u001b[30;48;2;255;255;0mseed\u001b[0m code snippet collected from the wild from GitHub The \u001b[30;48;2;255;255;0mseed\u001b[0m snippet offers \n",
              "controllability of the gen eration and encourages the LLM to create diverse coding problems that can reflect real \n",
              "world programming scenarios Generating Coding Problems OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m is powered by \u001b[30;48;2;255;255;0mseed\u001b[0m code snippets that can be \n",
              "easily collected from open source In this work we directly adopt starcoderdata as our \u001b[30;48;2;255;255;0mseed\u001b[0m corpus fil tered version\n",
              "of The Stack Kocetkov et al 2022 dataset that StarCoder is trained on containing permissively li censed source code\n",
              "documents in various programming lan guages\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> Our eval uation across benchmarks shows that all Magicoders significantly improve the base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> Notably \n",
              "both MagicoderS CL and MagicoderS DS outperform Chat GPT on HumanEval with only 7B parameters We fully open source \n",
              "the model weights training data and source code at https github com ise uiuc <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">magicoder</span> to facilitate future \n",
              "research OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> Instruction Tuning from Open Source In this section we elaborate on our OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> ap \n",
              "proach From high level as shown in Figure OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> works by prompting an LLM <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> to generate coding problem\n",
              "and its solution according to some <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">seed</span> code snippet collected from the wild from GitHub The <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">seed</span> snippet offers \n",
              "controllability of the gen eration and encourages the LLM to create diverse coding problems that can reflect real \n",
              "world programming scenarios Generating Coding Problems OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> is powered by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">seed</span> code snippets that can be \n",
              "easily collected from open source In this work we directly adopt starcoderdata as our <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">seed</span> corpus fil tered version\n",
              "of The Stack Kocetkov et al 2022 dataset that StarCoder is trained on containing permissively li censed source code\n",
              "documents in various programming lan guages\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "We chose \u001b[30;48;2;255;255;0mstarcoderdata\u001b[0m because it is widely adopted includes massive high quality code snippets and is even post \n",
              "processed for data decontamination Li et al \u001b[30;48;2;255;255;0m2023\u001b[0m Allal et al \u001b[30;48;2;255;255;0m2023\u001b[0m For each code document from the corpus we \n",
              "randomly extract 15 consecutive lines as the seed snippet for the model to gain inspiration from and produce coding\n",
              "problems In total we collected 80K initial seed snippets from 80K code documents 40K from Python and 5K from each \n",
              "of Java TypeScript Shell Rust \u001b[30;48;2;255;255;0mPHP\u001b[0m and Swift respectively Then each collected seed code snippet is applied to the \n",
              "prompt template shown in Appendix which teacher model takes as input and outputs both coding problem and its \n",
              "solution Data Cleaning and Decontamination We perform data cleaning by excluding \u001b[30;48;2;255;255;0msamples\u001b[0m that are identical or \n",
              "share the same seed code snippet While there exist other sorts of noisiness the solution is incom plete in the \n",
              "generated data inspired by Honovich et al \u001b[30;48;2;255;255;0m2023\u001b[0m they are not removed as we believe they still con tain valuable \n",
              "information for \u001b[30;48;2;255;255;0mLLMs\u001b[0m to learn More experi mental details can be found in Appendix Finally we apply the same logic \n",
              "as StarCoder Li et al \u001b[30;48;2;255;255;0m2023\u001b[0m to decon taminate our training data by removing coding problems\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">We chose <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">starcoderdata</span> because it is widely adopted includes massive high quality code snippets and is even post \n",
              "processed for data decontamination Li et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Allal et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> For each code document from the corpus we \n",
              "randomly extract 15 consecutive lines as the seed snippet for the model to gain inspiration from and produce coding\n",
              "problems In total we collected 80K initial seed snippets from 80K code documents 40K from Python and 5K from each \n",
              "of Java TypeScript Shell Rust <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PHP</span> and Swift respectively Then each collected seed code snippet is applied to the \n",
              "prompt template shown in Appendix which teacher model takes as input and outputs both coding problem and its \n",
              "solution Data Cleaning and Decontamination We perform data cleaning by excluding <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">samples</span> that are identical or \n",
              "share the same seed code snippet While there exist other sorts of noisiness the solution is incom plete in the \n",
              "generated data inspired by Honovich et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> they are not removed as we believe they still con tain valuable \n",
              "information for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> to learn More experi mental details can be found in Appendix Finally we apply the same logic \n",
              "as StarCoder Li et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> to decon taminate our training data by removing coding problems\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "that contain docstrings or solutions from HumanEval Chen et al 2021 and MBPP Austin et al 2021 docstrings from APPS\n",
              "Hendrycks et al 2021 prompts from DS 1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m or questions from GSM8K Cobbe et al 2021 As part of our \n",
              "analysis the decontamination procedure only filters out additional samples Since the seed corpus \u001b[30;48;2;255;255;0mstarcoderdata\u001b[0m has \n",
              "already gone through rigorous data decontamination this observation suggests that \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT is unlikely to \n",
              "introduce additional data leakage beyond the seeds The eventual \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT dataset contains about 75K entries An \n",
              "overview of the dataset statistics can be found in Appendix Qualitative Examples of \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT Figure shows some \n",
              "qualitative examples of how \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT can help LLM get inspiration from seed code snippet to create new coding \n",
              "problems and solutions For example the shell \u001b[30;48;2;255;255;0mscript\u001b[0m example shows how an LLM crafts Python coding problem with just\n",
              "one line of shell \u001b[30;48;2;255;255;0mscript\u001b[0m The library imports example demonstrates how an LLM can create realistic machine learning \n",
              "problem using just few import statements Meanwhile the class signature instance illustrates the ability of LLM to \n",
              "draw inspiration from an \u001b[30;48;2;255;255;0mincomplete\u001b[0m class definition featuring annotations like SpringBootApplication and keywords \n",
              "such as bank From this the\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">that contain docstrings or solutions from HumanEval Chen et al 2021 and MBPP Austin et al 2021 docstrings from APPS\n",
              "Hendrycks et al 2021 prompts from DS 1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> or questions from GSM8K Cobbe et al 2021 As part of our \n",
              "analysis the decontamination procedure only filters out additional samples Since the seed corpus <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">starcoderdata</span> has \n",
              "already gone through rigorous data decontamination this observation suggests that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT is unlikely to \n",
              "introduce additional data leakage beyond the seeds The eventual <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT dataset contains about 75K entries An \n",
              "overview of the dataset statistics can be found in Appendix Qualitative Examples of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT Figure shows some \n",
              "qualitative examples of how <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT can help LLM get inspiration from seed code snippet to create new coding \n",
              "problems and solutions For example the shell <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">script</span> example shows how an LLM crafts Python coding problem with just\n",
              "one line of shell <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">script</span> The library imports example demonstrates how an LLM can create realistic machine learning \n",
              "problem using just few import statements Meanwhile the class signature instance illustrates the ability of LLM to \n",
              "draw inspiration from an <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">incomplete</span> class definition featuring annotations like SpringBootApplication and keywords \n",
              "such as bank From this the\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mLLM\u001b[0m generates problem that re quires implementing complete banking system based on Spring Boot Overall OSS INSTRUCT\n",
              "can inspire an \u001b[30;48;2;255;255;0mLLM\u001b[0m with distinct code structures and semantics to create diverse coding tasks including algorithmic\n",
              "challenges realistic issues single function code generation library based pro gram completion whole program \n",
              "development and even whole application construction \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Seed \n",
              "shell script python3 makeErrorFile py data test_dataset_14 14 Problem Create Python program that generates an error\n",
              "file based on given \u001b[30;48;2;255;255;0mdataset\u001b[0m Code def \u001b[30;48;2;255;255;0mgenerate_error_file\u001b[0m dataset_file error_lines with open dataset_file as file \n",
              "for line in file with open error_file_name as error_file for error_line in error_lines error_file write error_line \n",
              "if __name__ \u001b[30;48;2;255;255;0m__main__\u001b[0m if len sys argv print Usage else dataset_file sys argv dataset_number sys argv \n",
              "\u001b[30;48;2;255;255;0mgenerate_error_file\u001b[0m Seed library imports import numpy as np import gym_electric\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> generates problem that re quires implementing complete banking system based on Spring Boot Overall OSS INSTRUCT\n",
              "can inspire an <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> with distinct code structures and semantics to create diverse coding tasks including algorithmic\n",
              "challenges realistic issues single function code generation library based pro gram completion whole program \n",
              "development and even whole application construction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Seed \n",
              "shell script python3 makeErrorFile py data test_dataset_14 14 Problem Create Python program that generates an error\n",
              "file based on given <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">dataset</span> Code def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generate_error_file</span> dataset_file error_lines with open dataset_file as file \n",
              "for line in file with open error_file_name as error_file for error_line in error_lines error_file write error_line \n",
              "if __name__ <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__main__</span> if len sys argv print Usage else dataset_file sys argv dataset_number sys argv \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generate_error_file</span> Seed library imports import numpy as np import gym_electric\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "_motor as gem import matplotlib pyplot as plt Problem Create reinforcement learning agent to control an electric \n",
              "motor using the OpenAI Gym environment Code import numpy as np import \u001b[30;48;2;255;255;0mgym_electric_motor\u001b[0m as gem env gem make \n",
              "DcSeriesCont v1 class DQNAgent def \u001b[30;48;2;255;255;0m__init__\u001b[0m \u001b[30;48;2;255;255;0mself\u001b[0m state_dim \u001b[30;48;2;255;255;0maction_dim\u001b[0m def build_model \u001b[30;48;2;255;255;0mself\u001b[0m def act \u001b[30;48;2;255;255;0mself\u001b[0m state def \n",
              "train \u001b[30;48;2;255;255;0mself\u001b[0m state action reward for episode in range episodes state env reset state np reshape state state_dim Seed \n",
              "class signature SpringBootApplication Import AxonConfig class public class \u001b[30;48;2;255;255;0mAxonbankApplication\u001b[0m public static void \n",
              "main String args Problem Create simple Java Spring Boot application for banking system Code import org \n",
              "axonframework commandhandling import org axonframework config SpringBootApplication Import AxonConfig class public \n",
              "class \u001b[30;48;2;255;255;0mAxonbankApplication\u001b[0m public class BankAccount public class CreateAccountCommand public class \n",
              "DepositFundsCommand public class WithdrawFundsCommand public class Account\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">_motor as gem import matplotlib pyplot as plt Problem Create reinforcement learning agent to control an electric \n",
              "motor using the OpenAI Gym environment Code import numpy as np import <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">gym_electric_motor</span> as gem env gem make \n",
              "DcSeriesCont v1 class DQNAgent def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__init__</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> state_dim <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">action_dim</span> def build_model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> def act <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> state def \n",
              "train <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> state action reward for episode in range episodes state env reset state np reshape state state_dim Seed \n",
              "class signature SpringBootApplication Import AxonConfig class public class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">AxonbankApplication</span> public static void \n",
              "main String args Problem Create simple Java Spring Boot application for banking system Code import org \n",
              "axonframework commandhandling import org axonframework config SpringBootApplication Import AxonConfig class public \n",
              "class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">AxonbankApplication</span> public class BankAccount public class CreateAccountCommand public class \n",
              "DepositFundsCommand public class WithdrawFundsCommand public class Account\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "CreatedEvent public class \u001b[30;48;2;255;255;0mFundsDepositedEvent\u001b[0m public class FundsWithdrawnEvent Figure Examples showing how OSS \n",
              "\u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m \u001b[30;48;2;255;255;0mgenerates\u001b[0m problems and solutions from seed code snippets Detailed problem requirements implementations and\n",
              "explanations are omitted for brevity More examples can be found in Appendix \u001b[30;48;2;255;255;0mCosine\u001b[0m Similarity Score 00 02 04 06 08 \n",
              "10 12 14 Percentage Self \u001b[30;48;2;255;255;0mInstruct\u001b[0m Avg Score 169 Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m Avg Score 131 OSS \u001b[30;48;2;255;255;0mInstruct\u001b[0m Avg Score 105 Figure \n",
              "\u001b[30;48;2;255;255;0mCosine\u001b[0m similarities between \u001b[30;48;2;255;255;0mHumanEval\u001b[0m and syn thetic data generated by different methods Similarity with \u001b[30;48;2;255;255;0mHumanEval\u001b[0m \n",
              "To study whether our data generation process produces more \u001b[30;48;2;255;255;0mHumanEval\u001b[0m like prob lems or solutions that contribute to\n",
              "high performance we pair each sample from our 75K dataset with each of the 164 \u001b[30;48;2;255;255;0mHumanEval\u001b[0m Chen et al 2021 samples \n",
              "and compute their \u001b[30;48;2;255;255;0mcosine\u001b[0m similarity using TF IDF SPARCK JONES 1972 embeddings We then associate each OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m \n",
              "sample with \u001b[30;48;2;255;255;0mHumanEval\u001b[0m sample with the highest simi larity score We also compare our dataset against Code Alpaca 20\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CreatedEvent public class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">FundsDepositedEvent</span> public class FundsWithdrawnEvent Figure Examples showing how OSS \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generates</span> problems and solutions from seed code snippets Detailed problem requirements implementations and\n",
              "explanations are omitted for brevity More examples can be found in Appendix <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Cosine</span> Similarity Score 00 02 04 06 08 \n",
              "10 12 14 Percentage Self <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> Avg Score 169 Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> Avg Score 131 OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> Avg Score 105 Figure \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Cosine</span> similarities between <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> and syn thetic data generated by different methods Similarity with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> \n",
              "To study whether our data generation process produces more <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> like prob lems or solutions that contribute to\n",
              "high performance we pair each sample from our 75K dataset with each of the 164 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> Chen et al 2021 samples \n",
              "and compute their <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">cosine</span> similarity using TF IDF SPARCK JONES 1972 embeddings We then associate each OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> \n",
              "sample with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">HumanEval</span> sample with the highest simi larity score We also compare our dataset against Code Alpaca 20\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset applying SELF INSTRUCT to code and evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 theblackcat102 2023 an open source reproduction of \n",
              "Evol Instruct containing 110K coding instructions We resort to the open source implemen tation because the official\n",
              "Code Evol Instruct Luo et al 2023b dataset is not released We decontaminate all the datasets beforehand using the \n",
              "same way discussed in Figure shows that OSS INSTRUCT exhibits the \u001b[30;48;2;255;255;0mlowest\u001b[0m average similarity among all the studied \n",
              "data generation techniques while SELF INSTRUCT shows the highest aver age similarity This result indicates that the\n",
              "improvements from OSS INSTRUCT are not merely due to including data from the same distribution Evaluation We choose\n",
              "CODELLAMA PYTHON 7B and DeepSeek Coder Base 7B as the base \u001b[30;48;2;255;255;0mLLMs\u001b[0m To derive Magicoder series we first finetune them \n",
              "on 75K synthetic data generated through OSS INSTRUCT We then obtain \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m by continuing finetuning Magicoder \n",
              "with the evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 dataset an open source Evol Instruct implementation containing about 110K samples More \n",
              "implementation details and additional evaluation re sults are listed in Appendices and We also present interesting \n",
              "use cases that reflect the effectiveness of \u001b[30;48;2;255;255;0minstruc\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">dataset applying SELF INSTRUCT to code and evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 theblackcat102 2023 an open source reproduction of \n",
              "Evol Instruct containing 110K coding instructions We resort to the open source implemen tation because the official\n",
              "Code Evol Instruct Luo et al 2023b dataset is not released We decontaminate all the datasets beforehand using the \n",
              "same way discussed in Figure shows that OSS INSTRUCT exhibits the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">lowest</span> average similarity among all the studied \n",
              "data generation techniques while SELF INSTRUCT shows the highest aver age similarity This result indicates that the\n",
              "improvements from OSS INSTRUCT are not merely due to including data from the same distribution Evaluation We choose\n",
              "CODELLAMA PYTHON 7B and DeepSeek Coder Base 7B as the base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> To derive Magicoder series we first finetune them \n",
              "on 75K synthetic data generated through OSS INSTRUCT We then obtain <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> by continuing finetuning Magicoder \n",
              "with the evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 dataset an open source Evol Instruct implementation containing about 110K samples More \n",
              "implementation details and additional evaluation re sults are listed in Appendices and We also present interesting \n",
              "use cases that reflect the effectiveness of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruc</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tion tuning in Appendix and demonstrate \u001b[30;48;2;255;255;0mMagicoder\u001b[0m capability to \u001b[30;48;2;255;255;0mgenerate\u001b[0m complex programs in Appendix Python Text \n",
              "to Code Generation HumanEval Chen et al 2021 and \u001b[30;48;2;255;255;0mMBPP\u001b[0m Austin et al 2021 are two of the most widely used benchmarks \n",
              "for code generation Each task in these benchmarks includes task description docstring as the prompt where LLMs \n",
              "\u001b[30;48;2;255;255;0mgenerate\u001b[0m corresponding code whose correctness is checked by handful of test cases Because tests in these benchmarks\n",
              "can be insufficient for more rigorous evaluation we use HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m both powered by the EvalPlus framework \n",
              "Liu et al 2023b to obtain 80 35 more tests Following prior work Liu et al 2023b Chen et al \u001b[30;48;2;255;255;0m2023\u001b[0m for each task and \n",
              "\u001b[30;48;2;255;255;0mLLM\u001b[0m we use greedy decoding to \u001b[30;48;2;255;255;0mgenerate\u001b[0m \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Table Pass results of\n",
              "different LLMs on HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m computed with greedy decoding The abbreviations CL and SC refer to the base \n",
              "models CODELLAMA PYTHON and StarCoder respectively We report the results consistently from the EvalPlus Liu et al \n",
              "2023b Leaderboard Model Release Date Size Benchmark Open Source\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tion tuning in Appendix and demonstrate <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> capability to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generate</span> complex programs in Appendix Python Text \n",
              "to Code Generation HumanEval Chen et al 2021 and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> Austin et al 2021 are two of the most widely used benchmarks \n",
              "for code generation Each task in these benchmarks includes task description docstring as the prompt where LLMs \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generate</span> corresponding code whose correctness is checked by handful of test cases Because tests in these benchmarks\n",
              "can be insufficient for more rigorous evaluation we use HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> both powered by the EvalPlus framework \n",
              "Liu et al 2023b to obtain 80 35 more tests Following prior work Liu et al 2023b Chen et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> for each task and \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> we use greedy decoding to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generate</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Table Pass results of\n",
              "different LLMs on HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> computed with greedy decoding The abbreviations CL and SC refer to the base \n",
              "models CODELLAMA PYTHON and StarCoder respectively We report the results consistently from the EvalPlus Liu et al \n",
              "2023b Leaderboard Model Release Date Size Benchmark Open Source\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HumanEval MBPP \u001b[30;48;2;255;255;0mWeight\u001b[0m Data \u001b[30;48;2;255;255;0mGPT\u001b[0m Turbo Nov 2023 72 65 81 69 \u001b[30;48;2;255;255;0mGPT\u001b[0m Turbo Nov 2023 85 81 83 70 CODELLAMA PYTHON Aug 2023 \n",
              "34B 51 42 67 52 WizardCoder CL Sep 2023 34B 73 64 73 59 CodeT5 May 2023 16B 31 26 54 44 \u001b[30;48;2;255;255;0mCodeGen\u001b[0m Mono Mar \u001b[30;48;2;255;255;0m2022\u001b[0m 16B \n",
              "32 27 52 43 StarCoder May 2023 15B 34 29 55 46 CODELLAMA PYTHON Aug 2023 13B 42 36 61 50 WizardCoder SC Sep 2023 \n",
              "15B 51 45 61 50 StarCoder May 2023 7B 24 20 33 28 \u001b[30;48;2;255;255;0mMistral\u001b[0m Oct 2023 7B 28 23 50 40\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HumanEval MBPP <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Weight</span> Data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">GPT</span> Turbo Nov 2023 72 65 81 69 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">GPT</span> Turbo Nov 2023 85 81 83 70 CODELLAMA PYTHON Aug 2023 \n",
              "34B 51 42 67 52 WizardCoder CL Sep 2023 34B 73 64 73 59 CodeT5 May 2023 16B 31 26 54 44 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CodeGen</span> Mono Mar <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> 16B \n",
              "32 27 52 43 StarCoder May 2023 15B 34 29 55 46 CODELLAMA PYTHON Aug 2023 13B 42 36 61 50 WizardCoder SC Sep 2023 \n",
              "15B 51 45 61 50 StarCoder May 2023 7B 24 20 33 28 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mistral</span> Oct 2023 7B 28 23 50 40\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "CodeT5 May 2023 6B 29 23 51 40 CodeGen Mono Mar \u001b[30;48;2;255;255;0m2022\u001b[0m 6B 29 25 49 42 \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m \u001b[30;48;2;255;255;0mPYTHON\u001b[0m Aug 2023 \u001b[30;48;2;255;255;0m7B\u001b[0m 37 34 57 45 \n",
              "WizardCoder CL Sep 2023 \u001b[30;48;2;255;255;0m7B\u001b[0m 48 40 56 47 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL Dec 2023 \u001b[30;48;2;255;255;0m7B\u001b[0m 60 55 64 52 MagicoderS CL Dec 2023 \u001b[30;48;2;255;255;0m7B\u001b[0m 70 66 68 56 \n",
              "one sample and focus on comparing the pass metric We consider wide range of baseline models including \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mPYTHON\u001b[0m Roziere et al 2023 Wizard Coder Luo et al 2023b GPT Turbo OpenAI \u001b[30;48;2;255;255;0m2022\u001b[0m GPT Turbo OpenAI 2023 StarCoder Li et \n",
              "al 2023 CodeT5 Wang et al 2023b CodeGen Mono Nijkamp et al 2023 and Mistral Jiang et al 2023a All the re sults\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CodeT5 May 2023 6B 29 23 51 40 CodeGen Mono Mar <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> 6B 29 25 49 42 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PYTHON</span> Aug 2023 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 37 34 57 45 \n",
              "WizardCoder CL Sep 2023 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 48 40 56 47 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL Dec 2023 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 60 55 64 52 MagicoderS CL Dec 2023 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 70 66 68 56 \n",
              "one sample and focus on comparing the pass metric We consider wide range of baseline models including <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PYTHON</span> Roziere et al 2023 Wizard Coder Luo et al 2023b GPT Turbo OpenAI <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> GPT Turbo OpenAI 2023 StarCoder Li et \n",
              "al 2023 CodeT5 Wang et al 2023b CodeGen Mono Nijkamp et al 2023 and Mistral Jiang et al 2023a All the re sults\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "are consistently reported from the \u001b[30;48;2;255;255;0mEvalPlus\u001b[0m Liu et al \u001b[30;48;2;255;255;0m2023b\u001b[0m leaderboard \u001b[30;48;2;255;255;0mEvalPlus\u001b[0m hash 1895d2f Table shows the pass \n",
              "results of different LLMs on these benchmarks From the results we can first observe that Magicoder CL has clear \n",
              "improvement over the base CODELLAMA PYTHON 7B and outperforms all studied open source models except CODELLAMA \n",
              "PYTHON 34B and WizardCoder CL 34B Notably Magicoder CL sur passes WizardCoder SC 15B and has substantial improve \n",
              "ment on HumanEval and HumanEval over CODELLAMA PYTHON 34B \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL demonstrates further im provements by being\n",
              "trained with the orthogonal Evol Instruct method \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL outperforms \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and all other open source models\n",
              "on HumanEval More over although it scores slightly lower than WizardCoder CL 34B and \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval it \n",
              "surpasses both of them on the more rigorous HumanEval dataset indicating that \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL may produce more robust\n",
              "code \u001b[30;48;2;255;255;0mMultilingual\u001b[0m Code Generation In addition to Python as shown in Table we perform an extensive evaluation on \n",
              "widely used programming languages Java JavaScript PHP\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">are consistently reported from the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvalPlus</span> Liu et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> leaderboard <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvalPlus</span> hash 1895d2f Table shows the pass \n",
              "results of different LLMs on these benchmarks From the results we can first observe that Magicoder CL has clear \n",
              "improvement over the base CODELLAMA PYTHON 7B and outperforms all studied open source models except CODELLAMA \n",
              "PYTHON 34B and WizardCoder CL 34B Notably Magicoder CL sur passes WizardCoder SC 15B and has substantial improve \n",
              "ment on HumanEval and HumanEval over CODELLAMA PYTHON 34B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL demonstrates further im provements by being\n",
              "trained with the orthogonal Evol Instruct method <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL outperforms <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and all other open source models\n",
              "on HumanEval More over although it scores slightly lower than WizardCoder CL 34B and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval it \n",
              "surpasses both of them on the more rigorous HumanEval dataset indicating that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL may produce more robust\n",
              "code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Multilingual</span> Code Generation In addition to Python as shown in Table we perform an extensive evaluation on \n",
              "widely used programming languages Java JavaScript PHP\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Swift and Rust using the MultiPL benchmark Cassano et al 2022 We report available results from the \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m pa \n",
              "per Luo et al \u001b[30;48;2;255;255;0m2023b\u001b[0m and evaluate our models consistently through bigcode evaluation harness Ben Allal et al 2022 We\n",
              "skip proprietary models such as Chat GPT and GPT as they are not supported by the frame work Due to significant \n",
              "inference latency when running \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL 7B using the harness in our environment we choose not to include it \n",
              "in our analysis The results indicate that Magicoder CL \u001b[30;48;2;255;255;0mimproves\u001b[0m the base \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON 7B by large margin among \n",
              "all the studied programming languages Moreover Magicoder CL also achieves better results than the SOTA 15B \n",
              "\u001b[30;48;2;255;255;0mWizardCoder\u001b[0m SC among half of the programming lan guages Additionally MagicoderS CL demonstrates fur ther \n",
              "improvement over Magicoder CL on all program ming languages achieving comparable performance against \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL\n",
              "34B with only 7B parameters It is worth noting that Magicoder CL is only trained with very limited multilingual \n",
              "data but still outperforms other \u001b[30;48;2;255;255;0mLLMs\u001b[0m with similar or even larger sizes Also although the harness Magicoder \n",
              "Empowering Code Generation with OSS INSTRUCT evaluates models in completion formats which are for base models \n",
              "Magicod\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Swift and Rust using the MultiPL benchmark Cassano et al 2022 We report available results from the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> pa \n",
              "per Luo et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> and evaluate our models consistently through bigcode evaluation harness Ben Allal et al 2022 We\n",
              "skip proprietary models such as Chat GPT and GPT as they are not supported by the frame work Due to significant \n",
              "inference latency when running <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL 7B using the harness in our environment we choose not to include it \n",
              "in our analysis The results indicate that Magicoder CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">improves</span> the base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON 7B by large margin among \n",
              "all the studied programming languages Moreover Magicoder CL also achieves better results than the SOTA 15B \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> SC among half of the programming lan guages Additionally MagicoderS CL demonstrates fur ther \n",
              "improvement over Magicoder CL on all program ming languages achieving comparable performance against <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL\n",
              "34B with only 7B parameters It is worth noting that Magicoder CL is only trained with very limited multilingual \n",
              "data but still outperforms other <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with similar or even larger sizes Also although the harness Magicoder \n",
              "Empowering Code Generation with OSS INSTRUCT evaluates models in completion formats which are for base models \n",
              "Magicod\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ers still show significant improvements de spite being only instruction tuned This implies that LLMs can learn \n",
              "knowledge from the data beyond its format Code Generation for Data Science The DS 1000 \u001b[30;48;2;255;255;0mdataset\u001b[0m Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m \n",
              "contains 1K distinct data science coding issues ranging from popular data sci ence libraries in Python It evaluates\n",
              "the realistic and practi cal use case of an \u001b[30;48;2;255;255;0mLLM\u001b[0m and offers unit tests for validating each problem DS 1000 has both \n",
              "completion and insertion modes but here we only evaluate completion because the base CODELLAMA PYTHON does not \n",
              "support infilling Ta ble shows the evaluation results where we include the recent INCODER Fried et al 2023 CodeGen \n",
              "Nijkamp et al 2023 Code \u001b[30;48;2;255;255;0mCushman\u001b[0m 001 Microsoft 2023a Star Coder Li et al 2023 CODELLAMA PYTHON Roziere et al 2023 \n",
              "and \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m Luo et al 2023b We can see from the table that Magicoder CL 7B already out performs all the \n",
              "baselines we evaluate including state of the art \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL 7B and \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m SC 15B MagicoderS CL 7B further \n",
              "breaks the limit by introduc ing an percentage point absolute improvement over\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ers still show significant improvements de spite being only instruction tuned This implies that LLMs can learn \n",
              "knowledge from the data beyond its format Code Generation for Data Science The DS 1000 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">dataset</span> Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> \n",
              "contains 1K distinct data science coding issues ranging from popular data sci ence libraries in Python It evaluates\n",
              "the realistic and practi cal use case of an <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> and offers unit tests for validating each problem DS 1000 has both \n",
              "completion and insertion modes but here we only evaluate completion because the base CODELLAMA PYTHON does not \n",
              "support infilling Ta ble shows the evaluation results where we include the recent INCODER Fried et al 2023 CodeGen \n",
              "Nijkamp et al 2023 Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Cushman</span> 001 Microsoft 2023a Star Coder Li et al 2023 CODELLAMA PYTHON Roziere et al 2023 \n",
              "and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> Luo et al 2023b We can see from the table that Magicoder CL 7B already out performs all the \n",
              "baselines we evaluate including state of the art <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL 7B and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> SC 15B MagicoderS CL 7B further \n",
              "breaks the limit by introduc ing an percentage point absolute improvement over\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mizardCoder\u001b[0m SC 15B Comparison with \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Guo et al 2024 is series of models released \n",
              "concurrently to our work and they demonstrate su perior coding performance We only briefly discuss it in this \n",
              "section because its data and instruction tuning details are not publicly available at the time of writing We apply \n",
              "the same finetuning strategy on \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Base \u001b[30;48;2;255;255;0m7B\u001b[0m as we performed on \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON \u001b[30;48;2;255;255;0m7B\u001b[0m leading to Magicoder \n",
              "DS and MagicoderS DS Table shows similar trend as Table that the base model can be sig nificantly improved after \n",
              "applying OSS INSTRUCT Re markably the MagicoderS DS variant surpasses \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Instruct \u001b[30;48;2;255;255;0m7B\u001b[0m on all the \n",
              "benchmarks with \u001b[30;48;2;255;255;0mfewer\u001b[0m training tokens and it also closely matches \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Instruct 33B on these datasets \n",
              "Ablations of Data Source Impact of the Language Distribution To understand the correlation between the programming \n",
              "lan guages appearing in the training data and the downstream performance of different languages we conduct an addi \n",
              "tional ablation study about the training data We classify the 75K training data into approximately 43K Python only \n",
              "and 32K non Python data according to whether\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">izardCoder</span> SC 15B Comparison with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Guo et al 2024 is series of models released \n",
              "concurrently to our work and they demonstrate su perior coding performance We only briefly discuss it in this \n",
              "section because its data and instruction tuning details are not publicly available at the time of writing We apply \n",
              "the same finetuning strategy on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> as we performed on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> leading to Magicoder \n",
              "DS and MagicoderS DS Table shows similar trend as Table that the base model can be sig nificantly improved after \n",
              "applying OSS INSTRUCT Re markably the MagicoderS DS variant surpasses <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Instruct <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> on all the \n",
              "benchmarks with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">fewer</span> training tokens and it also closely matches <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Instruct 33B on these datasets \n",
              "Ablations of Data Source Impact of the Language Distribution To understand the correlation between the programming \n",
              "lan guages appearing in the training data and the downstream performance of different languages we conduct an addi \n",
              "tional ablation study about the training data We classify the 75K training data into approximately 43K Python only \n",
              "and 32K non Python data according to whether\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mpython\u001b[0m is substring of the generated data We do not classify the data based on the seed code snippet because \u001b[30;48;2;255;255;0mLLMs\u001b[0m \n",
              "per forming OSS INSTRUCT may produce code in different programming language than the seed Table shows the \n",
              "evaluation results where we consistently finetune the base CODELLAMA \u001b[30;48;2;255;255;0mPYTHON\u001b[0m \u001b[30;48;2;255;255;0m7B\u001b[0m for epochs on different data \n",
              "partitions using the same training hyper parameters explained in Appendix From the table we can see that as can be \n",
              "imagined training on \u001b[30;48;2;255;255;0mPython\u001b[0m or non \u001b[30;48;2;255;255;0mPython\u001b[0m data can substantially boost the performance of the base model in \u001b[30;48;2;255;255;0mPython\u001b[0m \n",
              "or non \u001b[30;48;2;255;255;0mPython\u001b[0m tasks respectively Interestingly instruction tuning on different programming languages can still \n",
              "boost the overall coding performance that includes out of distribution languages For example when trained on only \n",
              "non \u001b[30;48;2;255;255;0mPython\u001b[0m data Magicoder CL still achieves 10 percentage point improvement over the base model in the \u001b[30;48;2;255;255;0mPython\u001b[0m only \n",
              "evaluation This implies \u001b[30;48;2;255;255;0mLLMs\u001b[0m can establish correlations between different programming languages and perform \n",
              "transfer learning of deeper code se mantics Finally we observe more significant boost in \u001b[30;48;2;255;255;0mPython\u001b[0m evaluation when \n",
              "combining data from both sources with slight \u001b[30;48;2;255;255;0mdecrease\u001b[0m in multilingual performance compared with only \u001b[30;48;2;255;255;0mfinetuning\u001b[0m on \n",
              "multilingual data We attribute this \u001b[30;48;2;255;255;0mdecrease\u001b[0m to the dominant amount of \u001b[30;48;2;255;255;0mPython\u001b[0m data around 57 during instruction \n",
              "tuning OSS INSTRUCT vs Direct \u001b[30;48;2;255;255;0mFinetuning\u001b[0m The fact\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">python</span> is substring of the generated data We do not classify the data based on the seed code snippet because <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> \n",
              "per forming OSS INSTRUCT may produce code in different programming language than the seed Table shows the \n",
              "evaluation results where we consistently finetune the base CODELLAMA <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PYTHON</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> for epochs on different data \n",
              "partitions using the same training hyper parameters explained in Appendix From the table we can see that as can be \n",
              "imagined training on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> or non <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> data can substantially boost the performance of the base model in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> \n",
              "or non <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> tasks respectively Interestingly instruction tuning on different programming languages can still \n",
              "boost the overall coding performance that includes out of distribution languages For example when trained on only \n",
              "non <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> data Magicoder CL still achieves 10 percentage point improvement over the base model in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> only \n",
              "evaluation This implies <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> can establish correlations between different programming languages and perform \n",
              "transfer learning of deeper code se mantics Finally we observe more significant boost in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> evaluation when \n",
              "combining data from both sources with slight <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">decrease</span> in multilingual performance compared with only <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> on \n",
              "multilingual data We attribute this <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">decrease</span> to the dominant amount of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> data around 57 during instruction \n",
              "tuning OSS INSTRUCT vs Direct <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Finetuning</span> The fact\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "that \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT gets an LLM inspired from open source code snippets may lead to natural question why not directly\n",
              "\u001b[30;48;2;255;255;0mfinetuning\u001b[0m on these open source code To answer this question we follow \u001b[30;48;2;255;255;0mCodeSearchNet\u001b[0m Husain et al 2020 to mine \n",
              "semantically \u001b[30;48;2;255;255;0mrelevant\u001b[0m comment function pairs from the same seed document corpus we use to con struct the 75K \u001b[30;48;2;255;255;0mOSS\u001b[0m \n",
              "INSTRUCT dataset We then train the model to predict the function bodies from the function signa tures and comments \n",
              "We prioritize comment function pairs that overlap with our 75K seed snippets resulting in about 11K data points To \n",
              "align with our 75K samples we collect the remaining 64K samples using the whole corpus of 75K seed documents \n",
              "Eventually we have the same number of comment function pairs with \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT data We finetune the base CODELLAMA \n",
              "PYTHON 7B for epochs using the paired data following the same training setup discussed in Appendix From Table we \n",
              "observe that \u001b[30;48;2;255;255;0mfinetuning\u001b[0m on 75K paired comment function data even worsens the base model while \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT helps to\n",
              "introduce substantial \u001b[30;48;2;255;255;0mboost\u001b[0m We conjecture that the degra dation is owing to the substantial noise and inconsistency\n",
              "that exists intrinsically in the data pairs even though these paired data exhibit very similar format as HumanEval \n",
              "or MultiPL problems\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT gets an LLM inspired from open source code snippets may lead to natural question why not directly\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> on these open source code To answer this question we follow <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CodeSearchNet</span> Husain et al 2020 to mine \n",
              "semantically <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">relevant</span> comment function pairs from the same seed document corpus we use to con struct the 75K <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> \n",
              "INSTRUCT dataset We then train the model to predict the function bodies from the function signa tures and comments \n",
              "We prioritize comment function pairs that overlap with our 75K seed snippets resulting in about 11K data points To \n",
              "align with our 75K samples we collect the remaining 64K samples using the whole corpus of 75K seed documents \n",
              "Eventually we have the same number of comment function pairs with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT data We finetune the base CODELLAMA \n",
              "PYTHON 7B for epochs using the paired data following the same training setup discussed in Appendix From Table we \n",
              "observe that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> on 75K paired comment function data even worsens the base model while <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT helps to\n",
              "introduce substantial <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">boost</span> We conjecture that the degra dation is owing to the substantial noise and inconsistency\n",
              "that exists intrinsically in the data pairs even though these paired data exhibit very similar format as HumanEval \n",
              "or MultiPL problems\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "This further shows that data factual ity rather than the format is essential to code instruction tuning It also \n",
              "indicates the superiority of OSS INSTRUCT which can translate these loosely related code fragments Magicoder \n",
              "Empowering Code Generation with OSS INSTRUCT Table Pass results of different LLMs on \u001b[30;48;2;255;255;0mMultiPL\u001b[0m Cassano et al 2022 \n",
              "following the same hyperparameter settings as the \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m paper Luo et al \u001b[30;48;2;255;255;0m2023b\u001b[0m temperature top 95 max length \n",
              "512 and num samples 50 We evaluate all 7B models using bigcode evaluation harness Ben Allal et al 2022 and report \n",
              "other results from \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m Model Size Programming Language Java JavaScript PHP Swift Rust \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m 34B 40 41 \n",
              "41 40 35 38 \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON 34B 39 44 39 39 34 39 \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m INSTRUCT 34B 41 45 41 \u001b[30;48;2;255;255;0m37\u001b[0m \u001b[30;48;2;255;255;0m37\u001b[0m 39 \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL 34B 44 \n",
              "55 47 47 44 46 StarCoderBase 15B 28 31 30 26 16 24 Star\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This further shows that data factual ity rather than the format is essential to code instruction tuning It also \n",
              "indicates the superiority of OSS INSTRUCT which can translate these loosely related code fragments Magicoder \n",
              "Empowering Code Generation with OSS INSTRUCT Table Pass results of different LLMs on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MultiPL</span> Cassano et al 2022 \n",
              "following the same hyperparameter settings as the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> paper Luo et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> temperature top 95 max length \n",
              "512 and num samples 50 We evaluate all 7B models using bigcode evaluation harness Ben Allal et al 2022 and report \n",
              "other results from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> Model Size Programming Language Java JavaScript PHP Swift Rust <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> 34B 40 41 \n",
              "41 40 35 38 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON 34B 39 44 39 39 34 39 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> INSTRUCT 34B 41 45 41 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">37</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">37</span> 39 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL 34B 44 \n",
              "55 47 47 44 46 StarCoderBase 15B 28 31 30 26 16 24 Star\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Coder 15B 30 30 31 26 22 21 \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m SC 15B 35 41 39 39 33 27 \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m 7B 29 31 27 25 25 25 \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON 7B\n",
              "29 35 30 29 27 27 Magicoder CL 7B 36 45 36 39 33 30 MagicoderS CL 7B 42 57 44 47 44 40 Table \u001b[30;48;2;255;255;0mPass\u001b[0m results on DS \n",
              "1000 completion format with temperature top max length 1024 and num samples 40 following the same hyperparameter \n",
              "setting used in \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m Luo et al \u001b[30;48;2;255;255;0m2023b\u001b[0m We evaluate all the 7B models with their preferred prompt formats and \n",
              "report other results from \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m Model Size 155 \u001b[30;48;2;255;255;0mMatplotlib\u001b[0m 220 NumPy 291 Pandas 68 PyTorch 106 SciPy 115 \n",
              "Sklearn 45 TensorFlow 1000 Overall INCODER 7B 28\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Coder 15B 30 30 31 26 22 21 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> SC 15B 35 41 39 39 33 27 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> 7B 29 31 27 25 25 25 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON 7B\n",
              "29 35 30 29 27 27 Magicoder CL 7B 36 45 36 39 33 30 MagicoderS CL 7B 42 57 44 47 44 40 Table <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Pass</span> results on DS \n",
              "1000 completion format with temperature top max length 1024 and num samples 40 following the same hyperparameter \n",
              "setting used in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> Luo et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> We evaluate all the 7B models with their preferred prompt formats and \n",
              "report other results from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> Model Size 155 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Matplotlib</span> 220 NumPy 291 Pandas 68 PyTorch 106 SciPy 115 \n",
              "Sklearn 45 TensorFlow 1000 Overall INCODER 7B 28\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mCodeGen\u001b[0m Mono 16B 31 10 10 15 11 Code \u001b[30;48;2;255;255;0mCushman\u001b[0m 001 40 21 12 11 18 12 18 StarCoder 15B 51 29 11 21 20 29 24 26 \n",
              "\u001b[30;48;2;255;255;0mWizardCoder\u001b[0m SC 15B 55 33 16 26 24 24 26 29 CODELLAMA PYTHON 7B 55 34 16 19 22 17 28 28 \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL 7B 53 34 15 \n",
              "25 21 24 28 28 Magicoder CL 7B 54 34 19 24 25 22 28 29 MagicoderS CL 7B 55 40 28 40 28 35 37 37 Table Pass greedy \n",
              "decoding comparison between Magicoder and \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Guo et al 2024 on Hu manEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder \n",
              "results are\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CodeGen</span> Mono 16B 31 10 10 15 11 Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Cushman</span> 001 40 21 12 11 18 12 18 StarCoder 15B 51 29 11 21 20 29 24 26 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> SC 15B 55 33 16 26 24 24 26 29 CODELLAMA PYTHON 7B 55 34 16 19 22 17 28 28 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL 7B 53 34 15 \n",
              "25 21 24 28 28 Magicoder CL 7B 54 34 19 24 25 22 28 29 MagicoderS CL 7B 55 40 28 40 28 35 37 37 Table Pass greedy \n",
              "decoding comparison between Magicoder and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Guo et al 2024 on Hu manEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder \n",
              "results are\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "reported from \u001b[30;48;2;255;255;0mEvalPlus\u001b[0m Liu et al \u001b[30;48;2;255;255;0m2023b\u001b[0m Leaderboard Model Size Training Tokens Benchmark Open Source HumanEval \u001b[30;48;2;255;255;0mMBPP\u001b[0m \n",
              "Weight Data \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Base 3B 2T 55 46 7B 2T 47 39 70 56 33B 2T 51 43 \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder Instruct 3B 2B 64 58 63 \n",
              "53 7B 2B 73 70 72 63 33B 2B 78 72 78 66 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B 90M 66 60 75 61 MagicoderS DS 7B 240M 76 70 75 64 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m\n",
              "Empowering Code Generation with OSS INSTRUCT Table Ablation study of using different programming languages as \n",
              "training data We show the pass results on HumanEval Liu et al \u001b[30;48;2;255;255;0m2023b\u001b[0m for Python and the average pass results on \n",
              "MultiPL Cassano et al\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">reported from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvalPlus</span> Liu et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Leaderboard Model Size Training Tokens Benchmark Open Source HumanEval <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> \n",
              "Weight Data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Base 3B 2T 55 46 7B 2T 47 39 70 56 33B 2T 51 43 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder Instruct 3B 2B 64 58 63 \n",
              "53 7B 2B 73 70 72 63 33B 2B 78 72 78 66 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B 90M 66 60 75 61 MagicoderS DS 7B 240M 76 70 75 64 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span>\n",
              "Empowering Code Generation with OSS INSTRUCT Table Ablation study of using different programming languages as \n",
              "training data We show the pass results on HumanEval Liu et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> for Python and the average pass results on \n",
              "MultiPL Cassano et al\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0m2022\u001b[0m for the same set of programming languages used in Table Java JavaScript PHP Swift and Rust All the variants \n",
              "are finetuned with epochs and evaluated through greedy decoding Model 7B \u001b[30;48;2;255;255;0mFinetuning\u001b[0m Data \u001b[30;48;2;255;255;0mPython\u001b[0m HumanEval Others \n",
              "MultiPL CODELLAMA \u001b[30;48;2;255;255;0mPYTHON\u001b[0m 34 29 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL \u001b[30;48;2;255;255;0mPython\u001b[0m 43K 47 32 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL Others 32K 44 38 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL Both 75K 55\n",
              "37 Table Comparison between \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT and directly \u001b[30;48;2;255;255;0mfinetuning\u001b[0m on comment function pairs with CODELLAMA \u001b[30;48;2;255;255;0mPYTHON\u001b[0m 7B\n",
              "as the base model \u001b[30;48;2;255;255;0mFinetuning\u001b[0m Data HumanEval MultiPL Base model \u001b[30;48;2;255;255;0mfinetuning\u001b[0m 34 29 Comment function pairs 75K 34 24 \n",
              "\u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT 75K 55 37 into semantically consistent instruction tuning data \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT with Less Powerful Teacher\n",
              "In this section we explore the factors contributing to the effectiveness of \u001b[30;48;2;255;255;0mOSS\u001b[0m INSTRUCT beyond just the \n",
              "distillation of the teacher model We propose two potential key reasons First since the base model is pretrained \n",
              "with comprehen\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> for the same set of programming languages used in Table Java JavaScript PHP Swift and Rust All the variants \n",
              "are finetuned with epochs and evaluated through greedy decoding Model 7B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Finetuning</span> Data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> HumanEval Others \n",
              "MultiPL CODELLAMA <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PYTHON</span> 34 29 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> 43K 47 32 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL Others 32K 44 38 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL Both 75K 55\n",
              "37 Table Comparison between <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT and directly <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> on comment function pairs with CODELLAMA <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PYTHON</span> 7B\n",
              "as the base model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Finetuning</span> Data HumanEval MultiPL Base model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> 34 29 Comment function pairs 75K 34 24 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT 75K 55 37 into semantically consistent instruction tuning data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT with Less Powerful Teacher\n",
              "In this section we explore the factors contributing to the effectiveness of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> INSTRUCT beyond just the \n",
              "distillation of the teacher model We propose two potential key reasons First since the base model is pretrained \n",
              "with comprehen\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sive code data the distillation process likely activates the model internal capabilities leading to improved perfor\n",
              "mance in \u001b[30;48;2;255;255;0mcoding\u001b[0m tasks Second OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m uses seed code snippets to generate problem solution pairs in one shot \n",
              "These seed snippets provide valuable context enabling the model to create better solutions than plain teacher model\n",
              "lacking such seed information These enhanced solutions can then be used to train more effective student models To \n",
              "verify these points we conduct an additional experiment by generating subset of 20K OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m data using \u001b[30;48;2;255;255;0mMixtral\u001b[0m\n",
              "8x7B \u001b[30;48;2;255;255;0mInstruct\u001b[0m v0 Jiang et al 2024 state of the art general purpose open source LLM Table Pass on HumanEval and \u001b[30;48;2;255;255;0mMBPP\u001b[0m\n",
              "when fine tuning CODELLAMA PYTHON 7B for epochs on 20K OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m data generated by \u001b[30;48;2;255;255;0mMixtral\u001b[0m 8x7B \u001b[30;48;2;255;255;0mInstruct\u001b[0m v0 \n",
              "Jiang et al 2024 Model HumanEval \u001b[30;48;2;255;255;0mMBPP\u001b[0m \u001b[30;48;2;255;255;0mMixtral\u001b[0m 8x7B \u001b[30;48;2;255;255;0mInstruct\u001b[0m v0 39 47 CODELLAMA PYTHON 7B 34 45 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL \u001b[30;48;2;255;255;0mMixtral\u001b[0m\n",
              "7B 55 50 Table indicates that \u001b[30;48;2;255;255;0mMagicoder\u001b[0m CL \u001b[30;48;2;255;255;0mMixtral\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">sive code data the distillation process likely activates the model internal capabilities leading to improved perfor\n",
              "mance in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> tasks Second OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> uses seed code snippets to generate problem solution pairs in one shot \n",
              "These seed snippets provide valuable context enabling the model to create better solutions than plain teacher model\n",
              "lacking such seed information These enhanced solutions can then be used to train more effective student models To \n",
              "verify these points we conduct an additional experiment by generating subset of 20K OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> data using <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span>\n",
              "8x7B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> v0 Jiang et al 2024 state of the art general purpose open source LLM Table Pass on HumanEval and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span>\n",
              "when fine tuning CODELLAMA PYTHON 7B for epochs on 20K OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> data generated by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span> 8x7B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> v0 \n",
              "Jiang et al 2024 Model HumanEval <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MBPP</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span> 8x7B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> v0 39 47 CODELLAMA PYTHON 7B 34 45 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span>\n",
              "7B 55 50 Table indicates that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "7B not only significantly improves over the base CODELLAMA PYTHON but is also better than \u001b[30;48;2;255;255;0mMixtral\u001b[0m 8x7B \u001b[30;48;2;255;255;0mInstruct\u001b[0m v0 \n",
              "the teacher model across HumanEval and MBPP These results suggest that OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m is not simply dis tilling \n",
              "teacher model but also triggering the base model own capability and effectively leveraging the information \n",
              "encapsulated in seed \u001b[30;48;2;255;255;0mcode\u001b[0m snippets Related Work Foundation models for \u001b[30;48;2;255;255;0mcode\u001b[0m Trained over billions of lines of \u001b[30;48;2;255;255;0mcode\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mLLMs\u001b[0m have demonstrated outstanding per formance in wide range of software engineering tasks including \u001b[30;48;2;255;255;0mcode\u001b[0m \n",
              "generation Chen et al 2021 Austin et al 2021 program repair Xia Zhang 2022 Wei et al 2023 Xia et al \u001b[30;48;2;255;255;0m2023b\u001b[0m Jiang et \n",
              "al \u001b[30;48;2;255;255;0m2023b\u001b[0m Bouzenia et al 2024 and software testing Xia et al 2023a Deng et al 2023 Yuan et al 2023 Schafer et al \n",
              "2023 Lemieux et al 2023 In particular prominent base models such as \u001b[30;48;2;255;255;0mCode\u001b[0m Gen Nijkamp et al 2023 CodeT5 Wang et al \n",
              "2021 StarCoder Li et al 2023 and CODELLAMA Roziere et al 2023 are pre\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">7B not only significantly improves over the base CODELLAMA PYTHON but is also better than <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Mixtral</span> 8x7B <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> v0 \n",
              "the teacher model across HumanEval and MBPP These results suggest that OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> is not simply dis tilling \n",
              "teacher model but also triggering the base model own capability and effectively leveraging the information \n",
              "encapsulated in seed <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> snippets Related Work Foundation models for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> Trained over billions of lines of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> have demonstrated outstanding per formance in wide range of software engineering tasks including <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> \n",
              "generation Chen et al 2021 Austin et al 2021 program repair Xia Zhang 2022 Wei et al 2023 Xia et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Jiang et \n",
              "al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Bouzenia et al 2024 and software testing Xia et al 2023a Deng et al 2023 Yuan et al 2023 Schafer et al \n",
              "2023 Lemieux et al 2023 In particular prominent base models such as <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Gen Nijkamp et al 2023 CodeT5 Wang et al \n",
              "2021 StarCoder Li et al 2023 and CODELLAMA Roziere et al 2023 are pre\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "trained over huge number of code base from scratch establishing the fundamental ability of general code generation \n",
              "and understanding More recent code \u001b[30;48;2;255;255;0mLLMs\u001b[0m such as DeepSeek Coder Guo et al 2024 and \u001b[30;48;2;255;255;0mStarCoder2\u001b[0m Lozhkov et al 2024 \n",
              "additionally organize the pretraining data at the repository level to enhance the model contextual understanding \n",
              "capabilities Furthermore these base models are also finetuned Luo et al 2023b or prompted Chen et al 2023 to unlock\n",
              "their true potential to specialize in solving domain specific coding tasks Instruction tuning with \u001b[30;48;2;255;255;0msynthetic\u001b[0m data \n",
              "Instruction tun ing aims to improve pretrained \u001b[30;48;2;255;255;0mLLMs\u001b[0m by \u001b[30;48;2;255;255;0mfinetuning\u001b[0m them with mixture of instructions and \n",
              "corresponding re sponses Wei et al \u001b[30;48;2;255;255;0m2022\u001b[0m However obtaining high quality instructional data is oftentimes laborious \n",
              "Hence researchers are increasingly focusing on the development of methods to generate \u001b[30;48;2;255;255;0msynthetic\u001b[0m instruction data \n",
              "Wang et al 2023a introduces SELF INSTRUCT where founda Magicoder Empowering Code Generation with OSS INSTRUCT tion \n",
              "LLM GPT Brown et al 2020 is used to gen erate \u001b[30;48;2;255;255;0msynthetic\u001b[0m instruction response pairs with carefully crafted prompts \n",
              "The same LLM is then instruction tuned on the \u001b[30;48;2;255;255;0msynthetic\u001b[0m data to distill such self generated knowledge This \n",
              "technique has been further extended to\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">trained over huge number of code base from scratch establishing the fundamental ability of general code generation \n",
              "and understanding More recent code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> such as DeepSeek Coder Guo et al 2024 and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">StarCoder2</span> Lozhkov et al 2024 \n",
              "additionally organize the pretraining data at the repository level to enhance the model contextual understanding \n",
              "capabilities Furthermore these base models are also finetuned Luo et al 2023b or prompted Chen et al 2023 to unlock\n",
              "their true potential to specialize in solving domain specific coding tasks Instruction tuning with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data \n",
              "Instruction tun ing aims to improve pretrained <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> them with mixture of instructions and \n",
              "corresponding re sponses Wei et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> However obtaining high quality instructional data is oftentimes laborious \n",
              "Hence researchers are increasingly focusing on the development of methods to generate <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> instruction data \n",
              "Wang et al 2023a introduces SELF INSTRUCT where founda Magicoder Empowering Code Generation with OSS INSTRUCT tion \n",
              "LLM GPT Brown et al 2020 is used to gen erate <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> instruction response pairs with carefully crafted prompts \n",
              "The same LLM is then instruction tuned on the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data to distill such self generated knowledge This \n",
              "technique has been further extended to\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "create \u001b[30;48;2;255;255;0msynthetic\u001b[0m data with different LLMs For example Alpaca Taori et al 2023 and Code Alpaca Chaudhary 2023 apply \n",
              "SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m to finetune LLAMA with ChatGPT generated instructions To improve SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m WizardLM Xu et al 2023\n",
              "and WizardCoder Luo et al 2023a propose Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m and Code Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m by guiding ChatGPT with heuristic \n",
              "prompts to make the \u001b[30;48;2;255;255;0msynthetic\u001b[0m data more com plex and diverse More recently Gunasekar et al 2023 shows that textbook\n",
              "quality \u001b[30;48;2;255;255;0msynthetic\u001b[0m data alone can help the model achieve remarkable coding and reasoning capa bilities Orthogonal to\n",
              "all existing methods our proposed OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m allows LLMs to get inspired from real world code snippets for better\n",
              "controllability quality and creativity in coding tasks Evaluating LLMs for code Most code benchmarks eval uate LLMs\n",
              "on generating single function programs from natural language descriptions Such benchmarks include HumanEval Chen et\n",
              "al 2021 MBPP Austin et al 2021 APPS Hendrycks et al 2021 and \u001b[30;48;2;255;255;0mCodeContests\u001b[0m Li et al \u001b[30;48;2;255;255;0m2022\u001b[0m handful of manual tests are\n",
              "used to assess the functional correctness of \u001b[30;48;2;255;255;0mLLM\u001b[0m generated solutions How ever insufficient tests can lead to false \n",
              "negatives\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">create <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data with different LLMs For example Alpaca Taori et al 2023 and Code Alpaca Chaudhary 2023 apply \n",
              "SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> to finetune LLAMA with ChatGPT generated instructions To improve SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> WizardLM Xu et al 2023\n",
              "and WizardCoder Luo et al 2023a propose Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> and Code Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> by guiding ChatGPT with heuristic \n",
              "prompts to make the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data more com plex and diverse More recently Gunasekar et al 2023 shows that textbook\n",
              "quality <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthetic</span> data alone can help the model achieve remarkable coding and reasoning capa bilities Orthogonal to\n",
              "all existing methods our proposed OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> allows LLMs to get inspired from real world code snippets for better\n",
              "controllability quality and creativity in coding tasks Evaluating LLMs for code Most code benchmarks eval uate LLMs\n",
              "on generating single function programs from natural language descriptions Such benchmarks include HumanEval Chen et\n",
              "al 2021 MBPP Austin et al 2021 APPS Hendrycks et al 2021 and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CodeContests</span> Li et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> handful of manual tests are\n",
              "used to assess the functional correctness of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLM</span> generated solutions How ever insufficient tests can lead to false \n",
              "negatives\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Conse quently the EvalPlus framework Liu et al 2023b pro duces HumanEval and MBPP by extending 80 35 more tests To \n",
              "address dataset contamination issues re searchers propose \u001b[30;48;2;255;255;0mLiveCodeBench\u001b[0m Jain et al 2024 which compiles fresh coding\n",
              "problems not included in model training and \u001b[30;48;2;255;255;0mEvoEval\u001b[0m Xia et al 2024 which strategi cally leverages \u001b[30;48;2;255;255;0mLLMs\u001b[0m to evolve \n",
              "existing benchmarks into new coding tasks Meanwhile there are comprehensive benchmarks evaluating code generation \n",
              "for data science DS 1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m addressing open source issues SWE bench Jimenez et al 2023 and repository \n",
              "level code generation CROSSCODEEVAL Ding et al 2023 and RepoEval Zhang et al 2023 Conclusion and Future Work We \n",
              "propose OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m novel data generation method using Large Language Models to generate diverse coding challenges\n",
              "from open source code snippets This approach enables Magicoder which significantly improves the base LLM Despite \n",
              "having less than 7B parameters it can outperform all evaluate \u001b[30;48;2;255;255;0mLLMs\u001b[0m with less than or equal to 16B parameters \n",
              "including the 15B WizardCoder Combin ing OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m with Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m allows us to build the enhanced MagicoderS\n",
              "models They achieve\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Conse quently the EvalPlus framework Liu et al 2023b pro duces HumanEval and MBPP by extending 80 35 more tests To \n",
              "address dataset contamination issues re searchers propose <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LiveCodeBench</span> Jain et al 2024 which compiles fresh coding\n",
              "problems not included in model training and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">EvoEval</span> Xia et al 2024 which strategi cally leverages <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> to evolve \n",
              "existing benchmarks into new coding tasks Meanwhile there are comprehensive benchmarks evaluating code generation \n",
              "for data science DS 1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> addressing open source issues SWE bench Jimenez et al 2023 and repository \n",
              "level code generation CROSSCODEEVAL Ding et al 2023 and RepoEval Zhang et al 2023 Conclusion and Future Work We \n",
              "propose OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> novel data generation method using Large Language Models to generate diverse coding challenges\n",
              "from open source code snippets This approach enables Magicoder which significantly improves the base LLM Despite \n",
              "having less than 7B parameters it can outperform all evaluate <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with less than or equal to 16B parameters \n",
              "including the 15B WizardCoder Combin ing OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> with Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> allows us to build the enhanced MagicoderS\n",
              "models They achieve\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "remark able results by rivaling leading models like \u001b[30;48;2;255;255;0mChatGPT\u001b[0m in HumanEval benchmarks We fully open \u001b[30;48;2;255;255;0msource\u001b[0m the model \n",
              "weights training data and \u001b[30;48;2;255;255;0msource\u001b[0m code to enable future research in \u001b[30;48;2;255;255;0mLLMs\u001b[0m for code In the near future we will ap ply \n",
              "OSS INSTRUCT to larger base models We will also continue advancing OSS INSTRUCT by generating higher quality data \n",
              "with strategically designed distribution of the seed code snippets and with more advanced teacher \u001b[30;48;2;255;255;0mLLMs\u001b[0m such as GPT \n",
              "Acknowledgement We thank all the reviewers for their insightful comments and suggestions for our paper This work \n",
              "was partially supported by NSF grant CCF \u001b[30;48;2;255;255;0m2131943\u001b[0m as well as Kwai Inc Impact Statement This work is motivated to \n",
              "boost large language models in terms of their code generation and understanding ca pabilities through \u001b[30;48;2;255;255;0minstruction\u001b[0m \n",
              "tuning The proposed OSS INSTRUCT method leverages the abundance of open \u001b[30;48;2;255;255;0msource\u001b[0m to generate diverse and controllable\n",
              "\u001b[30;48;2;255;255;0minstruction\u001b[0m data We ex pect this idea to also foster innovative software solutions tai lored to domain specific \n",
              "needs particularly in areas where real data is private and scarce by generating extensive syn thetic data \n",
              "Additionally our method reinforces the value of community driven content and knowledge sharing by incorporating \n",
              "open \u001b[30;48;2;255;255;0msource\u001b[0m code as references However it is essential to recognize the potential for misuse such as the deliberate\n",
              "generation of vulnerable code that can be exploited for malicious purposes Ultimately adhering\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">remark able results by rivaling leading models like <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> in HumanEval benchmarks We fully open <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">source</span> the model \n",
              "weights training data and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">source</span> code to enable future research in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for code In the near future we will ap ply \n",
              "OSS INSTRUCT to larger base models We will also continue advancing OSS INSTRUCT by generating higher quality data \n",
              "with strategically designed distribution of the seed code snippets and with more advanced teacher <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> such as GPT \n",
              "Acknowledgement We thank all the reviewers for their insightful comments and suggestions for our paper This work \n",
              "was partially supported by NSF grant CCF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2131943</span> as well as Kwai Inc Impact Statement This work is motivated to \n",
              "boost large language models in terms of their code generation and understanding ca pabilities through <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> \n",
              "tuning The proposed OSS INSTRUCT method leverages the abundance of open <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">source</span> to generate diverse and controllable\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> data We ex pect this idea to also foster innovative software solutions tai lored to domain specific \n",
              "needs particularly in areas where real data is private and scarce by generating extensive syn thetic data \n",
              "Additionally our method reinforces the value of community driven content and knowledge sharing by incorporating \n",
              "open <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">source</span> code as references However it is essential to recognize the potential for misuse such as the deliberate\n",
              "generation of vulnerable code that can be exploited for malicious purposes Ultimately adhering\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "to \u001b[30;48;2;255;255;0methical\u001b[0m guidelines is crucial to ensure the responsible use of this technique \u001b[30;48;2;255;255;0mReferences\u001b[0m Allal Li Kocetkov Mou \n",
              "Akiki Fer randis Muennighoff Mishra Gu Dey Umapathi Anderson Zi Poirier Schoelkopf Troshin Abulkhanov Romero \n",
              "Lappert Toni del ıo Liu Bose Bhattacharyya Zhuo \u001b[30;48;2;255;255;0mYu\u001b[0m Villegas Zocca Mangrulkar Lansky Nguyen Contractor Villa Li \n",
              "Bahdanau Jernite Hughes Fried Guha de Vries and von \u001b[30;48;2;255;255;0mWerra\u001b[0m \u001b[30;48;2;255;255;0mSantacoder\u001b[0m don reach for the stars 2023 Austin Odena Nye \n",
              "Bosma Michalewski Dohan Jiang\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ethical</span> guidelines is crucial to ensure the responsible use of this technique <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">References</span> Allal Li Kocetkov Mou \n",
              "Akiki Fer randis Muennighoff Mishra Gu Dey Umapathi Anderson Zi Poirier Schoelkopf Troshin Abulkhanov Romero \n",
              "Lappert Toni del ıo Liu Bose Bhattacharyya Zhuo <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Yu</span> Villegas Zocca Mangrulkar Lansky Nguyen Contractor Villa Li \n",
              "Bahdanau Jernite Hughes Fried Guha de Vries and von <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Werra</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Santacoder</span> don reach for the stars 2023 Austin Odena Nye \n",
              "Bosma Michalewski Dohan Jiang\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cai Terry Le and Sutton Program \u001b[30;48;2;255;255;0msynthesis\u001b[0m with large language models CoRR abs 2108 07732 2021 URL https arxiv org \n",
              "abs 2108 07732 Ben Allal Muennighoff Kumar Umapathi \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Lipkin \n",
              "and von Werra framework for the evaluation of code generation models https github com \u001b[30;48;2;255;255;0mbigcode\u001b[0m project \u001b[30;48;2;255;255;0mbigcode\u001b[0m \n",
              "evaluation harness \u001b[30;48;2;255;255;0m2022\u001b[0m Bouzenia Devanbu and Pradel \u001b[30;48;2;255;255;0mRepairagent\u001b[0m An autonomous llm based agent for program repair \n",
              "arXiv preprint arXiv 2403 17134 2024 Brown Mann Ryder Subbiah Kaplan Dhariwal Neelakantan Shyam Sastry Askell \n",
              "Agarwal Herbert Voss Krueger Henighan Child Ramesh Ziegler Wu Winter Hesse\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cai Terry Le and Sutton Program <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">synthesis</span> with large language models CoRR abs 2108 07732 2021 URL https arxiv org \n",
              "abs 2108 07732 Ben Allal Muennighoff Kumar Umapathi <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Lipkin \n",
              "and von Werra framework for the evaluation of code generation models https github com <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">bigcode</span> project <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">bigcode</span> \n",
              "evaluation harness <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> Bouzenia Devanbu and Pradel <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Repairagent</span> An autonomous llm based agent for program repair \n",
              "arXiv preprint arXiv 2403 17134 2024 Brown Mann Ryder Subbiah Kaplan Dhariwal Neelakantan Shyam Sastry Askell \n",
              "Agarwal Herbert Voss Krueger Henighan Child Ramesh Ziegler Wu Winter Hesse\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chen Sigler Litwin Gray \u001b[30;48;2;255;255;0mChess\u001b[0m Clark Berner McCandlish Radford Sutskever and \u001b[30;48;2;255;255;0mAmodei\u001b[0m Language \u001b[30;48;2;255;255;0mmodels\u001b[0m are few shot \n",
              "learners In Larochelle Ranzato Hadsell Balcan and Lin eds Advances in Neural Information Processing Systems volume \n",
              "33 pp 1877 1901 Curran Associates Inc 2020 URL https proceedings neurips cc paper_files paper 2020 file \n",
              "\u001b[30;48;2;255;255;0m1457c0d6bfcb4967418bfb8ac142f64a\u001b[0m Paper pdf Cambronero Gulwani Le Perelman Rad hakrishna Simon and Tiwari Flashfill \n",
              "\u001b[30;48;2;255;255;0mScaling\u001b[0m programming by example by cutting to the chase Proc ACM Program Lang POPL jan 2023 doi 10 1145 3571226 URL \n",
              "https doi org 10 1145 3571226 Cassano Gouwar Nguyen Nguyen Phipps Costin Pinck\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chen Sigler Litwin Gray <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Chess</span> Clark Berner McCandlish Radford Sutskever and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Amodei</span> Language <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">models</span> are few shot \n",
              "learners In Larochelle Ranzato Hadsell Balcan and Lin eds Advances in Neural Information Processing Systems volume \n",
              "33 pp 1877 1901 Curran Associates Inc 2020 URL https proceedings neurips cc paper_files paper 2020 file \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">1457c0d6bfcb4967418bfb8ac142f64a</span> Paper pdf Cambronero Gulwani Le Perelman Rad hakrishna Simon and Tiwari Flashfill \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Scaling</span> programming by example by cutting to the chase Proc ACM Program Lang POPL jan 2023 doi 10 1145 3571226 URL \n",
              "https doi org 10 1145 3571226 Cassano Gouwar Nguyen Nguyen Phipps Costin Pinck\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mBalaji\u001b[0m Jain Saunders Hesse Carr Leike Achiam Misra Morikawa Radford Knight Brundage Murati Mayer Welinder McGrew \n",
              "Amodei McCandlish Sutskever and Zaremba Evaluating large language models trained on code 2021 Chen Lin Scharli and \n",
              "Zhou Teaching large language models to self debug \u001b[30;48;2;255;255;0m2023\u001b[0m Cobbe Kosaraju Bavarian Chen Jun Kaiser Plappert Tworek \n",
              "Hilton Nakano Hesse and Schulman Training \u001b[30;48;2;255;255;0mverifiers\u001b[0m to solve math word problems 2021 \u001b[30;48;2;255;255;0mDeng\u001b[0m Xia Peng Yang and Zhang \n",
              "Large language models are zero shot fuzzers \u001b[30;48;2;255;255;0mFuzzing\u001b[0m deep learning libraries via large language models \u001b[30;48;2;255;255;0m2023\u001b[0m Ding \n",
              "Wang Ahmad Ding Tan Jain\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Balaji</span> Jain Saunders Hesse Carr Leike Achiam Misra Morikawa Radford Knight Brundage Murati Mayer Welinder McGrew \n",
              "Amodei McCandlish Sutskever and Zaremba Evaluating large language models trained on code 2021 Chen Lin Scharli and \n",
              "Zhou Teaching large language models to self debug <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Cobbe Kosaraju Bavarian Chen Jun Kaiser Plappert Tworek \n",
              "Hilton Nakano Hesse and Schulman Training <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">verifiers</span> to solve math word problems 2021 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Deng</span> Xia Peng Yang and Zhang \n",
              "Large language models are zero shot fuzzers <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Fuzzing</span> deep learning libraries via large language models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Ding \n",
              "Wang Ahmad Ding Tan Jain\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Ramanathan Nallapati Bhatia Roth and Xiang \u001b[30;48;2;255;255;0mCrosscodeeval\u001b[0m diverse and multilingual benchmark for cross file code \n",
              "comple tion In Thirty seventh Conference on Neural Informa tion Processing Systems Datasets and \u001b[30;48;2;255;255;0mBenchmarks\u001b[0m Track \n",
              "\u001b[30;48;2;255;255;0m2023\u001b[0m URL https openreview net forum id wgDcbBMSfh Feng Martins Bastani and Dillig Program synthesis using conflict \n",
              "driven learning SIGPLAN Not 53 420 435 jun 2018 ISSN 0362 1340 doi 10 1145 3296979 3192382 URL https doi org 10 \n",
              "1145 3296979 3192382 Fried Aghajanyan Lin Wang Wallace Shi Zhong Yih Zettlemoyer and Lewis Incoder \u001b[30;48;2;255;255;0mgenerative\u001b[0m model\n",
              "for code \u001b[30;48;2;255;255;0minfilling\u001b[0m and synthesis In The Eleventh International Confer ence on Learning Representations \u001b[30;48;2;255;255;0m2023\u001b[0m URL \n",
              "https openreview net forum id hQwb lbM6EL Gulwani Polozov and\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Ramanathan Nallapati Bhatia Roth and Xiang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Crosscodeeval</span> diverse and multilingual benchmark for cross file code \n",
              "comple tion In Thirty seventh Conference on Neural Informa tion Processing Systems Datasets and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Benchmarks</span> Track \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> URL https openreview net forum id wgDcbBMSfh Feng Martins Bastani and Dillig Program synthesis using conflict \n",
              "driven learning SIGPLAN Not 53 420 435 jun 2018 ISSN 0362 1340 doi 10 1145 3296979 3192382 URL https doi org 10 \n",
              "1145 3296979 3192382 Fried Aghajanyan Lin Wang Wallace Shi Zhong Yih Zettlemoyer and Lewis Incoder <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generative</span> model\n",
              "for code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">infilling</span> and synthesis In The Eleventh International Confer ence on Learning Representations <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> URL \n",
              "https openreview net forum id hQwb lbM6EL Gulwani Polozov and\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Singh Program syn thesis Foundations and Trends in Programming Lan guages 119 2017 ISSN 2325 1107 doi 10 1561 \n",
              "\u001b[30;48;2;255;255;0m2500000010\u001b[0m URL http dx doi org 10 1561 \u001b[30;48;2;255;255;0m2500000010\u001b[0m Gunasekar \u001b[30;48;2;255;255;0mZhang\u001b[0m Aneja Mendes Giorno Gopi Javaheripi Kauffmann de \n",
              "Rosa Saarikivi Salim Shah Behl Wang Bubeck Eldan Kalai Lee and Li \u001b[30;48;2;255;255;0mTextbooks\u001b[0m are all you need 2023 Guo Zhu Yang Xie \n",
              "Dong \u001b[30;48;2;255;255;0mZhang\u001b[0m Chen Bi Wu Li Luo Xiong and Liang \u001b[30;48;2;255;255;0mDeepseek\u001b[0m \u001b[30;48;2;255;255;0mcoder\u001b[0m When the large language model meets programming the \n",
              "rise of code intelligence 2024 Hendrycks Basart Kadavath Mazeika Arora Guo\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Singh Program syn thesis Foundations and Trends in Programming Lan guages 119 2017 ISSN 2325 1107 doi 10 1561 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2500000010</span> URL http dx doi org 10 1561 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2500000010</span> Gunasekar <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> Aneja Mendes Giorno Gopi Javaheripi Kauffmann de \n",
              "Rosa Saarikivi Salim Shah Behl Wang Bubeck Eldan Kalai Lee and Li <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Textbooks</span> are all you need 2023 Guo Zhu Yang Xie \n",
              "Dong <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> Chen Bi Wu Li Luo Xiong and Liang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Deepseek</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coder</span> When the large language model meets programming the \n",
              "rise of code intelligence 2024 Hendrycks Basart Kadavath Mazeika Arora Guo\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Burns Puranik He Song and Steinhardt Measuring coding challenge competence with apps 2021 10 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering \n",
              "Code Generation with OSS INSTRUCT \u001b[30;48;2;255;255;0mHonovich\u001b[0m Scialom Levy and Schick Unnat ural instructions Tuning language models \n",
              "with almost no human labor In Rogers Boyd Graber and Okazaki eds Proceedings of the 61st Annual Meet ing of the \n",
              "Association for Computational Linguistics Vol ume Long Papers pp 14409 14428 Toronto Canada July \u001b[30;48;2;255;255;0m2023\u001b[0m Association \n",
              "for Computational Linguistics doi 10 18653 v1 \u001b[30;48;2;255;255;0m2023\u001b[0m acl long 806 URL https aclanthology org \u001b[30;48;2;255;255;0m2023\u001b[0m acl long 806 \n",
              "Hugging Face Hugging face The ai community build ing the future https \u001b[30;48;2;255;255;0mhuggingface\u001b[0m co \u001b[30;48;2;255;255;0m2023\u001b[0m Accessed \u001b[30;48;2;255;255;0m2023\u001b[0m 12 01 \n",
              "Husain Wu Gazit Allamanis and Brockschmidt \u001b[30;48;2;255;255;0mCodesearchnet\u001b[0m challenge Evaluat ing the state of semantic code search \n",
              "2020 Jain Han Gu\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Burns Puranik He Song and Steinhardt Measuring coding challenge competence with apps 2021 10 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering \n",
              "Code Generation with OSS INSTRUCT <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Honovich</span> Scialom Levy and Schick Unnat ural instructions Tuning language models \n",
              "with almost no human labor In Rogers Boyd Graber and Okazaki eds Proceedings of the 61st Annual Meet ing of the \n",
              "Association for Computational Linguistics Vol ume Long Papers pp 14409 14428 Toronto Canada July <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Association \n",
              "for Computational Linguistics doi 10 18653 v1 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> acl long 806 URL https aclanthology org <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> acl long 806 \n",
              "Hugging Face Hugging face The ai community build ing the future https <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">huggingface</span> co <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Accessed <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> 12 01 \n",
              "Husain Wu Gazit Allamanis and Brockschmidt <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Codesearchnet</span> challenge Evaluat ing the state of semantic code search \n",
              "2020 Jain Han Gu\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Li Yan Zhang Wang Solar Lezama Sen and Stoica Live \u001b[30;48;2;255;255;0mcodebench\u001b[0m \u001b[30;48;2;255;255;0mHolistic\u001b[0m and \u001b[30;48;2;255;255;0mcontamination\u001b[0m free evaluation of large \n",
              "language models for code \u001b[30;48;2;255;255;0m2024\u001b[0m Jiang Sablayrolles Mensch Bamford Chaplot de las Casas Bressand Lengyel Lample \n",
              "Saulnier \u001b[30;48;2;255;255;0mLavaud\u001b[0m Lachaux Stock Scao Lavril Wang Lacroix and Sayed Mistral 7b 2023a Jiang Sablayrolles Roux Mensch \n",
              "Savary Bamford Chaplot de las Casas Hanna Bressand Lengyel Bour Lample \u001b[30;48;2;255;255;0mLavaud\u001b[0m Saulnier Lachaux Stock Subramanian \n",
              "Yang Antoniak Scao Gervet\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Li Yan Zhang Wang Solar Lezama Sen and Stoica Live <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codebench</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Holistic</span> and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">contamination</span> free evaluation of large \n",
              "language models for code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2024</span> Jiang Sablayrolles Mensch Bamford Chaplot de las Casas Bressand Lengyel Lample \n",
              "Saulnier <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Lavaud</span> Lachaux Stock Scao Lavril Wang Lacroix and Sayed Mistral 7b 2023a Jiang Sablayrolles Roux Mensch \n",
              "Savary Bamford Chaplot de las Casas Hanna Bressand Lengyel Bour Lample <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Lavaud</span> Saulnier Lachaux Stock Subramanian \n",
              "Yang Antoniak Scao Gervet\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mLavril\u001b[0m Wang Lacroix and Sayed Mixtral of experts 2024 Jiang Liu Lutellier and Tan Impact of code language models on\n",
              "automated program \u001b[30;48;2;255;255;0mrepair\u001b[0m \u001b[30;48;2;255;255;0m2023b\u001b[0m Jimenez Yang Wettig Yao Pei Press and Narasimhan Swe bench Can language mod els \n",
              "resolve real world \u001b[30;48;2;255;255;0mgithub\u001b[0m issues 2023 Kocetkov Li Allal Li Mou Ferrandis Jernite Mitchell Hughes Wolf Bahdanau von \n",
              "Werra and de Vries The stack tb of \u001b[30;48;2;255;255;0mpermissively\u001b[0m licensed source code 2022 Lai Li Wang Zhang Zhong Zettlemoyer tau \n",
              "Yih Fried Wang and Yu Ds 1000 natural and reliable benchmark for data science code generation 2022 Lemieux Inala \n",
              "Lahiri and Sen Co\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Lavril</span> Wang Lacroix and Sayed Mixtral of experts 2024 Jiang Liu Lutellier and Tan Impact of code language models on\n",
              "automated program <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">repair</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Jimenez Yang Wettig Yao Pei Press and Narasimhan Swe bench Can language mod els \n",
              "resolve real world <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">github</span> issues 2023 Kocetkov Li Allal Li Mou Ferrandis Jernite Mitchell Hughes Wolf Bahdanau von \n",
              "Werra and de Vries The stack tb of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">permissively</span> licensed source code 2022 Lai Li Wang Zhang Zhong Zettlemoyer tau \n",
              "Yih Fried Wang and Yu Ds 1000 natural and reliable benchmark for data science code generation 2022 Lemieux Inala \n",
              "Lahiri and Sen Co\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "damosa Escaping \u001b[30;48;2;255;255;0mcoverage\u001b[0m plateaus in test genera tion with pre trained large language models In \u001b[30;48;2;255;255;0m2023\u001b[0m IEEE ACM 45th \n",
              "International Conference on Software Engineering \u001b[30;48;2;255;255;0mICSE\u001b[0m pp 919 931 IEEE \u001b[30;48;2;255;255;0m2023\u001b[0m Li Allal Zi Muennighoff Kocetkov Mou \n",
              "Marone Akiki Li Chim Liu \u001b[30;48;2;255;255;0mZheltonozhskii\u001b[0m Zhuo Wang Dehaene Davaadorj Lamy Poirier Monteiro Shliazhko Gontier Meade \n",
              "\u001b[30;48;2;255;255;0mZebaze\u001b[0m Yee Uma pathi Zhu Lipkin Oblokulov Wang Murthy Stillerman Patel Abulkhanov Zocca Dey Zhang Fahmy Bhat \n",
              "tacharyya Yu Singh Luccioni Villegas Kunakov Zhdanov Romero\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">damosa Escaping <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coverage</span> plateaus in test genera tion with pre trained large language models In <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> IEEE ACM 45th \n",
              "International Conference on Software Engineering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ICSE</span> pp 919 931 IEEE <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Li Allal Zi Muennighoff Kocetkov Mou \n",
              "Marone Akiki Li Chim Liu <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zheltonozhskii</span> Zhuo Wang Dehaene Davaadorj Lamy Poirier Monteiro Shliazhko Gontier Meade \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zebaze</span> Yee Uma pathi Zhu Lipkin Oblokulov Wang Murthy Stillerman Patel Abulkhanov Zocca Dey Zhang Fahmy Bhat \n",
              "tacharyya Yu Singh Luccioni Villegas Kunakov Zhdanov Romero\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Lee Timor Ding Schlesinger Schoelkopf Ebert Dao Mishra Gu Robinson Anderson Dolan Gavitt Contractor Reddy Fried \n",
              "Bahdanau \u001b[30;48;2;255;255;0mJernite\u001b[0m Ferrandis Hughes Wolf Guha von Werra and de Vries \u001b[30;48;2;255;255;0mStarcoder\u001b[0m may the source be with you \u001b[30;48;2;255;255;0m2023\u001b[0m Li \n",
              "Choi Chung Kushman Schrittwieser Leblond Eccles Keeling Gimeno Dal Lago Hubert Choy de Masson Autume Babuschkin \n",
              "Chen Huang Welbl \u001b[30;48;2;255;255;0mGowal\u001b[0m Cherepanov Molloy Mankowitz Sutherland Robson Kohli de Freitas Kavukcuoglu and Vinyals \n",
              "\u001b[30;48;2;255;255;0mCompetition\u001b[0m level code generation with alphac\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Lee Timor Ding Schlesinger Schoelkopf Ebert Dao Mishra Gu Robinson Anderson Dolan Gavitt Contractor Reddy Fried \n",
              "Bahdanau <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Jernite</span> Ferrandis Hughes Wolf Guha von Werra and de Vries <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Starcoder</span> may the source be with you <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Li \n",
              "Choi Chung Kushman Schrittwieser Leblond Eccles Keeling Gimeno Dal Lago Hubert Choy de Masson Autume Babuschkin \n",
              "Chen Huang Welbl <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Gowal</span> Cherepanov Molloy Mankowitz Sutherland Robson Kohli de Freitas Kavukcuoglu and Vinyals \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Competition</span> level code generation with alphac\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ode Science 378 6624 1092 1097 December 2022 ISSN 1095 9203 doi 10 1126 science abq1158 URL http dx doi org 10 1126\n",
              "science abq1158 Liu Peng Wang and Zhang \u001b[30;48;2;255;255;0mNeuri\u001b[0m Di versifying dnn generation via inductive rule inference In \n",
              "Proceedings of the 31st ACM Joint European Soft ware Engineering Conference and Symposium on the Foundations of \n",
              "Software Engineering ESEC FSE \u001b[30;48;2;255;255;0m2023\u001b[0m pp 657 669 New York NY USA 2023a Associa tion for Computing Machinery ISBN \n",
              "9798400703270 doi 10 1145 3611643 3616337 URL https doi org 10 1145 3611643 3616337 Liu Xia Wang and Zhang Is your \n",
              "code \u001b[30;48;2;255;255;0mgenerated\u001b[0m by \u001b[30;48;2;255;255;0mchatGPT\u001b[0m really correct rigorous evaluation of large language models for code generation In Thirty\n",
              "seventh Conference on Neural Information Processing Systems 2023b URL https openreview net forum id \u001b[30;48;2;255;255;0m1qvx610Cu7\u001b[0m \n",
              "Lozhkov Li Allal Cassano Lamy Poir\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ode Science 378 6624 1092 1097 December 2022 ISSN 1095 9203 doi 10 1126 science abq1158 URL http dx doi org 10 1126\n",
              "science abq1158 Liu Peng Wang and Zhang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Neuri</span> Di versifying dnn generation via inductive rule inference In \n",
              "Proceedings of the 31st ACM Joint European Soft ware Engineering Conference and Symposium on the Foundations of \n",
              "Software Engineering ESEC FSE <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> pp 657 669 New York NY USA 2023a Associa tion for Computing Machinery ISBN \n",
              "9798400703270 doi 10 1145 3611643 3616337 URL https doi org 10 1145 3611643 3616337 Liu Xia Wang and Zhang Is your \n",
              "code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generated</span> by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chatGPT</span> really correct rigorous evaluation of large language models for code generation In Thirty\n",
              "seventh Conference on Neural Information Processing Systems 2023b URL https openreview net forum id <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">1qvx610Cu7</span> \n",
              "Lozhkov Li Allal Cassano Lamy Poir\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mier\u001b[0m Tazi Tang Pykhtar Liu Wei Liu Tian Kocetkov Zucker Belkada Wang Liu Abulkhanov Paul Li Li Risdal Li Zhu \u001b[30;48;2;255;255;0mZhuo\u001b[0m \n",
              "Zheltonozhskii Dade Yu Krauß Jain Su He Dey Abati Chai Muennighoff Tang Oblokulov Akiki Marone Mou Mishra Gu Hui \n",
              "Dao Zebaze Dehaene Patry Xu McAuley Hu 11 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m \u001b[30;48;2;255;255;0mEmpowering\u001b[0m \u001b[30;48;2;255;255;0mCode\u001b[0m Generation with OSS INSTRUCT Scholak Paquet \n",
              "Robinson Anderson Chapados Patwary Tajbakhsh Jernite Ferr\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ier</span> Tazi Tang Pykhtar Liu Wei Liu Tian Kocetkov Zucker Belkada Wang Liu Abulkhanov Paul Li Li Risdal Li Zhu <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhuo</span> \n",
              "Zheltonozhskii Dade Yu Krauß Jain Su He Dey Abati Chai Muennighoff Tang Oblokulov Akiki Marone Mou Mishra Gu Hui \n",
              "Dao Zebaze Dehaene Patry Xu McAuley Hu 11 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Empowering</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Generation with OSS INSTRUCT Scholak Paquet \n",
              "Robinson Anderson Chapados Patwary Tajbakhsh Jernite Ferr\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "andis Zhang Hughes Wolf Guha von Werra and de Vries Starcoder and the stack v2 The next generation 2024 Luo Xu Zhao\n",
              "Sun Geng Hu Tao Ma Lin and Jiang \u001b[30;48;2;255;255;0mWizardcoder\u001b[0m Empowering code large language models with \u001b[30;48;2;255;255;0mevol\u001b[0m instruct arXiv \n",
              "preprint arXiv 2306 08568 2023a Luo Xu Zhao Sun Geng Hu Tao Ma Lin and Jiang \u001b[30;48;2;255;255;0mWizardcoder\u001b[0m Empowering code large \n",
              "language models with \u001b[30;48;2;255;255;0mevol\u001b[0m instruct \u001b[30;48;2;255;255;0m2023b\u001b[0m Microsoft Azure \u001b[30;48;2;255;255;0mopenai\u001b[0m service models https learn microsoft com en us \n",
              "azure cognitive services \u001b[30;48;2;255;255;0mopenai\u001b[0m concepts models 2023a Microsoft GitHub Copilot Your AI pair pro grammer https \n",
              "github com features copilot \u001b[30;48;2;255;255;0m2023b\u001b[0m Muennighoff Liu Zebaze Zheng Hui \u001b[30;48;2;255;255;0mZhuo\u001b[0m Singh\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">andis Zhang Hughes Wolf Guha von Werra and de Vries Starcoder and the stack v2 The next generation 2024 Luo Xu Zhao\n",
              "Sun Geng Hu Tao Ma Lin and Jiang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Wizardcoder</span> Empowering code large language models with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">evol</span> instruct arXiv \n",
              "preprint arXiv 2306 08568 2023a Luo Xu Zhao Sun Geng Hu Tao Ma Lin and Jiang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Wizardcoder</span> Empowering code large \n",
              "language models with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">evol</span> instruct <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Microsoft Azure <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">openai</span> service models https learn microsoft com en us \n",
              "azure cognitive services <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">openai</span> concepts models 2023a Microsoft GitHub Copilot Your AI pair pro grammer https \n",
              "github com features copilot <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023b</span> Muennighoff Liu Zebaze Zheng Hui <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhuo</span> Singh\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tang von Werra and Longpre Octopack Instruction \u001b[30;48;2;255;255;0mtuning\u001b[0m code large language models 2023 Nijkamp Pang Hayashi Tu Wang\n",
              "Zhou Savarese and Xiong \u001b[30;48;2;255;255;0mCodegen\u001b[0m An open large language model for code with multi turn pro gram synthesis In The \n",
              "Eleventh International Confer ence on Learning Representations 2023 URL https openreview net forum id iaYcJKpY2B_ \n",
              "Olausson Inala Wang Gao and Solar Lezama Is self repair silver bullet for code generation In The Twelfth \n",
              "International Conference on Learning Representations 2024 URL https openreview net forum id y0GJXRungR \u001b[30;48;2;255;255;0mOpenAI\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mChatgpt\u001b[0m Optimizing language models for dialogue https \u001b[30;48;2;255;255;0mopenai\u001b[0m com blog \u001b[30;48;2;255;255;0mchatgpt\u001b[0m \u001b[30;48;2;255;255;0m2022\u001b[0m \u001b[30;48;2;255;255;0mOpenAI\u001b[0m Gpt technical report 2023\n",
              "Roziere Gehring Gloeckle Sootla Gat Tan Adi Liu Remez\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Tang von Werra and Longpre Octopack Instruction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tuning</span> code large language models 2023 Nijkamp Pang Hayashi Tu Wang\n",
              "Zhou Savarese and Xiong <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Codegen</span> An open large language model for code with multi turn pro gram synthesis In The \n",
              "Eleventh International Confer ence on Learning Representations 2023 URL https openreview net forum id iaYcJKpY2B_ \n",
              "Olausson Inala Wang Gao and Solar Lezama Is self repair silver bullet for code generation In The Twelfth \n",
              "International Conference on Learning Representations 2024 URL https openreview net forum id y0GJXRungR <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OpenAI</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Chatgpt</span> Optimizing language models for dialogue https <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">openai</span> com blog <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chatgpt</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OpenAI</span> Gpt technical report 2023\n",
              "Roziere Gehring Gloeckle Sootla Gat Tan Adi Liu Remez\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rapin Kozhevnikov Evtimov Bitton Bhatt Ferrer Grattafiori Xiong Defossez Copet Azhar Touvron Martin Usunier Scialom\n",
              "and Synnaeve Code \u001b[30;48;2;255;255;0mllama\u001b[0m Open foundation models for code \u001b[30;48;2;255;255;0m2023\u001b[0m Schafer Nadi Eghbali and Tip An empirical evaluation \n",
              "of using large language models for automated unit test generation IEEE Transactions on Software En gineering \u001b[30;48;2;255;255;0m2023\u001b[0m \n",
              "Services AI Code Generator Amazon Code Whisperer AWS https aws amazon com \u001b[30;48;2;255;255;0mcodewhisperer\u001b[0m \u001b[30;48;2;255;255;0m2023\u001b[0m Shazeer and Stern \n",
              "Adafactor Adaptive learning rates with sublinear memory cost 2018 SPARCK JONES statistical interpretation of term \n",
              "\u001b[30;48;2;255;255;0mspecificity\u001b[0m and its application in retrieval 28 11 21 \u001b[30;48;2;255;255;0m2023\u001b[0m 11 30 1972 doi 10 1108 \u001b[30;48;2;255;255;0meb026526\u001b[0m URL https doi org 10 \n",
              "1108 \u001b[30;48;2;255;255;0meb026526\u001b[0m Su Shi Kasai\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rapin Kozhevnikov Evtimov Bitton Bhatt Ferrer Grattafiori Xiong Defossez Copet Azhar Touvron Martin Usunier Scialom\n",
              "and Synnaeve Code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">llama</span> Open foundation models for code <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Schafer Nadi Eghbali and Tip An empirical evaluation \n",
              "of using large language models for automated unit test generation IEEE Transactions on Software En gineering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> \n",
              "Services AI Code Generator Amazon Code Whisperer AWS https aws amazon com <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codewhisperer</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Shazeer and Stern \n",
              "Adafactor Adaptive learning rates with sublinear memory cost 2018 SPARCK JONES statistical interpretation of term \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">specificity</span> and its application in retrieval 28 11 21 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> 11 30 1972 doi 10 1108 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">eb026526</span> URL https doi org 10 \n",
              "1108 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">eb026526</span> Su Shi Kasai\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Wang Hu Ostendorf Yih Smith Zettlemoyer and Yu One \u001b[30;48;2;255;255;0membedder\u001b[0m any task \u001b[30;48;2;255;255;0mInstruction\u001b[0m finetuned text em beddings \u001b[30;48;2;255;255;0m2022\u001b[0m \n",
              "URL https arxiv org abs 2212 09741 Taori Gulrajani Zhang Dubois Li \u001b[30;48;2;255;255;0mGuestrin\u001b[0m Liang and Hashimoto Stanford alpaca An \n",
              "\u001b[30;48;2;255;255;0minstruction\u001b[0m following llama model https github com tatsu lab stanford_alpaca 2023 theblackcat102 The evolved code \n",
              "alpaca dataset https huggingface co datasets theblackcat102 evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 2023 Wang Dillig and Singh Program \n",
              "synthesis using abstraction refinement Proc ACM Program Lang POPL dec 2017 doi 10 1145 3158151 URL https doi org 10\n",
              "1145 3158151 Wang Wang Joty and Hoi CodeT5 Identifier\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Wang Hu Ostendorf Yih Smith Zettlemoyer and Yu One <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">embedder</span> any task <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruction</span> finetuned text em beddings <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> \n",
              "URL https arxiv org abs 2212 09741 Taori Gulrajani Zhang Dubois Li <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Guestrin</span> Liang and Hashimoto Stanford alpaca An \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">instruction</span> following llama model https github com tatsu lab stanford_alpaca 2023 theblackcat102 The evolved code \n",
              "alpaca dataset https huggingface co datasets theblackcat102 evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 2023 Wang Dillig and Singh Program \n",
              "synthesis using abstraction refinement Proc ACM Program Lang POPL dec 2017 doi 10 1145 3158151 URL https doi org 10\n",
              "1145 3158151 Wang Wang Joty and Hoi CodeT5 Identifier\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "aware unified pre trained \u001b[30;48;2;255;255;0mencoder\u001b[0m decoder mod els for code understanding and generation In Moens Huang Specia and \n",
              "Yih eds Pro ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pp 8696 8708 On \n",
              "line and Punta Cana Dominican Republic November 2021 Association for \u001b[30;48;2;255;255;0mComputational\u001b[0m Linguistics doi 10 18653 v1 2021\n",
              "\u001b[30;48;2;255;255;0memnlp\u001b[0m main 685 URL https aclanthology org 2021 \u001b[30;48;2;255;255;0memnlp\u001b[0m main 685 Wang Kordi Mishra Liu Smith Khashabi and Hajishirzi \n",
              "Self instruct Align ing language models with self generated instructions In Rogers Boyd Graber and Okazaki eds Pro \n",
              "ceedings of the 61st Annual Meeting of the Association for \u001b[30;48;2;255;255;0mComputational\u001b[0m Linguistics Volume Long Papers pp 13484 \n",
              "13508 Toronto Canada July \u001b[30;48;2;255;255;0m2023a\u001b[0m \u001b[30;48;2;255;255;0mAssoci\u001b[0m ation for \u001b[30;48;2;255;255;0mComputational\u001b[0m Linguistics doi 10 18653 v1 2023 acl long 754 URL \n",
              "https aclanthology org 2023 acl long\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">aware unified pre trained <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">encoder</span> decoder mod els for code understanding and generation In Moens Huang Specia and \n",
              "Yih eds Pro ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pp 8696 8708 On \n",
              "line and Punta Cana Dominican Republic November 2021 Association for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Computational</span> Linguistics doi 10 18653 v1 2021\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">emnlp</span> main 685 URL https aclanthology org 2021 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">emnlp</span> main 685 Wang Kordi Mishra Liu Smith Khashabi and Hajishirzi \n",
              "Self instruct Align ing language models with self generated instructions In Rogers Boyd Graber and Okazaki eds Pro \n",
              "ceedings of the 61st Annual Meeting of the Association for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Computational</span> Linguistics Volume Long Papers pp 13484 \n",
              "13508 Toronto Canada July <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023a</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Associ</span> ation for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Computational</span> Linguistics doi 10 18653 v1 2023 acl long 754 URL \n",
              "https aclanthology org 2023 acl long\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "754 12 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering \u001b[30;48;2;255;255;0mCode\u001b[0m Generation with OSS INSTRUCT Wang Le Gotmare Bui Li and Hoi Codet5 Open \u001b[30;48;2;255;255;0mcode\u001b[0m large\n",
              "language models for \u001b[30;48;2;255;255;0mcode\u001b[0m understanding and generation 2023b Wei Bosma Zhao Guu Yu Lester Du Dai and Le Finetuned \n",
              "language models are zero shot learners \u001b[30;48;2;255;255;0m2022\u001b[0m Wei Xia and Zhang Copiloting the copilots Fusing large language models \n",
              "with completion engines for automated program repair 2023 Xia and Zhang Less training more repairing please \n",
              "Revisiting automated program repair via zero shot learning \u001b[30;48;2;255;255;0m2022\u001b[0m Xia and Zhang Keep the conversation going Fixing \n",
              "162 out of 337 bugs for 42 each using \u001b[30;48;2;255;255;0mchatgpt\u001b[0m arXiv preprint arXiv 2304 00385 2023 Xia Paltenghi Tian Pradel and \n",
              "Zhang Universal \u001b[30;48;2;255;255;0mfuzzing\u001b[0m via large language models 20\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">754 12 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Generation with OSS INSTRUCT Wang Le Gotmare Bui Li and Hoi Codet5 Open <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> large\n",
              "language models for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> understanding and generation 2023b Wei Bosma Zhao Guu Yu Lester Du Dai and Le Finetuned \n",
              "language models are zero shot learners <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> Wei Xia and Zhang Copiloting the copilots Fusing large language models \n",
              "with completion engines for automated program repair 2023 Xia and Zhang Less training more repairing please \n",
              "Revisiting automated program repair via zero shot learning <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> Xia and Zhang Keep the conversation going Fixing \n",
              "162 out of 337 bugs for 42 each using <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chatgpt</span> arXiv preprint arXiv 2304 00385 2023 Xia Paltenghi Tian Pradel and \n",
              "Zhang Universal <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">fuzzing</span> via large language models 20\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "23a Xia Wei and \u001b[30;48;2;255;255;0mZhang\u001b[0m \u001b[30;48;2;255;255;0mAutomated\u001b[0m program repair in the era of large pre trained language models In \u001b[30;48;2;255;255;0m2023\u001b[0m IEEE ACM \n",
              "45th International Conference on Software Engineering ICSE pp 1482 1494 2023b doi 10 1109 ICSE48619 \u001b[30;48;2;255;255;0m2023\u001b[0m 00129 Xia \n",
              "Deng and \u001b[30;48;2;255;255;0mZhang\u001b[0m Top leaderboard ranking top coding proficiency always evoeval Evolving coding benchmarks via \u001b[30;48;2;255;255;0mllm\u001b[0m \n",
              "2024 Xu Sun Zheng Geng Zhao Feng Tao and Jiang Wizardlm Empowering large language models to follow complex \n",
              "instructions arXiv preprint arXiv 2304 12244 \u001b[30;48;2;255;255;0m2023\u001b[0m Yu Zhuang \u001b[30;48;2;255;255;0mZhang\u001b[0m Meng Ratner Krishna Shen and \u001b[30;48;2;255;255;0mZhang\u001b[0m Large language\n",
              "model as attributed training data generator tale of diversity and bias \u001b[30;48;2;255;255;0m2023\u001b[0m Yuan Lou Liu Ding Wang Chen and Peng No\n",
              "more manual tests evaluating and improving \u001b[30;48;2;255;255;0mchatgpt\u001b[0m for unit test generation arXiv preprint\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">23a Xia Wei and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Automated</span> program repair in the era of large pre trained language models In <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> IEEE ACM \n",
              "45th International Conference on Software Engineering ICSE pp 1482 1494 2023b doi 10 1109 ICSE48619 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> 00129 Xia \n",
              "Deng and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> Top leaderboard ranking top coding proficiency always evoeval Evolving coding benchmarks via <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">llm</span> \n",
              "2024 Xu Sun Zheng Geng Zhao Feng Tao and Jiang Wizardlm Empowering large language models to follow complex \n",
              "instructions arXiv preprint arXiv 2304 12244 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Yu Zhuang <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> Meng Ratner Krishna Shen and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Zhang</span> Large language\n",
              "model as attributed training data generator tale of diversity and bias <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Yuan Lou Liu Ding Wang Chen and Peng No\n",
              "more manual tests evaluating and improving <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chatgpt</span> for unit test generation arXiv preprint\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "arXiv 2305 04207 \u001b[30;48;2;255;255;0m2023\u001b[0m Zhang Chen Zhang Keung Liu Zan Mao Lou and Chen \u001b[30;48;2;255;255;0mRepocoder\u001b[0m Repository level code completion \n",
              "through \u001b[30;48;2;255;255;0miterative\u001b[0m re trieval and generation \u001b[30;48;2;255;255;0m2023\u001b[0m 13 Magicoder Empowering Code Generation with \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m You are \n",
              "exceptionally skilled at crafting high quality programming problems and offering precise solutions Please gain \n",
              "inspiration from the following random code snippet to create high quality programming problem Present your output \n",
              "in two distinct sections Problem Description and Solution Code snippet for inspiration code Guidelines for each \n",
              "section Problem Description This should be completely self contained providing all the contextual information one \n",
              "needs to understand and solve the problem Assume common programming knowledge but ensure that any specific context \n",
              "variables or code snippets pertinent to this problem are explicitly included Solution Offer comprehensive correct \n",
              "solution that accurately addresses the Problem Description you provided Figure The detailed prompt design for \u001b[30;48;2;255;255;0mOSS\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m More Details of \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m Prompt Design Figure illustrates the prompt template of \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m where \n",
              "the first\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">arXiv 2305 04207 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> Zhang Chen Zhang Keung Liu Zan Mao Lou and Chen <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Repocoder</span> Repository level code completion \n",
              "through <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">iterative</span> re trieval and generation <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2023</span> 13 Magicoder Empowering Code Generation with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> You are \n",
              "exceptionally skilled at crafting high quality programming problems and offering precise solutions Please gain \n",
              "inspiration from the following random code snippet to create high quality programming problem Present your output \n",
              "in two distinct sections Problem Description and Solution Code snippet for inspiration code Guidelines for each \n",
              "section Problem Description This should be completely self contained providing all the contextual information one \n",
              "needs to understand and solve the problem Assume common programming knowledge but ensure that any specific context \n",
              "variables or code snippets pertinent to this problem are explicitly included Solution Offer comprehensive correct \n",
              "solution that accurately addresses the Problem Description you provided Figure The detailed prompt design for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> More Details of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> Prompt Design Figure illustrates the prompt template of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> where \n",
              "the first\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "section presents \u001b[30;48;2;255;255;0mhigh\u001b[0m level description of the task the second section incorporates the code snippet and the third \n",
              "section offers guidelines on the response Qualitative Examples Figure extends Figure and shows more qualitative \n",
              "examples of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m generation including outputs that are inspired by method definitions shell scripts library\n",
              "imports class signatures code statements and code comments Breakdown of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m Dataset Categories To study \n",
              "the categories of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m generated data we use INSTRUCTOR Su et al \u001b[30;48;2;255;255;0m2022\u001b[0m which is one of the SOTA embedding \n",
              "models and can generate different text \u001b[30;48;2;255;255;0membeddings\u001b[0m according to task instruction Inspired by OctoPack Muennighoff et\n",
              "al 2023 and the topic tags on GitHub we manually designed 10 categories specific to coding As shown in Figure we \n",
              "calculate the cosine similarity between the \u001b[30;48;2;255;255;0membeddings\u001b[0m of each sample in OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and the 10 categories to \n",
              "obtain the category breakdown Overall OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m exhibits diversity and balance across different categories \n",
              "Length distribution We depict the length distribution for both generated problems and solutions in Figure The axis \n",
              "represents the number of tokens in each problem solution while the axis shows the correspondent number of samples \n",
              "Implementation Details Data Generation We use \u001b[30;48;2;255;255;0mgpt\u001b[0m turbo 1106 as the foundation model\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">section presents <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high</span> level description of the task the second section incorporates the code snippet and the third \n",
              "section offers guidelines on the response Qualitative Examples Figure extends Figure and shows more qualitative \n",
              "examples of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> generation including outputs that are inspired by method definitions shell scripts library\n",
              "imports class signatures code statements and code comments Breakdown of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> Dataset Categories To study \n",
              "the categories of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> generated data we use INSTRUCTOR Su et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> which is one of the SOTA embedding \n",
              "models and can generate different text <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">embeddings</span> according to task instruction Inspired by OctoPack Muennighoff et\n",
              "al 2023 and the topic tags on GitHub we manually designed 10 categories specific to coding As shown in Figure we \n",
              "calculate the cosine similarity between the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">embeddings</span> of each sample in OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and the 10 categories to \n",
              "obtain the category breakdown Overall OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> exhibits diversity and balance across different categories \n",
              "Length distribution We depict the length distribution for both generated problems and solutions in Figure The axis \n",
              "represents the number of tokens in each problem solution while the axis shows the correspondent number of samples \n",
              "Implementation Details Data Generation We use <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">gpt</span> turbo 1106 as the foundation model\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "to do OSS INSTRUCT due to its high cost effectiveness We randomly extract 15 lines from each selected \u001b[30;48;2;255;255;0mcode\u001b[0m document\n",
              "from \u001b[30;48;2;255;255;0mstarcoderdata\u001b[0m and let gpt turbo 1106 imagine self contained coding problem and correct solution Given the \n",
              "numerous seed \u001b[30;48;2;255;255;0mcode\u001b[0m snippets we perform greedy decoding to maximize the consistency between the \u001b[30;48;2;255;255;0mgenerated\u001b[0m problems \n",
              "and solutions 14 Magicoder Empowering \u001b[30;48;2;255;255;0mCode\u001b[0m Generation with OSS INSTRUCT Seed method definition render Problem Your \n",
              "task is to complete the render method to generate the rendered shape as string \u001b[30;48;2;255;255;0mCode\u001b[0m class \u001b[30;48;2;255;255;0mShapeRenderer\u001b[0m constructor\n",
              "vertices this vertices vertices render let renderedShape for let this vertices length const vertex this vertices \n",
              "renderedShape vertex vertex if this vertices length renderedShape return renderedShape Seed library imports import \n",
              "numpy as np import \u001b[30;48;2;255;255;0mgym_electric_motor\u001b[0m as gem import matplotlib pyplot as plt Problem Create reinforcement learning \n",
              "agent to control an electric motor using the OpenAI Gym\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">to do OSS INSTRUCT due to its high cost effectiveness We randomly extract 15 lines from each selected <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> document\n",
              "from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">starcoderdata</span> and let gpt turbo 1106 imagine self contained coding problem and correct solution Given the \n",
              "numerous seed <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> snippets we perform greedy decoding to maximize the consistency between the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">generated</span> problems \n",
              "and solutions 14 Magicoder Empowering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Generation with OSS INSTRUCT Seed method definition render Problem Your \n",
              "task is to complete the render method to generate the rendered shape as string <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ShapeRenderer</span> constructor\n",
              "vertices this vertices vertices render let renderedShape for let this vertices length const vertex this vertices \n",
              "renderedShape vertex vertex if this vertices length renderedShape return renderedShape Seed library imports import \n",
              "numpy as np import <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">gym_electric_motor</span> as gem import matplotlib pyplot as plt Problem Create reinforcement learning \n",
              "agent to control an electric motor using the OpenAI Gym\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "environment Code import numpy as np import gym_electric_motor as gem env gem make DcSeriesCont v1 class DQNAgent \n",
              "def \u001b[30;48;2;255;255;0m__init__\u001b[0m self \u001b[30;48;2;255;255;0mstate_dim\u001b[0m action_dim def build_model self def act self state def train self state action reward \n",
              "for episode in range episodes state env reset state np reshape state \u001b[30;48;2;255;255;0mstate_dim\u001b[0m Seed class signature \n",
              "SpringBootApplication Import AxonConfig class public class \u001b[30;48;2;255;255;0mAxonbankApplication\u001b[0m public static void main String \u001b[30;48;2;255;255;0margs\u001b[0m \n",
              "Problem Create simple Java Spring Boot application for banking system Code import org axonframework commandhandling\n",
              "CommandBus import org axonframework config Configuration SpringBootApplication Import AxonConfig class public class\n",
              "\u001b[30;48;2;255;255;0mAxonbankApplication\u001b[0m public class BankAccount public class CreateAccountCommand public class DepositFundsCommand \n",
              "public class \u001b[30;48;2;255;255;0mWithdrawFundsCommand\u001b[0m public class AccountCreatedEvent public class FundsDepositedEvent public class \n",
              "FundsWithdrawnEvent Seed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">environment Code import numpy as np import gym_electric_motor as gem env gem make DcSeriesCont v1 class DQNAgent \n",
              "def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__init__</span> self <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">state_dim</span> action_dim def build_model self def act self state def train self state action reward \n",
              "for episode in range episodes state env reset state np reshape state <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">state_dim</span> Seed class signature \n",
              "SpringBootApplication Import AxonConfig class public class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">AxonbankApplication</span> public static void main String <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">args</span> \n",
              "Problem Create simple Java Spring Boot application for banking system Code import org axonframework commandhandling\n",
              "CommandBus import org axonframework config Configuration SpringBootApplication Import AxonConfig class public class\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">AxonbankApplication</span> public class BankAccount public class CreateAccountCommand public class DepositFundsCommand \n",
              "public class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WithdrawFundsCommand</span> public class AccountCreatedEvent public class FundsDepositedEvent public class \n",
              "FundsWithdrawnEvent Seed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "code statements cutoff_range np ptp cutoffs if cutoff_range cutoff_range cutoff_colors plt get_cmap plasma cutoffs \n",
              "np min cutoffs cutoff_range Problem Implement function that calculates the color values for given set of cutof \n",
              "values based on specified color map Code import numpy as np import matplotlib pyplot as plt def \n",
              "\u001b[30;48;2;255;255;0mcalculate_cutoff_colors\u001b[0m cutoffs cmap_name cutoff_range np ptp cutoffs if cutoff_range cutoff_range \u001b[30;48;2;255;255;0mcmap\u001b[0m plt \n",
              "get_cmap cmap_name normalized_cutoffs cutoff_colors return cutoff_colors Seed shell script python3 \u001b[30;48;2;255;255;0mmakeErrorFile\u001b[0m \u001b[30;48;2;255;255;0mpy\u001b[0m\n",
              "data \u001b[30;48;2;255;255;0mtest_dataset_14\u001b[0m 14 Problem Create Python program that generates an error file based on given dataset Code def \n",
              "generate_error_file dataset_file error_lines with open dataset_file as file for line in file with open \n",
              "error_file_name as error_file for error_line in error_lines\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">code statements cutoff_range np ptp cutoffs if cutoff_range cutoff_range cutoff_colors plt get_cmap plasma cutoffs \n",
              "np min cutoffs cutoff_range Problem Implement function that calculates the color values for given set of cutof \n",
              "values based on specified color map Code import numpy as np import matplotlib pyplot as plt def \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">calculate_cutoff_colors</span> cutoffs cmap_name cutoff_range np ptp cutoffs if cutoff_range cutoff_range <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">cmap</span> plt \n",
              "get_cmap cmap_name normalized_cutoffs cutoff_colors return cutoff_colors Seed shell script python3 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">makeErrorFile</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">py</span>\n",
              "data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">test_dataset_14</span> 14 Problem Create Python program that generates an error file based on given dataset Code def \n",
              "generate_error_file dataset_file error_lines with open dataset_file as file for line in file with open \n",
              "error_file_name as error_file for error_line in error_lines\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "error_file write error_line if \u001b[30;48;2;255;255;0m__name__\u001b[0m __main__ if len sys argv print Usage else dataset_file sys argv \n",
              "dataset_number sys argv generate_error_file Seed comments Set degrees Problem Implement \u001b[30;48;2;255;255;0mPython\u001b[0m class that \n",
              "represents temperature in degrees Code class TemperatureConverter def __init__ \u001b[30;48;2;255;255;0mself\u001b[0m def set_celsius \u001b[30;48;2;255;255;0mself\u001b[0m degrees \n",
              "def set_fahrenheit \u001b[30;48;2;255;255;0mself\u001b[0m degrees def set_kelvin \u001b[30;48;2;255;255;0mself\u001b[0m degrees def \u001b[30;48;2;255;255;0mget_celsius\u001b[0m \u001b[30;48;2;255;255;0mself\u001b[0m def get_fahrenheit \u001b[30;48;2;255;255;0mself\u001b[0m def \n",
              "get_kelvin \u001b[30;48;2;255;255;0mself\u001b[0m def convert_to \u001b[30;48;2;255;255;0mself\u001b[0m unit if unit return \u001b[30;48;2;255;255;0mself\u001b[0m \u001b[30;48;2;255;255;0mget_celsius\u001b[0m elif unit return \u001b[30;48;2;255;255;0mself\u001b[0m get_fahrenheit elif \n",
              "unit return \u001b[30;48;2;255;255;0mself\u001b[0m get_kelvin Figure More examples showing how OSS INSTRUCT generates problems and solutions from \n",
              "seed code snippets Detailed problem requirements implementations and explanations are omitted for brevity 15 \n",
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">error_file write error_line if <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__name__</span> __main__ if len sys argv print Usage else dataset_file sys argv \n",
              "dataset_number sys argv generate_error_file Seed comments Set degrees Problem Implement <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Python</span> class that \n",
              "represents temperature in degrees Code class TemperatureConverter def __init__ <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> def set_celsius <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> degrees \n",
              "def set_fahrenheit <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> degrees def set_kelvin <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> degrees def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">get_celsius</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> def get_fahrenheit <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> def \n",
              "get_kelvin <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> def convert_to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> unit if unit return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">get_celsius</span> elif unit return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> get_fahrenheit elif \n",
              "unit return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">self</span> get_kelvin Figure More examples showing how OSS INSTRUCT generates problems and solutions from \n",
              "seed code snippets Detailed problem requirements implementations and explanations are omitted for brevity 15 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Figure The category constitution of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m 100 200 300 400 500 600 700 Number of Tokens Count Thousand \n",
              "problem solution Figure Token count distribution of OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m generated problems and solutions Data \n",
              "Decontamination We apply data decontamination before training our \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS models Following Li et \n",
              "al 2023 we decontaminate both our 75K OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m dataset and the evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 theblackcat102 2023 dataset \n",
              "an open source reproduction of Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m generated by GPT OpenAI 2023 by removing exact matches from HumanEval \n",
              "Chen et al 2021 MBPP Austin et al 2021 DS 1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m and GSM8K Cobbe et al 2021 Eventually we filtered out\n",
              "problems for OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m dataset and 89 for evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 Training We employ CODELLAMA PYTHON 7B and DeepSeek\n",
              "Coder Base 7B as the base \u001b[30;48;2;255;255;0mLLMs\u001b[0m To obtain \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series we first finetune the base models on about 75K synthetic \n",
              "data generated through OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m using the\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Figure The category constitution of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> 100 200 300 400 500 600 700 Number of Tokens Count Thousand \n",
              "problem solution Figure Token count distribution of OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> generated problems and solutions Data \n",
              "Decontamination We apply data decontamination before training our <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS models Following Li et \n",
              "al 2023 we decontaminate both our 75K OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> dataset and the evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 theblackcat102 2023 dataset \n",
              "an open source reproduction of Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> generated by GPT OpenAI 2023 by removing exact matches from HumanEval \n",
              "Chen et al 2021 MBPP Austin et al 2021 DS 1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> and GSM8K Cobbe et al 2021 Eventually we filtered out\n",
              "problems for OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> dataset and 89 for evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 Training We employ CODELLAMA PYTHON 7B and DeepSeek\n",
              "Coder Base 7B as the base <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> To obtain <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series we first finetune the base models on about 75K synthetic \n",
              "data generated through OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> using the\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "transformers library from Hugging Face Hugging Face 2023 We finetune the base models for epochs using two NVIDIA \n",
              "A100 \u001b[30;48;2;255;255;0m80GB\u001b[0m GPUs through the Distributed Data Parallel DDP module from \u001b[30;48;2;255;255;0mPyTorch\u001b[0m We set the initial learning rate at 5e\n",
              "with 15 warmup steps and linear scheduler We use Adafactor Shazeer Stern 2018 as our optimizer and choose batch \n",
              "size of 512 with sequence truncation length of 1216 To obtain \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m we continue to finetune Magicoder models \n",
              "with the evol \u001b[30;48;2;255;255;0mcodealpaca\u001b[0m v1 dataset an open source Evol Instruct implementation containing about 110K samples We \n",
              "use the same hyperparameters except for 15 warmup steps and 1024 maximum sequence length More Evaluation Results \n",
              "Evaluation on APPS for Competitive Programming We additionally \u001b[30;48;2;255;255;0mevaluate\u001b[0m Magicoder on APPS Hendrycks et al 2021 \n",
              "benchmark suite of competitive programming problems Following Olausson et al 2024 we select subset of 300 problems \n",
              "from the APPS test set From Table we can observe that the CODELLAMA PYTHON based Magicoder CL significantly \n",
              "outperforms the base model and WizardCoder CL \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL 7B is even better than WizardCoder SC 15B despite \n",
              "having less than half the number of parameters\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">transformers library from Hugging Face Hugging Face 2023 We finetune the base models for epochs using two NVIDIA \n",
              "A100 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">80GB</span> GPUs through the Distributed Data Parallel DDP module from <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PyTorch</span> We set the initial learning rate at 5e\n",
              "with 15 warmup steps and linear scheduler We use Adafactor Shazeer Stern 2018 as our optimizer and choose batch \n",
              "size of 512 with sequence truncation length of 1216 To obtain <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> we continue to finetune Magicoder models \n",
              "with the evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">codealpaca</span> v1 dataset an open source Evol Instruct implementation containing about 110K samples We \n",
              "use the same hyperparameters except for 15 warmup steps and 1024 maximum sequence length More Evaluation Results \n",
              "Evaluation on APPS for Competitive Programming We additionally <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">evaluate</span> Magicoder on APPS Hendrycks et al 2021 \n",
              "benchmark suite of competitive programming problems Following Olausson et al 2024 we select subset of 300 problems \n",
              "from the APPS test set From Table we can observe that the CODELLAMA PYTHON based Magicoder CL significantly \n",
              "outperforms the base model and WizardCoder CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL 7B is even better than WizardCoder SC 15B despite \n",
              "having less than half the number of parameters\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "16 Magicoder Empowering \u001b[30;48;2;255;255;0mCode\u001b[0m Generation with OSS INSTRUCT Meanwhile \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder based \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS achieves the\n",
              "best result among all the evaluated baselines substantially outperforming the instruction tuned \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder \u001b[30;48;2;255;255;0m7B\u001b[0m \n",
              "Instruct Table Pass results on APPS evaluated using greedy decoding in zero shot setting Model \u001b[30;48;2;255;255;0mIntroductory\u001b[0m 60 \n",
              "Interview 180 Competition 60 Overall 300 WizardCoder SC 15B 21 CODELLAMA PYTHON \u001b[30;48;2;255;255;0m7B\u001b[0m WizardCoder CL \u001b[30;48;2;255;255;0m7B\u001b[0m 10 Magicoder \n",
              "CL \u001b[30;48;2;255;255;0m7B\u001b[0m 18 \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL \u001b[30;48;2;255;255;0m7B\u001b[0m 23 \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder \u001b[30;48;2;255;255;0m7B\u001b[0m Base 16 \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder \u001b[30;48;2;255;255;0m7B\u001b[0m Instruct 23 10 Magicoder DS \u001b[30;48;2;255;255;0m7B\u001b[0m 20 \n",
              "\u001b[30;48;2;255;255;0mMagicoderS\u001b[0m DS \u001b[30;48;2;255;255;0m7B\u001b[0m 28 11 13\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">16 Magicoder Empowering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Generation with OSS INSTRUCT Meanwhile <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder based <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS achieves the\n",
              "best result among all the evaluated baselines substantially outperforming the instruction tuned <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> \n",
              "Instruct Table Pass results on APPS evaluated using greedy decoding in zero shot setting Model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Introductory</span> 60 \n",
              "Interview 180 Competition 60 Overall 300 WizardCoder SC 15B 21 CODELLAMA PYTHON <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> WizardCoder CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 10 Magicoder \n",
              "CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 18 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 23 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> Base 16 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> Instruct 23 10 Magicoder DS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 20 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> DS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">7B</span> 28 11 13\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fill in the \u001b[30;48;2;255;255;0mMiddle\u001b[0m Evaluation on DS 1000 Table shows the evaluation results of \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS and MagicoderS DS on DS\n",
              "1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m Insertion format assessing model fill in the \u001b[30;48;2;255;255;0mmiddle\u001b[0m capability In this experiment we use \n",
              "\u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder as the base model and exclude \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON based results as \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m PYTHON does not support \n",
              "the fill in the \u001b[30;48;2;255;255;0mmiddle\u001b[0m format The results highlight \u001b[30;48;2;255;255;0mMagicoder\u001b[0m superior performance in fill in the \u001b[30;48;2;255;255;0mmiddle\u001b[0m tasks \n",
              "compared to all other evaluated baselines This outstanding capability suggests that \u001b[30;48;2;255;255;0mMagicoder\u001b[0m can serve as valuable\n",
              "copilot for developers Table Pass results on DS 1000 Insertion format with temperature top max length 1024 and num \n",
              "samples 40 Model NumPy Pandas PyTorch SciPy Sklearn TensorFlow Overall WizardCoder SC 15B 35 20 30 28 32 37 28 \n",
              "\u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder 7B Base 36 28 15 19 32 35 29 \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder 7B Instruct 44 27 38 30 38\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fill in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Middle</span> Evaluation on DS 1000 Table shows the evaluation results of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS and MagicoderS DS on DS\n",
              "1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> Insertion format assessing model fill in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">middle</span> capability In this experiment we use \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder as the base model and exclude <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON based results as <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> PYTHON does not support \n",
              "the fill in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">middle</span> format The results highlight <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> superior performance in fill in the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">middle</span> tasks \n",
              "compared to all other evaluated baselines This outstanding capability suggests that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> can serve as valuable\n",
              "copilot for developers Table Pass results on DS 1000 Insertion format with temperature top max length 1024 and num \n",
              "samples 40 Model NumPy Pandas PyTorch SciPy Sklearn TensorFlow Overall WizardCoder SC 15B 35 20 30 28 32 37 28 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder 7B Base 36 28 15 19 32 35 29 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder 7B Instruct 44 27 38 30 38\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "29 34 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B 39 31 27 23 44 30 33 MagicoderS DS 7B 43 29 39 26 44 36 35 Impact of Removing \u001b[30;48;2;255;255;0mNoisy\u001b[0m Data In \n",
              "we highlight the benefits of preserving certain types of noise in instruction tuning data To support this argument \n",
              "we removed data samples with partially implemented \u001b[30;48;2;255;255;0mcode\u001b[0m such as those containing pass or NotImplemented tokens in \n",
              "Python problems Table 10 shows the pass performance of CODELLAMA PYTHON 7B on HumanEval when finetuned for epochs \n",
              "on the raw OSS INSTRUCT dataset compared to the same dataset with \u001b[30;48;2;255;255;0mnoisy\u001b[0m samples removed We can observe that \n",
              "retaining the \u001b[30;48;2;255;255;0mnoisy\u001b[0m samples results in slightly better performance Table 10 Pass on HumanEval when \u001b[30;48;2;255;255;0mfinetuning\u001b[0m \n",
              "CODELLAMA PYTHON 7B with and without \u001b[30;48;2;255;255;0mnoisy\u001b[0m samples Setting Dataset Size HumanEval With \u001b[30;48;2;255;255;0mnoisy\u001b[0m data \u001b[30;48;2;255;255;0m75K\u001b[0m 55 Without \n",
              "\u001b[30;48;2;255;255;0mnoisy\u001b[0m data 68K 54 17 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering \u001b[30;48;2;255;255;0mCode\u001b[0m Generation with OSS INSTRUCT Cases where OSS INSTRUCT Improves \u001b[30;48;2;255;255;0mCode\u001b[0m \n",
              "Understanding In this section we compare the base model DeepSeek Coder\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">29 34 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B 39 31 27 23 44 30 33 MagicoderS DS 7B 43 29 39 26 44 36 35 Impact of Removing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Noisy</span> Data In \n",
              "we highlight the benefits of preserving certain types of noise in instruction tuning data To support this argument \n",
              "we removed data samples with partially implemented <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> such as those containing pass or NotImplemented tokens in \n",
              "Python problems Table 10 shows the pass performance of CODELLAMA PYTHON 7B on HumanEval when finetuned for epochs \n",
              "on the raw OSS INSTRUCT dataset compared to the same dataset with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">noisy</span> samples removed We can observe that \n",
              "retaining the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">noisy</span> samples results in slightly better performance Table 10 Pass on HumanEval when <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">finetuning</span> \n",
              "CODELLAMA PYTHON 7B with and without <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">noisy</span> samples Setting Dataset Size HumanEval With <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">noisy</span> data <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">75K</span> 55 Without \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">noisy</span> data 68K 54 17 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Generation with OSS INSTRUCT Cases where OSS INSTRUCT Improves <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> \n",
              "Understanding In this section we compare the base model DeepSeek Coder\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base and its OSS INSTRUCT tuned version \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B in both in domain and out of domain scenarios and \n",
              "demonstrate how OSS INSTRUCT helps improve the base model instruction following capability We provide the base \n",
              "model with an appropriate prompt prefix to make sure it can produce plausible output format More \u001b[30;48;2;255;255;0mPrecise\u001b[0m \n",
              "Understanding of Requirements The following example shows that after instruction tuning the model can understand \n",
              "the requirement Check whether the \u001b[30;48;2;255;255;0minput\u001b[0m is valid more precisely \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder 7B Base only considers the integer \n",
              "case while \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B effectively checks if the parameter is of the correct type Prompt Write function that \n",
              "takes an integer and returns True if this integer is cube of some integer number Note you should check whether the \n",
              "\u001b[30;48;2;255;255;0minput\u001b[0m is valid def \u001b[30;48;2;255;255;0mis_cube\u001b[0m if return False int return \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder 7B Base def \u001b[30;48;2;255;255;0mis_cube\u001b[0m if not isinstance int or \n",
              "return False Invalid \u001b[30;48;2;255;255;0minput\u001b[0m Check if the number is perfect cube int return \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base and its OSS INSTRUCT tuned version <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B in both in domain and out of domain scenarios and \n",
              "demonstrate how OSS INSTRUCT helps improve the base model instruction following capability We provide the base \n",
              "model with an appropriate prompt prefix to make sure it can produce plausible output format More <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Precise</span> \n",
              "Understanding of Requirements The following example shows that after instruction tuning the model can understand \n",
              "the requirement Check whether the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">input</span> is valid more precisely <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder 7B Base only considers the integer \n",
              "case while <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B effectively checks if the parameter is of the correct type Prompt Write function that \n",
              "takes an integer and returns True if this integer is cube of some integer number Note you should check whether the \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">input</span> is valid def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">is_cube</span> if return False int return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder 7B Base def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">is_cube</span> if not isinstance int or \n",
              "return False Invalid <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">input</span> Check if the number is perfect cube int return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Handling Corner Cases The example below indicates that \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B can correctly handle the corner case where \n",
              "the input string has less than or equal to characters even if this is not explicitly stated in the prompt This is \n",
              "also an out of domain example since the \u001b[30;48;2;255;255;0mScala\u001b[0m programming language is not included in OSS INSTRUCT dataset Prompt \n",
              "Implement \u001b[30;48;2;255;255;0mScala\u001b[0m function that takes string as input and returns new string where all the characters are reversed \n",
              "except the first and last characters Reply only with the function def \u001b[30;48;2;255;255;0mreverseExceptFirstLast\u001b[0m str String String val \n",
              "\u001b[30;48;2;255;255;0mchars\u001b[0m str toCharArray val first \u001b[30;48;2;255;255;0mchars\u001b[0m val last \u001b[30;48;2;255;255;0mchars\u001b[0m \u001b[30;48;2;255;255;0mchars\u001b[0m length val \u001b[30;48;2;255;255;0mmiddle\u001b[0m \u001b[30;48;2;255;255;0mchars\u001b[0m slice \u001b[30;48;2;255;255;0mchars\u001b[0m length reverse first\n",
              "\u001b[30;48;2;255;255;0mmiddle\u001b[0m last DeepSeek Coder 7B Base def \u001b[30;48;2;255;255;0mreverseExceptFirstLast\u001b[0m input String String if input length input else val \n",
              "reversedMiddle input substring input length reverse input charAt reversedMiddle input charAt input length \u001b[30;48;2;255;255;0mMagicoder\u001b[0m\n",
              "DS 7B Generalizing to Unseen Tasks We also show that after instruction tuning Magic\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Handling Corner Cases The example below indicates that <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B can correctly handle the corner case where \n",
              "the input string has less than or equal to characters even if this is not explicitly stated in the prompt This is \n",
              "also an out of domain example since the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Scala</span> programming language is not included in OSS INSTRUCT dataset Prompt \n",
              "Implement <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Scala</span> function that takes string as input and returns new string where all the characters are reversed \n",
              "except the first and last characters Reply only with the function def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">reverseExceptFirstLast</span> str String String val \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> str toCharArray val first <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> val last <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> length val <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">middle</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> slice <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chars</span> length reverse first\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">middle</span> last DeepSeek Coder 7B Base def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">reverseExceptFirstLast</span> input String String if input length input else val \n",
              "reversedMiddle input substring input length reverse input charAt reversedMiddle input charAt input length <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span>\n",
              "DS 7B Generalizing to Unseen Tasks We also show that after instruction tuning Magic\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "oder DS 7B can respond plausibly to unseen requests during training This example asks the model to create an easy \n",
              "Python coding exercise which the base model cannot interpret Such kinds of tasks are also not incorporated in the \n",
              "OSS INSTRUCT dataset 18 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Prompt Create an easy Python coding \n",
              "exercise def add_numbers return \u001b[30;48;2;255;255;0mDeepSeek\u001b[0m Coder 7B Base Given list of integers return the sum of all the \u001b[30;48;2;255;255;0mpositive\u001b[0m \n",
              "integers in the list def \u001b[30;48;2;255;255;0msum_positive_integers\u001b[0m nums return sum num for num in nums if num Test the function numbers\n",
              "10 print \u001b[30;48;2;255;255;0msum_positive_integers\u001b[0m numbers Output 25 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m DS 7B Generating Complex Programs In this section we \n",
              "present the cases where the best performing MagicoderS DS 7B is capable of generating real world complex programs \n",
              "Implementing \u001b[30;48;2;255;255;0mSnake\u001b[0m Game in Python Prompt Write \u001b[30;48;2;255;255;0msnake\u001b[0m game in Python using the turtle library the game is created by\n",
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m Response Here is simple \u001b[30;48;2;255;255;0msnake\u001b[0m game in Python using the turtle library\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">oder DS 7B can respond plausibly to unseen requests during training This example asks the model to create an easy \n",
              "Python coding exercise which the base model cannot interpret Such kinds of tasks are also not incorporated in the \n",
              "OSS INSTRUCT dataset 18 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Prompt Create an easy Python coding \n",
              "exercise def add_numbers return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">DeepSeek</span> Coder 7B Base Given list of integers return the sum of all the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">positive</span> \n",
              "integers in the list def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sum_positive_integers</span> nums return sum num for num in nums if num Test the function numbers\n",
              "10 print <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">sum_positive_integers</span> numbers Output 25 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> DS 7B Generating Complex Programs In this section we \n",
              "present the cases where the best performing MagicoderS DS 7B is capable of generating real world complex programs \n",
              "Implementing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Snake</span> Game in Python Prompt Write <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game in Python using the turtle library the game is created by\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Response Here is simple <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game in Python using the turtle library\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "This game is basic version and does not include many features that full fledged \u001b[30;48;2;255;255;0msnake\u001b[0m game would have import turtle\n",
              "import time import random \u001b[30;48;2;255;255;0mdelay\u001b[0m Score score \u001b[30;48;2;255;255;0mhigh_score\u001b[0m Set up the screen wn turtle Screen wn title \u001b[30;48;2;255;255;0mSnake\u001b[0m Game by \n",
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m wn bgcolor green wn setup width 600 height 600 wn tracer Turns off the screen updates \u001b[30;48;2;255;255;0mSnake\u001b[0m head head \n",
              "turtle Turtle head speed head shape square head color black head \u001b[30;48;2;255;255;0mpenup\u001b[0m head goto head direction stop \u001b[30;48;2;255;255;0mSnake\u001b[0m food \n",
              "food turtle Turtle food speed food shape circle food color red food \u001b[30;48;2;255;255;0mpenup\u001b[0m food goto 100 segments Pen pen turtle \n",
              "Turtle pen speed pen shape square 19 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT pen color white pen \n",
              "\u001b[30;48;2;255;255;0mpenup\u001b[0m pen hideturtle pen goto 260 pen write Score High Score align center font Courier 24\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This game is basic version and does not include many features that full fledged <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game would have import turtle\n",
              "import time import random <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> Score score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high_score</span> Set up the screen wn turtle Screen wn title <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Snake</span> Game by \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> wn bgcolor green wn setup width 600 height 600 wn tracer Turns off the screen updates <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Snake</span> head head \n",
              "turtle Turtle head speed head shape square head color black head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">penup</span> head goto head direction stop <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Snake</span> food \n",
              "food turtle Turtle food speed food shape circle food color red food <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">penup</span> food goto 100 segments Pen pen turtle \n",
              "Turtle pen speed pen shape square 19 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT pen color white pen \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">penup</span> pen hideturtle pen goto 260 pen write Score High Score align center font Courier 24\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normal Functions def go_up if head direction down head direction up def go_down if head direction up head direction\n",
              "down def go_left if head direction right head direction left def \u001b[30;48;2;255;255;0mgo_right\u001b[0m if head direction left head direction \n",
              "right def move if head direction up head ycor head sety 20 if head direction down head ycor head sety 20 if head \n",
              "direction left head \u001b[30;48;2;255;255;0mxcor\u001b[0m head setx 20 if head direction right head \u001b[30;48;2;255;255;0mxcor\u001b[0m head setx 20 Keyboard bindings wn listen wn\n",
              "\u001b[30;48;2;255;255;0monkeypress\u001b[0m go_up wn \u001b[30;48;2;255;255;0monkeypress\u001b[0m go_down wn \u001b[30;48;2;255;255;0monkeypress\u001b[0m go_left wn \u001b[30;48;2;255;255;0monkeypress\u001b[0m \u001b[30;48;2;255;255;0mgo_right\u001b[0m Main game loop while \u001b[30;48;2;255;255;0mTrue\u001b[0m wn \n",
              "update Check for collision with the border if head \u001b[30;48;2;255;255;0mxcor\u001b[0m \u001b[30;48;2;255;255;0m290\u001b[0m or head \u001b[30;48;2;255;255;0mxcor\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">normal Functions def go_up if head direction down head direction up def go_down if head direction up head direction\n",
              "down def go_left if head direction right head direction left def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">go_right</span> if head direction left head direction \n",
              "right def move if head direction up head ycor head sety 20 if head direction down head ycor head sety 20 if head \n",
              "direction left head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">xcor</span> head setx 20 if head direction right head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">xcor</span> head setx 20 Keyboard bindings wn listen wn\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">onkeypress</span> go_up wn <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">onkeypress</span> go_down wn <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">onkeypress</span> go_left wn <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">onkeypress</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">go_right</span> Main game loop while <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">True</span> wn \n",
              "update Check for collision with the border if head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">xcor</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">290</span> or head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">xcor</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "290 or head ycor 290 or head ycor 290 time sleep head goto head direction stop Hide the segments for segment in \n",
              "segments segment goto 1000 1000 Clear the segments list segments clear Reset the score score Reset the \u001b[30;48;2;255;255;0mdelay\u001b[0m \u001b[30;48;2;255;255;0mdelay\u001b[0m \n",
              "Update the score display pen clear pen write Score High Score format score \u001b[30;48;2;255;255;0mhigh_score\u001b[0m align center font Courier 24 \n",
              "20 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT normal Check for collision with the food if head distance\n",
              "food 20 Move the food to random spot random randint \u001b[30;48;2;255;255;0m280\u001b[0m \u001b[30;48;2;255;255;0m280\u001b[0m random randint \u001b[30;48;2;255;255;0m280\u001b[0m \u001b[30;48;2;255;255;0m280\u001b[0m food goto Add segment \n",
              "new_segment turtle Turtle new_segment speed new_segment shape square new_segment color grey new_segment penup \n",
              "segments append new_segment Shorten the \u001b[30;48;2;255;255;0mdelay\u001b[0m \u001b[30;48;2;255;255;0mdelay\u001b[0m 001 \u001b[30;48;2;255;255;0mIncrease\u001b[0m the score score 10 if score high_\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">290 or head ycor 290 or head ycor 290 time sleep head goto head direction stop Hide the segments for segment in \n",
              "segments segment goto 1000 1000 Clear the segments list segments clear Reset the score score Reset the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> \n",
              "Update the score display pen clear pen write Score High Score format score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high_score</span> align center font Courier 24 \n",
              "20 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT normal Check for collision with the food if head distance\n",
              "food 20 Move the food to random spot random randint <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">280</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">280</span> random randint <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">280</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">280</span> food goto Add segment \n",
              "new_segment turtle Turtle new_segment speed new_segment shape square new_segment color grey new_segment penup \n",
              "segments append new_segment Shorten the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> 001 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Increase</span> the score score 10 if score high_\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "score \u001b[30;48;2;255;255;0mhigh_score\u001b[0m score \u001b[30;48;2;255;255;0mpen\u001b[0m clear \u001b[30;48;2;255;255;0mpen\u001b[0m write Score High Score format score \u001b[30;48;2;255;255;0mhigh_score\u001b[0m align center font Courier 24 \n",
              "normal Move the end \u001b[30;48;2;255;255;0msegments\u001b[0m first in reverse order for index in range len \u001b[30;48;2;255;255;0msegments\u001b[0m \u001b[30;48;2;255;255;0msegments\u001b[0m index xcor \u001b[30;48;2;255;255;0msegments\u001b[0m \n",
              "index \u001b[30;48;2;255;255;0mycor\u001b[0m \u001b[30;48;2;255;255;0msegments\u001b[0m index goto Move segment to where the head is if len \u001b[30;48;2;255;255;0msegments\u001b[0m head xcor head \u001b[30;48;2;255;255;0mycor\u001b[0m \u001b[30;48;2;255;255;0msegments\u001b[0m goto \n",
              "move Check for head collision with the body \u001b[30;48;2;255;255;0msegments\u001b[0m for segment in \u001b[30;48;2;255;255;0msegments\u001b[0m if segment distance head 20 time sleep\n",
              "head goto head direction stop Hide the \u001b[30;48;2;255;255;0msegments\u001b[0m for segment in \u001b[30;48;2;255;255;0msegments\u001b[0m segment goto 1000 1000 Clear the \u001b[30;48;2;255;255;0msegments\u001b[0m \n",
              "list \u001b[30;48;2;255;255;0msegments\u001b[0m clear Reset the score score Reset the \u001b[30;48;2;255;255;0mdelay\u001b[0m \u001b[30;48;2;255;255;0mdelay\u001b[0m Update the score display \u001b[30;48;2;255;255;0mpen\u001b[0m clear \u001b[30;48;2;255;255;0mpen\u001b[0m write Score \n",
              "High Score format score \u001b[30;48;2;255;255;0mhigh_score\u001b[0m align center font Courier 24\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high_score</span> score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">pen</span> clear <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">pen</span> write Score High Score format score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high_score</span> align center font Courier 24 \n",
              "normal Move the end <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> first in reverse order for index in range len <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> index xcor <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> \n",
              "index <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ycor</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> index goto Move segment to where the head is if len <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> head xcor head <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ycor</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> goto \n",
              "move Check for head collision with the body <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> for segment in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> if segment distance head 20 time sleep\n",
              "head goto head direction stop Hide the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> for segment in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> segment goto 1000 1000 Clear the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> \n",
              "list <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">segments</span> clear Reset the score score Reset the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">delay</span> Update the score display <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">pen</span> clear <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">pen</span> write Score \n",
              "High Score format score <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">high_score</span> align center font Courier 24\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normal time sleep delay wn mainloop This game is simple version of the classic \u001b[30;48;2;255;255;0msnake\u001b[0m game The \u001b[30;48;2;255;255;0msnake\u001b[0m starts with \n",
              "single segment and grows in size when it eats food The game ends when the \u001b[30;48;2;255;255;0msnake\u001b[0m hits the border or its own body The\n",
              "score is increased by 10 points for each piece of food eaten The game speed increases as the \u001b[30;48;2;255;255;0msnake\u001b[0m grows 21 \n",
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Building \u001b[30;48;2;255;255;0mOthello\u001b[0m Game in Java Prompt Build console based \n",
              "\u001b[30;48;2;255;255;0mOthello\u001b[0m game in Java with row and column numbers shown on the board The game should end when there are no more \n",
              "valid moves for either player Response Here is simple console based \u001b[30;48;2;255;255;0mOthello\u001b[0m game in Java It does not include the \n",
              "\u001b[30;48;2;255;255;0mgraphical\u001b[0m interface but it does include the row and column numbers on the board import java util Scanner public \n",
              "class \u001b[30;48;2;255;255;0mOthello\u001b[0m private static final int BOARD_SIZE private static final char EMPTY private static final char BLACK \n",
              "private static final char WHITE private static final char board new char BOARD_SIZE BOARD_SIZE private static char \n",
              "currentPlayer BLACK public static void main String args \u001b[30;48;2;255;255;0minitializeBoard\u001b[0m printBoard while true\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">normal time sleep delay wn mainloop This game is simple version of the classic <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game The <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> starts with \n",
              "single segment and grows in size when it eats food The game ends when the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> hits the border or its own body The\n",
              "score is increased by 10 points for each piece of food eaten The game speed increases as the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> grows 21 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Building <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Othello</span> Game in Java Prompt Build console based \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Othello</span> game in Java with row and column numbers shown on the board The game should end when there are no more \n",
              "valid moves for either player Response Here is simple console based <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Othello</span> game in Java It does not include the \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">graphical</span> interface but it does include the row and column numbers on the board import java util Scanner public \n",
              "class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Othello</span> private static final int BOARD_SIZE private static final char EMPTY private static final char BLACK \n",
              "private static final char WHITE private static final char board new char BOARD_SIZE BOARD_SIZE private static char \n",
              "currentPlayer BLACK public static void main String args <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">initializeBoard</span> printBoard while true\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "System out \u001b[30;48;2;255;255;0mprintln\u001b[0m Player currentPlayer \u001b[30;48;2;255;255;0menter\u001b[0m your move row column Scanner scanner new Scanner System in int row \n",
              "scanner nextInt int col scanner nextInt if \u001b[30;48;2;255;255;0misValidMove\u001b[0m row col makeMove row col printBoard if hasValidMove System \n",
              "out \u001b[30;48;2;255;255;0mprintln\u001b[0m Player currentPlayer has no valid moves Game over break currentPlayer currentPlayer \u001b[30;48;2;255;255;0mBLACK\u001b[0m WHITE \u001b[30;48;2;255;255;0mBLACK\u001b[0m \n",
              "else System out \u001b[30;48;2;255;255;0mprintln\u001b[0m Invalid move Try again private static void \u001b[30;48;2;255;255;0minitializeBoard\u001b[0m for int BOARD_SIZE for int \n",
              "BOARD_SIZE board EMPTY board board \u001b[30;48;2;255;255;0mBLACK\u001b[0m board board WHITE private static void printBoard System out print for int \n",
              "BOARD_SIZE System out print System out \u001b[30;48;2;255;255;0mprintln\u001b[0m for int BOARD_SIZE\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">System out <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">println</span> Player currentPlayer <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">enter</span> your move row column Scanner scanner new Scanner System in int row \n",
              "scanner nextInt int col scanner nextInt if <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">isValidMove</span> row col makeMove row col printBoard if hasValidMove System \n",
              "out <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">println</span> Player currentPlayer has no valid moves Game over break currentPlayer currentPlayer <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BLACK</span> WHITE <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BLACK</span> \n",
              "else System out <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">println</span> Invalid move Try again private static void <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">initializeBoard</span> for int BOARD_SIZE for int \n",
              "BOARD_SIZE board EMPTY board board <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BLACK</span> board board WHITE private static void printBoard System out print for int \n",
              "BOARD_SIZE System out print System out <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">println</span> for int BOARD_SIZE\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "System out print for int \u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m System out print board System out println private static boolean isValidMove \n",
              "int row int col if board row col EMPTY return \u001b[30;48;2;255;255;0mfalse\u001b[0m for int for int if if \u001b[30;48;2;255;255;0misValidDirection\u001b[0m row col return true 22 \n",
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT return \u001b[30;48;2;255;255;0mfalse\u001b[0m private static boolean \u001b[30;48;2;255;255;0misValidDirection\u001b[0m int row\n",
              "int col int rowDir int colDir int newRow row rowDir int newCol col colDir while newRow newRow \u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m newCol \n",
              "newCol \u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m if board newRow newCol EMPTY return \u001b[30;48;2;255;255;0mfalse\u001b[0m if board newRow newCol \u001b[30;48;2;255;255;0mcurrentPlayer\u001b[0m return true newRow\n",
              "rowDir\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">System out print for int <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span> System out print board System out println private static boolean isValidMove \n",
              "int row int col if board row col EMPTY return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">false</span> for int for int if if <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">isValidDirection</span> row col return true 22 \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">false</span> private static boolean <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">isValidDirection</span> int row\n",
              "int col int rowDir int colDir int newRow row rowDir int newCol col colDir while newRow newRow <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span> newCol \n",
              "newCol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span> if board newRow newCol EMPTY return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">false</span> if board newRow newCol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">currentPlayer</span> return true newRow\n",
              "rowDir\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "newCol colDir return \u001b[30;48;2;255;255;0mfalse\u001b[0m private static void makeMove int row int col board row col \u001b[30;48;2;255;255;0mcurrentPlayer\u001b[0m for int for int\n",
              "if flipDirection row col private static void flipDirection int row int col int \u001b[30;48;2;255;255;0mrowDir\u001b[0m int colDir int newRow row \n",
              "\u001b[30;48;2;255;255;0mrowDir\u001b[0m int newCol col colDir while newRow newRow \u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m newCol newCol \u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m if board newRow newCol EMPTY \n",
              "return if board newRow newCol \u001b[30;48;2;255;255;0mcurrentPlayer\u001b[0m while true newRow \u001b[30;48;2;255;255;0mrowDir\u001b[0m newCol colDir if newRow row newCol col break \n",
              "board newRow newCol \u001b[30;48;2;255;255;0mcurrentPlayer\u001b[0m return newRow \u001b[30;48;2;255;255;0mrowDir\u001b[0m newCol colDir private static boolean \u001b[30;48;2;255;255;0mhasValidMove\u001b[0m for int \n",
              "\u001b[30;48;2;255;255;0mBOARD_SIZE\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">newCol colDir return <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">false</span> private static void makeMove int row int col board row col <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">currentPlayer</span> for int for int\n",
              "if flipDirection row col private static void flipDirection int row int col int <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rowDir</span> int colDir int newRow row \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rowDir</span> int newCol col colDir while newRow newRow <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span> newCol newCol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span> if board newRow newCol EMPTY \n",
              "return if board newRow newCol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">currentPlayer</span> while true newRow <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rowDir</span> newCol colDir if newRow row newCol col break \n",
              "board newRow newCol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">currentPlayer</span> return newRow <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rowDir</span> newCol colDir private static boolean <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">hasValidMove</span> for int \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">BOARD_SIZE</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "for int BOARD_SIZE if isValidMove return true return false This program starts with 8x8 board with two pieces in \n",
              "the center of the board one black and one white The player who goes first is black The game ends when there are no \n",
              "more valid moves for either player The player enters their move as two numbers the row and the column If the move \n",
              "is valid the game makes the move and switches the player If the move is not valid the player is asked to enter \n",
              "valid move Writing Specific \u001b[30;48;2;255;255;0mGradio\u001b[0m Application 23 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT Prompt \n",
              "Write \u001b[30;48;2;255;255;0mgradio\u001b[0m 48 application for the following use case Take an input image and return 45 degree clockwise rotated \n",
              "image You should also add text description under the output showing the rotation degree Response Here is simple \n",
              "\u001b[30;48;2;255;255;0msnake\u001b[0m game in Python using the turtle library This game is basic version and does not include many features that \n",
              "full fledged \u001b[30;48;2;255;255;0msnake\u001b[0m game would have import \u001b[30;48;2;255;255;0mgradio\u001b[0m as gr import \u001b[30;48;2;255;255;0mnumpy\u001b[0m as np from PIL import Image def \u001b[30;48;2;255;255;0mrotate_image\u001b[0m \n",
              "img img Image fromarray img rotated_img img rotate 45 return np\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">for int BOARD_SIZE if isValidMove return true return false This program starts with 8x8 board with two pieces in \n",
              "the center of the board one black and one white The player who goes first is black The game ends when there are no \n",
              "more valid moves for either player The player enters their move as two numbers the row and the column If the move \n",
              "is valid the game makes the move and switches the player If the move is not valid the player is asked to enter \n",
              "valid move Writing Specific <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Gradio</span> Application 23 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT Prompt \n",
              "Write <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">gradio</span> 48 application for the following use case Take an input image and return 45 degree clockwise rotated \n",
              "image You should also add text description under the output showing the rotation degree Response Here is simple \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game in Python using the turtle library This game is basic version and does not include many features that \n",
              "full fledged <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">snake</span> game would have import <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">gradio</span> as gr import <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">numpy</span> as np from PIL import Image def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rotate_image</span> \n",
              "img img Image fromarray img rotated_img img rotate 45 return np\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array rotated_img \u001b[30;48;2;255;255;0m45\u001b[0m degree clockwise rotation \u001b[30;48;2;255;255;0mdemo\u001b[0m gr Interface fn \u001b[30;48;2;255;255;0mrotate_image\u001b[0m inputs gr inputs Image type \u001b[30;48;2;255;255;0mnumpy\u001b[0m \n",
              "outputs gr outputs Image type \u001b[30;48;2;255;255;0mnumpy\u001b[0m text title Image Rotation description Upload an image and get \u001b[30;48;2;255;255;0m45\u001b[0m degree \n",
              "clockwise rotated image \u001b[30;48;2;255;255;0mdemo\u001b[0m launch This script defines function \u001b[30;48;2;255;255;0mrotate_image\u001b[0m that takes an image as input rotates \n",
              "it \u001b[30;48;2;255;255;0m45\u001b[0m degrees clockwise using the PIL library and returns the rotated image and text description The gr Interface \n",
              "function is then used to create Gradio interface for this function The gr inputs Image type \u001b[30;48;2;255;255;0mnumpy\u001b[0m line specifies \n",
              "that the input to the function should be an image and the gr outputs Image type \u001b[30;48;2;255;255;0mnumpy\u001b[0m line specifies that the \n",
              "output should be an image The text output type is used to display the text description Finally \u001b[30;48;2;255;255;0mdemo\u001b[0m launch is used \n",
              "to launch the Gradio interface Developing Machine Learning Pipeline with PyTorch Prompt Build simple neural network\n",
              "in Python using Pytorch to classify handwritten digits from the \u001b[30;48;2;255;255;0mMNIST\u001b[0m dataset You should use CNN as the model \n",
              "structure train the model for epochs draw chart of the training loss and show the final result Response Here is \n",
              "simple implementation\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">array rotated_img <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">45</span> degree clockwise rotation <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">demo</span> gr Interface fn <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rotate_image</span> inputs gr inputs Image type <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">numpy</span> \n",
              "outputs gr outputs Image type <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">numpy</span> text title Image Rotation description Upload an image and get <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">45</span> degree \n",
              "clockwise rotated image <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">demo</span> launch This script defines function <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">rotate_image</span> that takes an image as input rotates \n",
              "it <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">45</span> degrees clockwise using the PIL library and returns the rotated image and text description The gr Interface \n",
              "function is then used to create Gradio interface for this function The gr inputs Image type <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">numpy</span> line specifies \n",
              "that the input to the function should be an image and the gr outputs Image type <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">numpy</span> line specifies that the \n",
              "output should be an image The text output type is used to display the text description Finally <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">demo</span> launch is used \n",
              "to launch the Gradio interface Developing Machine Learning Pipeline with PyTorch Prompt Build simple neural network\n",
              "in Python using Pytorch to classify handwritten digits from the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> dataset You should use CNN as the model \n",
              "structure train the model for epochs draw chart of the training loss and show the final result Response Here is \n",
              "simple implementation\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "of \u001b[30;48;2;255;255;0mConvolutional\u001b[0m Neural Network CNN using PyTorch to classify handwritten digits from the \u001b[30;48;2;255;255;0mMNIST\u001b[0m dataset import \n",
              "torch import torch nn as nn import torchvision import torchvision transforms as transforms import matplotlib pyplot\n",
              "as plt Device configuration device torch device cuda if torch cuda is_available else cpu Hyper parameters \n",
              "num_epochs \u001b[30;48;2;255;255;0mbatch_size\u001b[0m 100 learning_rate 001 \u001b[30;48;2;255;255;0mMNIST\u001b[0m dataset \u001b[30;48;2;255;255;0mtrain_dataset\u001b[0m torchvision datasets \u001b[30;48;2;255;255;0mMNIST\u001b[0m root data train \n",
              "True transform transforms ToTensor download True test_dataset torchvision datasets \u001b[30;48;2;255;255;0mMNIST\u001b[0m root data train False \n",
              "transform transforms ToTensor Data loader train_loader torch utils data DataLoader dataset \u001b[30;48;2;255;255;0mtrain_dataset\u001b[0m \u001b[30;48;2;255;255;0mbatch_size\u001b[0m\n",
              "\u001b[30;48;2;255;255;0mbatch_size\u001b[0m 24 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with OSS INSTRUCT shuffle True test_loader torch utils data \n",
              "DataLoader dataset test_dataset \u001b[30;48;2;255;255;0mbatch_size\u001b[0m \u001b[30;48;2;255;255;0mbatch_size\u001b[0m shuffle False Convolution\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Convolutional</span> Neural Network CNN using PyTorch to classify handwritten digits from the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> dataset import \n",
              "torch import torch nn as nn import torchvision import torchvision transforms as transforms import matplotlib pyplot\n",
              "as plt Device configuration device torch device cuda if torch cuda is_available else cpu Hyper parameters \n",
              "num_epochs <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">batch_size</span> 100 learning_rate 001 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> dataset <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">train_dataset</span> torchvision datasets <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> root data train \n",
              "True transform transforms ToTensor download True test_dataset torchvision datasets <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> root data train False \n",
              "transform transforms ToTensor Data loader train_loader torch utils data DataLoader dataset <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">train_dataset</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">batch_size</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">batch_size</span> 24 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with OSS INSTRUCT shuffle True test_loader torch utils data \n",
              "DataLoader dataset test_dataset <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">batch_size</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">batch_size</span> shuffle False Convolution\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "al neural network two convolutional layers class \u001b[30;48;2;255;255;0mConvNet\u001b[0m nn Module def \u001b[30;48;2;255;255;0m__init__\u001b[0m self super \u001b[30;48;2;255;255;0mConvNet\u001b[0m self \u001b[30;48;2;255;255;0m__init__\u001b[0m \n",
              "self layer1 nn Sequential nn Conv2d 16 kernel_size stride padding nn BatchNorm2d 16 nn ReLU nn MaxPool2d \n",
              "kernel_size stride self \u001b[30;48;2;255;255;0mlayer2\u001b[0m nn Sequential nn Conv2d 16 32 kernel_size stride padding nn BatchNorm2d 32 nn ReLU \n",
              "nn MaxPool2d kernel_size stride self fc nn Linear 32 10 def forward self out self layer1 out self \u001b[30;48;2;255;255;0mlayer2\u001b[0m out out \n",
              "out reshape out size out self fc out return out model \u001b[30;48;2;255;255;0mConvNet\u001b[0m to device Loss and \u001b[30;48;2;255;255;0moptimizer\u001b[0m criterion nn \n",
              "CrossEntropyLoss \u001b[30;48;2;255;255;0moptimizer\u001b[0m torch optim Adam model parameters lr \u001b[30;48;2;255;255;0mlearning_rate\u001b[0m Train the model total\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">al neural network two convolutional layers class <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ConvNet</span> nn Module def <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__init__</span> self super <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ConvNet</span> self <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">__init__</span> \n",
              "self layer1 nn Sequential nn Conv2d 16 kernel_size stride padding nn BatchNorm2d 16 nn ReLU nn MaxPool2d \n",
              "kernel_size stride self <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">layer2</span> nn Sequential nn Conv2d 16 32 kernel_size stride padding nn BatchNorm2d 32 nn ReLU \n",
              "nn MaxPool2d kernel_size stride self fc nn Linear 32 10 def forward self out self layer1 out self <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">layer2</span> out out \n",
              "out reshape out size out self fc out return out model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ConvNet</span> to device Loss and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">optimizer</span> criterion nn \n",
              "CrossEntropyLoss <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">optimizer</span> torch optim Adam model parameters lr <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">learning_rate</span> Train the model total\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "_step len train_loader loss_list for epoch in range \u001b[30;48;2;255;255;0mnum_epochs\u001b[0m for images labels in enumerate train_loader images \n",
              "images to device labels labels to device Forward pass outputs \u001b[30;48;2;255;255;0mmodel\u001b[0m images loss criterion outputs labels \u001b[30;48;2;255;255;0mBackward\u001b[0m \n",
              "and optimize \u001b[30;48;2;255;255;0moptimizer\u001b[0m zero_grad loss \u001b[30;48;2;255;255;0mbackward\u001b[0m \u001b[30;48;2;255;255;0moptimizer\u001b[0m step if print Epoch Step Loss 4f format epoch \u001b[30;48;2;255;255;0mnum_epochs\u001b[0m \n",
              "total_step loss item loss_list append loss item Test the \u001b[30;48;2;255;255;0mmodel\u001b[0m \u001b[30;48;2;255;255;0mmodel\u001b[0m eval eval mode batchnorm uses moving mean \n",
              "variance instead of mini batch mean variance with torch no_grad correct total for images labels in \u001b[30;48;2;255;255;0mtest_loader\u001b[0m \n",
              "images images to device labels labels to device outputs \u001b[30;48;2;255;255;0mmodel\u001b[0m images predicted torch max outputs data total labels \n",
              "size correct predicted labels sum item print Test Accuracy of\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">_step len train_loader loss_list for epoch in range <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">num_epochs</span> for images labels in enumerate train_loader images \n",
              "images to device labels labels to device Forward pass outputs <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">model</span> images loss criterion outputs labels <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Backward</span> \n",
              "and optimize <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">optimizer</span> zero_grad loss <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">backward</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">optimizer</span> step if print Epoch Step Loss 4f format epoch <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">num_epochs</span> \n",
              "total_step loss item loss_list append loss item Test the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">model</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">model</span> eval eval mode batchnorm uses moving mean \n",
              "variance instead of mini batch mean variance with torch no_grad correct total for images labels in <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">test_loader</span> \n",
              "images images to device labels labels to device outputs <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">model</span> images predicted torch max outputs data total labels \n",
              "size correct predicted labels sum item print Test Accuracy of\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "the model on the 10000 test images Draw chart of the training loss plt plot \u001b[30;48;2;255;255;0mloss_list\u001b[0m plt title Training Loss plt \n",
              "xlabel Epoch plt ylabel Loss plt show 25 \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Empowering Code Generation with \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m This script first \n",
              "loads the \u001b[30;48;2;255;255;0mMNIST\u001b[0m dataset and then defines Convolutional Neural Network CNN with two convolutional layers The CNN is \n",
              "trained for epochs using the Adam optimizer and the Cross Entropy Loss function After training the model is tested \n",
              "on the test dataset and the accuracy is printed Finally chart of the training loss is drawn Limitations While \n",
              "Magicoders prove to be effective by being instruction tuned with \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m generated data we \n",
              "apply our techniques to models with no more than 7B parameters due to resource constraints This limitation hinders \n",
              "our ability to observe the method impact on larger models Moreover the \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m prompt is sophisticated and may\n",
              "require relatively strong model to comprehend the correct intent Weaker models may fail to produce plausible \n",
              "instruction data Future research may explore applying \u001b[30;48;2;255;255;0mOSS\u001b[0m \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m to models at different capacities in the context\n",
              "of self training\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">the model on the 10000 test images Draw chart of the training loss plt plot <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">loss_list</span> plt title Training Loss plt \n",
              "xlabel Epoch plt ylabel Loss plt show 25 <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Empowering Code Generation with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> This script first \n",
              "loads the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MNIST</span> dataset and then defines Convolutional Neural Network CNN with two convolutional layers The CNN is \n",
              "trained for epochs using the Adam optimizer and the Cross Entropy Loss function After training the model is tested \n",
              "on the test dataset and the accuracy is printed Finally chart of the training loss is drawn Limitations While \n",
              "Magicoders prove to be effective by being instruction tuned with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> generated data we \n",
              "apply our techniques to models with no more than 7B parameters due to resource constraints This limitation hinders \n",
              "our ability to observe the method impact on larger models Moreover the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> prompt is sophisticated and may\n",
              "require relatively strong model to comprehend the correct intent Weaker models may fail to produce plausible \n",
              "instruction data Future research may explore applying <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">OSS</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> to models at different capacities in the context\n",
              "of self training\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBfsU58XZblA",
        "outputId": "69e31561-4420-434c-d2ee-b7e8f4001798"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'We introduce Magicoder, a series of fully open\\x02source (code, weights, and data) Large Language\\nModels (LLMs) for code that significantly closes\\nthe gap with top code models while having no\\nmore than 7B parameters. Magicoder models are\\ntrained on 75K synthetic instruction data using\\nOSS-INSTRUCT, a novel approach to enlighten\\x02ing LLMs with open-source code snippets to gen\\x02erate diverse instruction data for code. Our main\\nmotivation is to mitigate the inherent bias of the\\nsynthetic data generated by LLMs through the\\nwealth of open-source references for the produc\\x02tion of more realistic and controllable data. The\\northogonality of OSS-INSTRUCT and other data\\ngeneration methods like Evol-Instruct further en\\x02ables us to build an enhanced MagicoderS. Both\\nMagicoder and MagicoderS substantially outper\\x02form state-of-the-art code models with similar or\\neven larger sizes on a wide range of coding bench\\x02marks. Notably, MagicoderS-CL-7B based on\\nCODELLAMA even surpasses the prominent Chat\\x02GPT on HumanEval+ (66.5 vs. 65.9 in pass@1).\\nOverall, OSS-INSTRUCT opens a new direction\\nfor crafting diverse synthetic instruction data for\\ncode using abundant open-source references.\\n1. Introduction\\nCode generation, also known as program synthesis (Gul\\x02wani et al., 2017), is a long-standing challenge',\n",
              "  'keywords': ['magicoders', 'instruct', 'llms', 'synthetic', '75k']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' in com\\x02puter science. In the past few decades, a large body of\\nresearch has been studying symbolic approaches, such as\\nabstraction-based synthesis (Wang et al., 2017; Feng et al.,\\n2018) for general-purpose synthesis problems and program-\\n†The work was done during a remote summer internship at the\\nUniversity of Illinois. 1University of Illinois at Urbana-Champaign,\\nUSA 2Tsinghua University, China. Correspondence to: Yuxiang\\nWei <ywei40@illinois.edu>.\\nProceedings of the 41 st International Conference on Machine\\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\\nthe author(s).\\nming by examples (Cambronero et al., 2023; Liu et al.,\\n2023a) for domain-specific tasks. Until recently, Large Lan\\x02guage Models (LLMs) trained on code (Austin et al., 2021;\\nChen et al., 2021) has shown outstanding breakthroughs in\\ngenerating code that accurately satisfies user intents, and\\nthey are widely deployed to assist real-world software de\\x02velopment (Microsoft, 2023b; Services, 2023).\\nInitially, closed-source models such as GPT-3.5 Turbo (Ope\\x02nAI, 2022) (i.e., ChatGPT) and GPT-4 (OpenAI, 2023) mas\\x02sively dominated various coding benchmarks and leader\\x02boards (Chen et al',\n",
              "  'keywords': ['benchmarks', 'chatgpt', 'llms', 'based', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '., 2021; Austin et al., 2021; Liu et al.,\\n2023b; Lai et al., 2022; Xia & Zhang, 2023). To further push\\nthe boundaries of code generation with open source LLMs,\\nSELF-INSTRUCT (Wang et al., 2023a) is adopted to boot\\x02strap the instruction-following ability of LLMs. In the realm\\nof code, practitioners commonly devise synthetic coding\\ninstructions using a stronger teacher model (e.g., ChatGPT\\nand GPT-4) and then finetune a weaker student model (e.g.,\\nCODELLAMA (Roziere et al. ` , 2023)) with the generated data\\nto distill the knowledge from the teacher (Taori et al., 2023;\\nChaudhary, 2023). For example, Code Alpaca (Chaudhary,\\n2023) consists of 20K automatically generated code instruc\\x02tions by applying SELF-INSTRUCT on ChatGPT using 21\\nseed tasks. To further enhance the coding abilities of LLMs,\\nLuo et al. (2023b) proposes Code Evol-Instruct that employs\\nvarious heuristics to increase the complexity of seed code\\ninstructions (Code Alpaca in this case), achieving state-of\\x02the-art (SOTA) results among open-source models.\\nWhile these data generation methods can effectively im\\x02prove the instruction-following capability of an LLM,',\n",
              "  'keywords': ['instruction', 'codellama', 'chatgpt', '2022', 'increase']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' they\\nrely on a narrow range of predefined tasks or heuristics\\nunder the hood. For example, on the one hand, Code Al\\x02paca that adopts SELF-INSTRUCT only relies on 21 seed\\ntasks to generate new code instructions using an identical\\nprompt template. On the other hand, Code Evol-Instruct\\ntakes Code Alpaca as seeds and merely depends on 5 heuris\\x02tics to evolve the dataset. As partly suggested by Yu et al.\\n(2023) and Wang et al. (2023a), such approaches may sig\\x02nificantly inherit the system bias inherent in the LLMs as\\nwell as the predefined tasks.\\nTherefore, in this paper, we propose OSS-INSTRUCT to\\nmitigate the inherent bias of LLMs and to unleash their\\npotential to craft diverse and creative code instructions via\\n1\\narXiv:2312.02120v2 [cs.CL] 7 Jun 2024\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nYou are working on a natural language processing (NLP) \\nproject and need to create a program to preprocess and \\nclassify movie reviews...\\n...\\nYour program should be able to preprocess new movie \\nreviews, train the model, and classify new reviews accurately.\\nQ Generated problem (details omitted)\\nDPosNeg.py\\nDLog.cpp\\nDGrantInfo.ts\\nDProgram.cs\\nDStrength.swift\\nD',\n",
              "  'keywords': ['instruct', 'dposneg', 'generated', 'alpaca', 'llms']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '…\\nOpen-source codebase\\n learn_model(\\n tf_idfSVM, tf_idfNB, target)\\ndef get_clean_review(raw_review):\\n letters_only = re.sub(\\n \"[^a-zA-Z]\", \" \", raw_review)\\nDSeed code snippet\\nOSS-INSTRUCT\\nPlease gain inspiration from the \\ncode snippet to create a high\\x02quality programming problem…\\ns Prompt (details omitted)\\nfrom sklearn.feature_extraction.text import TfidfVectorizer ...\\ndef get_clean_review(raw_review): ...\\ndef train_model(tf_idfSVM, tf_idfNB, reviews, labels): ...\\ndef classify_review(clean_review, tf_idfSVM, tf_idfNB): ... ...\\ntrain_model(tf_idfSVM, tf_idfNB, reviews, labels)\\ncleaned_review = get_clean_review(...)...\\nL Generated solution (details omitted)\\nsLanguage\\nModel\\nFigure 1: Overview of OSS-INSTRUCT and the pass@1 results of different LLMs on HumanEval (+)\\ndirect learning from the open source. As shown in Figure 1,\\nOSS-INSTRUCT leverages a powerful LLM to automati\\x02cally generate new coding problems by drawing inspira\\x02tion from any random code snippets collected from the\\nopen source. In this example, the LLM',\n",
              "  'keywords': ['learn_model',\n",
              "   'sklearn',\n",
              "   'cleaned_review',\n",
              "   'tf_idfsvm',\n",
              "   'slanguage']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' gets inspired by\\ntwo incomplete code fragments from different functions\\nand manages to relate them and craft a realistic machine\\nlearning problem. Thanks to the “infinite” real-world open\\x02source code, OSS-INSTRUCT can directly produce diverse,\\nrealistic, and controllable code instructions by providing\\ndistinct seed code snippets. In the end, we generate 75K\\nsynthetic data to finetune CODELLAMA-PYTHON-7B, re\\x02sulting in Magicoder-CL. While being simple and effective,\\nOSS-INSTRUCT is orthogonal to existing data generation\\nmethods, and they can be combined to further boost the\\nmodels’ coding capabilities. Therefore, we continually fine\\x02tune Magicoder-CL on an open-source Evol-Instruct dataset\\nwith 110K entries, producing MagicoderS-CL.\\nWe evaluate Magicoder and MagicoderS on a wide range\\nof coding tasks, including HumanEval (Chen et al., 2021)\\nand MBPP (Austin et al., 2021) for Python text-to-code gen\\x02eration, MultiPL-E (Cassano et al., 2022) for multilingual\\ncode completion, and DS-1000 (Lai et al., 2022) for solving\\ndata science problems. We further adopt EvalPlus (Liu et al.,\\n2023b), which includes the augmented HumanEval+ and\\nMBPP+ datasets for more rigorous model evaluation. Both\\nMagicoder-CL and',\n",
              "  'keywords': ['magicoder', 'evalplus', 'mbpp', 'infinite', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' MagicoderS-CL substantially boost the\\nbase CODELLAMA-PYTHON-7B. Additionally, Magicoder\\x02CL even outperforms WizardCoder-CL-7B, WizardCoder\\x02SC-15B, and all studied SOTA LLMs with less than or equal\\nto 16B parameters on all the benchmarks we tested. Also,\\nthe pass@1 result of the enhanced MagicoderS-CL is on\\npar with ChatGPT on HumanEval (70.7 vs. 72.6) and sur\\x02passes it on the more rigorous HumanEval+ (66.5 vs. 65.9),\\nindicating that MagicoderS-CL can generate more robust\\ncode. It also achieves SOTA results among all code models\\nat the same scale.\\nAdditionally, we notice a very recent advancement in the\\ndevelopment of the DeepSeek-Coder series (Guo et al.,\\n2024) which has shown exceptional coding performance.\\nHowever, due to the limited technical details disclosed,\\nwe only briefly discuss them in §3.4. Despite this, we\\napplied OSS-INSTRUCT on DeepSeek-Coder-Base 6.7B,\\nresulting in the creation of Magicoder-DS and MagicoderS\\x02DS. In addition to the consistent findings on the previous\\nresults with CODELLAMA-PYTHON-7B as the base model,\\nMagicoder-DS and MagicoderS-DS benefit from the more\\npowerful DeepSe',\n",
              "  'keywords': ['magicoders', 'deepseek', 'chatgpt', 'coding', 'llms']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'ek-Coder-Base-6.7B. This advantage is\\ndemonstrated by MagicoderS-DS, which achieves a remark\\x02able 76.8 pass@1 on HumanEval. MagicoderS-DS also out\\x02performs DeepSeek-Coder-Instruct-6.7B on HumanEval (+)\\nand MBPP (+) with 8× less finetuning tokens.\\nTo justify the design of OSS-INSTRUCT, i.e., generating\\n2\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\ninstruction-tuning data from open-source references rather\\nthan using the references directly, we demonstrate that fine\\x02tuning the base models with semantically relevant comment\\x02function pairs extracted from open-source projects even\\nnegatively impacts the model performance (§4.2).\\nIn general, we make the following contributions:\\n• We introduce OSS-INSTRUCT, a pioneering approach to\\nenlightening LLMs with open-source code snippets to\\ngenerate more diverse, realistic, and controllable coding\\ninstruction data, which can be leveraged to substantially\\nboost the performance of various LLMs via instruction\\ntuning. It opens a new dimension for creating low-bias\\nand diverse instruction-tuning data from the abundance of\\nopen-source references.\\n• We build the Magicoder series trained with OSS\\x02INSTRUCT and MagicoderS series trained on a combi\\x02nation of OSS-INSTRUCT and Evol',\n",
              "  'keywords': ['magicoder', 'mbpp', 'tuning', 'llms', 'relevant']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '-Instruct. Our eval\\x02uation across 6 benchmarks shows that all Magicoders\\nsignificantly improve the base LLMs. Notably, both\\nMagicoderS-CL and MagicoderS-DS outperform Chat\\x02GPT on HumanEval+ with only 7B parameters.\\n• We fully open source the model weights, training data, and\\nsource code at https://github.com/ise-uiuc/\\nmagicoder to facilitate future research.\\n2. OSS-INSTRUCT: Instruction Tuning from\\nOpen Source\\nIn this section, we elaborate on our OSS-INSTRUCT ap\\x02proach. From a high level, as shown in Figure 1, OSS\\x02INSTRUCT works by prompting an LLM (e.g., ChatGPT)\\nto generate a coding problem and its solution according to\\nsome seed code snippet collected from the wild (e.g., from\\nGitHub). The seed snippet offers controllability of the gen\\x02eration and encourages the LLM to create diverse coding\\nproblems that can reflect real-world programming scenarios.\\n2.1. Generating Coding Problems\\nOSS-INSTRUCT is powered by seed code snippets that can\\nbe easily collected from open source. In this work, we\\ndirectly adopt starcoderdata as our seed corpus, a fil\\x02tered version of The Stack (Kocetkov et al., 2022) dataset\\nthat StarCoder is trained on, containing permissively li\\x02censed source code documents in various programming lan\\x02guages',\n",
              "  'keywords': ['magicoder', 'instruct', 'chatgpt', 'seed', 'llms']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '. We chose starcoderdata because it is widely\\nadopted, includes massive high-quality code snippets, and\\nis even post-processed for data decontamination (Li et al.,\\n2023; Allal et al., 2023). For each code document from\\nthe corpus, we randomly extract 1–15 consecutive lines\\nas the seed snippet for the model to gain inspiration from\\nand produce coding problems. In total, we collected 80K\\ninitial seed snippets from 80K code documents, 40K from\\nPython, and 5K from each of C++, Java, TypeScript, Shell,\\nC#, Rust, PHP, and Swift respectively. Then, each collected\\nseed code snippet is applied to the prompt template shown\\nin Appendix A.1, which a teacher model takes as input and\\noutputs both a coding problem and its solution.\\n2.2. Data Cleaning and Decontamination\\nWe perform data cleaning by excluding samples that are\\nidentical or share the same seed code snippet. While there\\nexist other sorts of noisiness (e.g., the solution is incom\\x02plete) in the generated data, inspired by Honovich et al.\\n(2023), they are not removed as we believe they still con\\x02tain valuable information for LLMs to learn. More experi\\x02mental details can be found in Appendix C.3. Finally, we\\napply the same logic as StarCoder Li et al. (2023) to decon\\x02taminate our training data by removing coding problems',\n",
              "  'keywords': ['starcoderdata', 'samples', 'php', 'llms', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\nthat contain docstrings or solutions from HumanEval (Chen\\net al., 2021) and MBPP (Austin et al., 2021), docstrings\\nfrom APPS (Hendrycks et al., 2021), prompts from DS\\x021000 (Lai et al., 2022), or questions from GSM8K (Cobbe\\net al., 2021). As part of our analysis, the decontamination\\nprocedure only filters out 9 additional samples. Since the\\nseed corpus starcoderdata has already gone through\\nrigorous data decontamination, this observation suggests\\nthat OSS-INSTRUCT is unlikely to introduce additional data\\nleakage beyond the seeds. The eventual OSS-INSTRUCT\\ndataset contains about 75K entries. An overview of the\\ndataset statistics can be found in Appendix A.3.\\n2.3. Qualitative Examples of OSS-INSTRUCT\\nFigure 2 shows some qualitative examples of how OSS\\x02INSTRUCT can help LLM get inspiration from a seed code\\nsnippet to create new coding problems and solutions. For\\nexample, the shell script example shows how an LLM crafts\\na Python coding problem with just one line of shell script.\\nThe library imports example demonstrates how an LLM\\ncan create a realistic machine learning problem using just\\na few import statements. Meanwhile, the class signature\\ninstance illustrates the ability of LLM to draw inspiration\\nfrom an incomplete class definition featuring annotations\\nlike SpringBootApplication and keywords such as\\nbank. From this, the',\n",
              "  'keywords': ['oss', 'starcoderdata', 'incomplete', 'script', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' LLM generates a problem that re\\x02quires implementing a complete banking system based on\\nSpring Boot. Overall, OSS-INSTRUCT can inspire an LLM\\nwith distinct code structures and semantics to create diverse\\ncoding tasks, including algorithmic challenges, realistic\\nissues, single-function code generation, library-based pro\\x02gram completion, whole-program development, and even\\nwhole-application construction.\\n3\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD Seed: shell script\\npython3 makeErrorFile.py data/test_dataset_14 14\\nQ Problem\\nCreate a Python program that generates an error file\\nbased on a given dataset...\\nL Code\\ndef generate_error_file(dataset_file, ...):\\n error_lines = []\\n with open(dataset_file, \\'r\\') as file:\\n for line in file:\\n ...\\n with open(error_file_name, \\'w\\') as error_file:\\n for error_line in error_lines:\\n error_file.write(error_line + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n if len(sys.argv) != 3:\\n print(\"Usage: ...\")\\n else:\\n dataset_file = sys.argv[1]\\n dataset_number = sys.argv[2]\\n generate_error_file(...)\\nD Seed: library imports\\nimport numpy as np\\nimport gym_electric',\n",
              "  'keywords': ['generate_error_file',\n",
              "   'dataset',\n",
              "   '__main__',\n",
              "   'magicoder',\n",
              "   'llm']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '_motor as gem\\nimport matplotlib.pyplot as plt\\nQ Problem\\nCreate a reinforcement learning agent to control an \\nelectric motor using the OpenAI Gym environment...\\nL Code\\nimport numpy as np\\nimport gym_electric_motor as gem\\n...\\nenv = gem.make(\"DcSeriesCont-v1\")\\nclass DQNAgent:\\n def __init__(self, state_dim, action_dim): ...\\n def build_model(self): ...\\n def act(self, state): ...\\n def train(self, state, action, reward, ...): ...\\n...\\nfor episode in range(episodes):\\n state = env.reset()\\n state = np.reshape(state, [1, state_dim])\\n ...\\nD Seed: class signature\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {\\n public static void main(String[] args) {\\nQ Problem\\nCreate a simple Java Spring Boot application\\nfor a banking system...\\nL Code\\nimport org.axonframework.commandhandling...\\nimport org.axonframework.config...\\n...\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {...}\\npublic class BankAccount {...}\\npublic class CreateAccountCommand {...}\\npublic class DepositFundsCommand {...}\\npublic class WithdrawFundsCommand {...}\\npublic class Account',\n",
              "  'keywords': ['axonbankapplication',\n",
              "   '__init__',\n",
              "   'gym_electric_motor',\n",
              "   'action_dim',\n",
              "   'self']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'CreatedEvent {...}\\npublic class FundsDepositedEvent {...}\\npublic class FundsWithdrawnEvent {...}\\nFigure 2: Examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed\\nproblem requirements, implementations, and explanations are omitted for brevity. More examples can be found in\\nAppendix A.2.\\n0.0 0.1 0.2 0.3 0.4 0.5\\nCosine Similarity Score\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\n0.12\\n0.14\\nPercentage\\nSelf-Instruct; Avg Score: 0.169\\nEvol-Instruct; Avg Score: 0.131\\nOSS-Instruct; Avg Score: 0.105\\nFigure 3: Cosine similarities between HumanEval and syn\\x02thetic data generated by different methods.\\nSimilarity with HumanEval To study whether our data\\ngeneration process produces more HumanEval-like prob\\x02lems or solutions that contribute to high performance, we\\npair each sample from our 75K dataset with each of the\\n164 HumanEval (Chen et al., 2021) samples and compute\\ntheir cosine similarity using TF-IDF (SPARCK JONES,\\n1972) embeddings. We then associate each OSS-INSTRUCT\\nsample with a HumanEval sample with the highest simi\\x02larity score. We also compare our dataset against Code\\nAlpaca, a 20',\n",
              "  'keywords': ['fundsdepositedevent',\n",
              "   'cosine',\n",
              "   'instruct',\n",
              "   'generates',\n",
              "   'humaneval']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'K dataset applying SELF-INSTRUCT to code,\\nand evol-codealpaca-v1 (theblackcat102, 2023), an\\nopen-source reproduction of Evol-Instruct containing 110K\\ncoding instructions. We resort to the open-source implemen\\x02tation because the official Code Evol-Instruct (Luo et al.,\\n2023b) dataset is not released. We decontaminate all the\\ndatasets beforehand using the same way discussed in §2.2.\\nFigure 3 shows that OSS-INSTRUCT exhibits the lowest\\naverage similarity among all the studied data generation\\ntechniques while SELF-INSTRUCT shows the highest aver\\x02age similarity. This result indicates that the improvements\\nfrom OSS-INSTRUCT are not merely due to including data\\nfrom the same distribution.\\n3. Evaluation\\nWe choose CODELLAMA-PYTHON-7B and DeepSeek\\x02Coder-Base 6.7B as the base LLMs. To derive Magicoder\\nseries, we first finetune them on 75K synthetic data\\ngenerated through OSS-INSTRUCT. We then obtain\\nMagicoderS by continuing finetuning Magicoder with the\\nevol-codealpaca-v1 dataset, an open-source Evol\\x02Instruct implementation containing about 110K samples.\\nMore implementation details and additional evaluation re\\x02sults are listed in Appendices B and C. We also present\\ninteresting use cases that reflect the effectiveness of instruc',\n",
              "  'keywords': ['instruc', 'codealpaca', 'magicoders', 'lowest', 'llms']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\x02tion tuning in Appendix D and demonstrate Magicoder’s\\ncapability to generate complex programs in Appendix E.\\n3.1. Python Text-to-Code Generation\\nHumanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n2021) are two of the most widely used benchmarks for code\\ngeneration. Each task in these benchmarks includes a task\\ndescription (e.g., docstring) as the prompt, where LLMs\\ngenerate corresponding code whose correctness is checked\\nby a handful of test cases. Because tests in these benchmarks\\ncan be insufficient, for more rigorous evaluation, we use\\nHumanEval+ and MBPP+, both powered by the EvalPlus\\nframework (Liu et al., 2023b) to obtain 80×/35× more tests.\\nFollowing prior work (Liu et al., 2023b; Chen et al., 2023),\\nfor each task and LLM we use greedy decoding to generate\\n4\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 1: Pass@1 (%) results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding. The\\nabbreviations “CL” and “SC” refer to the base models CODELLAMA-PYTHON and StarCoder, respectively. We report the\\nresults consistently from the EvalPlus (Liu et al., 2023b) Leaderboard.\\nModel Release Date Size\\nBenchmark Open-Source',\n",
              "  'keywords': ['magicoder', 'mbpp', 'generate', 'llm', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\nHumanEval (+) MBPP (+) Weight Data\\nGPT-3.5 Turbo Nov 2023 - 72.6 (65.9) 81.7 (69.4) # #\\nGPT-4 Turbo Nov 2023 - 85.4 (81.7) 83.0 (70.7) # #\\nCODELLAMA-PYTHON Aug 2023 34B 51.8 (42.7) 67.2 (52.9) #\\nWizardCoder-CL Sep 2023 34B 73.2 (64.6) 73.2 (59.9) #\\nCodeT5+ May 2023 16B 31.7 (26.2) 54.6 (44.4) \\nCodeGen-Mono Mar 2022 16B 32.9 (27.4) 52.6 (43.6) \\nStarCoder May 2023 15B 34.1 (29.3) 55.1 (46.1) \\nCODELLAMA-PYTHON Aug 2023 13B 42.7 (36.6) 61.2 (50.9) #\\nWizardCoder-SC Sep 2023 15B 51.9 (45.1) 61.9 (50.6) #\\nStarCoder May 2023 7B 24.4 (20.7) 33.1 (28.8) \\nMistral Oct 2023 7B 28.7 (23.2) 50.1 (40.',\n",
              "  'keywords': ['codegen', '2022', 'gpt', 'mistral', 'weight']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '9) #\\nCodeT5+ May 2023 6B 29.3 (23.8) 51.9 (40.9) \\nCodeGen-Mono Mar 2022 6B 29.3 (25.6) 49.9 (42.1) \\nCODELLAMA-PYTHON Aug 2023 7B 37.8 (34.1) 57.6 (45.4) #\\nWizardCoder-CL Sep 2023 7B 48.2 (40.9) 56.6 (47.1) #\\nMagicoder-CL Dec 2023 7B 60.4 (55.5) 64.2 (52.6) \\nMagicoderS-CL Dec 2023 7B 70.7 (66.5) 68.4 (56.6) \\none sample and focus on comparing the pass@1 metric.\\nWe consider a wide range of baseline models, including\\nCODELLAMA-PYTHON (Roziere et al. ` , 2023), Wizard\\x02Coder (Luo et al., 2023b), GPT-3.5 Turbo (OpenAI, 2022),\\nGPT-4 Turbo (OpenAI, 2023), StarCoder (Li et al., 2023),\\nCodeT5+ (Wang et al., 2023b), CodeGen-Mono (Nijkamp\\net al., 2023), and Mistral (Jiang et al., 2023a). All the re\\x02sults',\n",
              "  'keywords': ['magicoder', 'codellama', '2022', 'python', '7b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' are consistently reported from the EvalPlus (Liu et al.,\\n2023b) leaderboard (EvalPlus hash: 1895d2f).\\nTable 1 shows the pass@1 results of different LLMs on\\nthese benchmarks. From the results, we can first observe\\nthat Magicoder-CL has a clear improvement over the base\\nCODELLAMA-PYTHON-7B, and outperforms all studied\\nopen-source models except CODELLAMA-PYTHON-34B\\nand WizardCoder-CL-34B. Notably, Magicoder-CL sur\\x02passes WizardCoder-SC-15B and has a substantial improve\\x02ment on HumanEval and HumanEval+ over CODELLAMA\\x02PYTHON-34B. MagicoderS-CL demonstrates further im\\x02provements by being trained with the orthogonal Evol\\x02Instruct method. MagicoderS-CL outperforms ChatGPT\\nand all other open-source models on HumanEval+. More\\x02over, although it scores slightly lower than WizardCoder\\x02CL-34B and ChatGPT on HumanEval, it surpasses both of\\nthem on the more rigorous HumanEval+ dataset, indicating\\nthat MagicoderS-CL may produce more robust code.\\n3.2. Multilingual Code Generation\\nIn addition to Python, as shown in Table 2, we perform\\nan extensive evaluation on 6 widely used programming\\nlanguages, i.e., Java, JavaScript, C++, PHP,',\n",
              "  'keywords': ['magicoders', 'evalplus', 'multilingual', 'chatgpt', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' Swift, and\\nRust, using the MultiPL-E benchmark (Cassano et al.,\\n2022). We report available results from the WizardCoder pa\\x02per (Luo et al., 2023b) and evaluate our models consistently\\nthrough bigcode-evaluation-harness (Ben Allal\\net al., 2022). We skip proprietary models such as Chat\\x02GPT and GPT-4 as they are not supported by the frame\\x02work. Due to a significant inference latency when running\\nWizardCoder-CL-7B using the harness in our environment,\\nwe choose not to include it in our analysis.\\nThe results indicate that Magicoder-CL improves the base\\nCODELLAMA-PYTHON-7B by a large margin among all\\nthe studied programming languages. Moreover, Magicoder\\x02CL also achieves better results than the SOTA 15B\\nWizardCoder-SC among half of the programming lan\\x02guages. Additionally, MagicoderS-CL demonstrates fur\\x02ther improvement over Magicoder-CL on all program\\x02ming languages, achieving comparable performance against\\nWizardCoder-CL-34B with only 7B parameters. It is worth\\nnoting that Magicoder-CL is only trained with very limited\\nmultilingual data but still outperforms other LLMs with\\nsimilar or even larger sizes. Also, although the harness\\n5\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nevaluates models in completion formats which are for base\\nmodels, Magicod',\n",
              "  'keywords': ['wizardcoder', 'improves', 'codellama', 'llms', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'ers still show significant improvements de\\x02spite being only instruction-tuned. This implies that LLMs\\ncan learn knowledge from the data beyond its format.\\n3.3. Code Generation for Data Science\\nThe DS-1000 dataset (Lai et al., 2022) contains 1K distinct\\ndata science coding issues ranging from 7 popular data sci\\x02ence libraries in Python. It evaluates the realistic and practi\\x02cal use case of an LLM and offers unit tests for validating\\neach problem. DS-1000 has both completion and insertion\\nmodes, but here we only evaluate completion because the\\nbase CODELLAMA-PYTHON does not support infilling. Ta\\x02ble 3 shows the evaluation results where we include the\\nrecent INCODER (Fried et al., 2023), CodeGen (Nijkamp\\net al., 2023), Code-Cushman-001 (Microsoft, 2023a), Star\\x02Coder (Li et al., 2023), CODELLAMA-PYTHON (Roziere `\\net al., 2023), and WizardCoder (Luo et al., 2023b). We\\ncan see from the table that Magicoder-CL-7B already out\\x02performs all the baselines we evaluate, including state\\x02of-the-art WizardCoder-CL-7B and WizardCoder-SC-15B.\\nMagicoderS-CL-7B further breaks the limit by introduc\\x02ing an 8.3 percentage point absolute improvement over\\nW',\n",
              "  'keywords': ['wizardcoder', 'dataset', 'llm', '2022', 'cushman']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'izardCoder-SC-15B.\\n3.4. Comparison with DeepSeek-Coder\\nDeepSeek-Coder (Guo et al., 2024) is a series of models\\nreleased concurrently to our work and they demonstrate su\\x02perior coding performance. We only briefly discuss it in\\nthis section because its data and instruction tuning details\\nare not publicly available at the time of writing. We apply\\nthe same finetuning strategy on DeepSeek-Coder-Base-6.7B\\nas we performed on CODELLAMA-PYTHON-7B, leading\\nto Magicoder-DS and MagicoderS-DS. Table 4 shows a\\nsimilar trend as Table 1 that the base model can be sig\\x02nificantly improved after applying OSS-INSTRUCT. Re\\x02markably, the MagicoderS-DS variant surpasses DeepSeek\\x02Coder-Instruct-6.7B on all the benchmarks with ×8 fewer\\ntraining tokens, and it also closely matches DeepSeek\\x02Coder-Instruct-33B on these datasets.\\n4. Ablations of Data Source\\n4.1. Impact of the Language Distribution\\nTo understand the correlation between the programming lan\\x02guages appearing in the training data and the downstream\\nperformance of different languages, we conduct an addi\\x02tional ablation study about the training data. We classify the\\n75K training data into approximately 43K Python-only, and\\n32K non-Python data according to whether ‘‘‘',\n",
              "  'keywords': ['izardcoder', 'deepseek', 'codellama', '7b', 'fewer']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'python\\nis a substring of the generated data. We do not classify\\nthe data based on the seed code snippet because LLMs per\\x02forming OSS-INSTRUCT may produce code in a different\\nprogramming language than the seed.\\nTable 5 shows the evaluation results, where we consistently\\nfinetune the base CODELLAMA-PYTHON-7B for 2 epochs\\non different data partitions using the same training hyper\\x02parameters explained in Appendix B. From the table, we\\ncan see that, as can be imagined, training on Python or\\nnon-Python data can substantially boost the performance of\\nthe base model in Python or non-Python tasks, respectively.\\nInterestingly, instruction tuning on different programming\\nlanguages can still boost the overall coding performance\\nthat includes out-of-distribution languages. For example,\\nwhen trained on only non-Python data, Magicoder-CL still\\nachieves a 10.4 percentage point improvement over the base\\nmodel in the Python-only evaluation. This implies LLMs\\ncan establish correlations between different programming\\nlanguages and perform transfer learning of deeper code se\\x02mantics. Finally, we observe a more significant boost in\\nPython evaluation when combining data from both sources,\\nwith a slight decrease in multilingual performance compared\\nwith only finetuning on multilingual data. We attribute this\\ndecrease to the dominant amount of Python data (around\\n57%) during instruction tuning.\\n4.2. OSS-INSTRUCT vs. Direct Finetuning\\nThe fact',\n",
              "  'keywords': ['finetuning', 'python', 'llms', '7b', 'decrease']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' that OSS-INSTRUCT gets an LLM inspired from\\nopen-source code snippets may lead to a natural question:\\nwhy not directly finetuning on these open-source code? To\\nanswer this question, we follow CodeSearchNet (Husain\\net al., 2020) to mine semantically relevant comment-function\\npairs from the same seed document corpus we use to con\\x02struct the 75K OSS-INSTRUCT dataset. We then train the\\nmodel to predict the function bodies from the function signa\\x02tures and comments. We prioritize comment-function pairs\\nthat overlap with our 75K seed snippets, resulting in about\\n11K data points. To align with our 75K samples, we collect\\nthe remaining 64K samples using the whole corpus of 75K\\nseed documents. Eventually, we have the same number of\\ncomment-function pairs with OSS-INSTRUCT data.\\nWe finetune the base CODELLAMA-PYTHON-7B for 2\\nepochs using the paired data, following the same training\\nsetup discussed in Appendix B. From Table 6, we observe\\nthat finetuning on 75K paired comment-function data even\\nworsens the base model, while OSS-INSTRUCT helps to\\nintroduce a substantial boost. We conjecture that the degra\\x02dation is owing to the substantial noise and inconsistency\\nthat exists intrinsically in the data pairs, even though these\\npaired data exhibit very similar format as HumanEval or\\nMultiPL-E problems.',\n",
              "  'keywords': ['codesearchnet', 'finetuning', 'oss', 'boost', 'relevant']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' This further shows that data factual\\x02ity, rather than the format, is essential to code instruction\\ntuning. It also indicates the superiority of OSS-INSTRUCT\\nwhich can translate these loosely related code fragments\\n6\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 2: Pass@1 results of different LLMs on MultiPL-E (Cassano et al., 2022) following the same hyperparameter\\nsettings as the WizardCoder paper (Luo et al., 2023b): temperature = 0.2, top p = 0.95, max length = 512, and\\nnum samples = 50. We evaluate all 7B models using bigcode-evaluation-harness (Ben Allal et al., 2022) and\\nreport other results from WizardCoder.\\nModel Size\\nProgramming Language\\nJava JavaScript C++ PHP Swift Rust\\nCODELLAMA 34B 40.2 41.7 41.4 40.4 35.3 38.7\\nCODELLAMA-PYTHON 34B 39.5 44.7 39.1 39.8 34.3 39.7\\nCODELLAMA-INSTRUCT 34B 41.5 45.9 41.5 37.0 37.6 39.3\\nWizardCoder-CL 34B 44.9 55.3 47.2 47.2 44.3 46.2\\nStarCoderBase 15B 28.5 31.7 30.6 26.8 16.7 24.5\\nStar',\n",
              "  'keywords': ['wizardcoder', 'codellama', 'multipl', '2023b', '37']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'Coder 15B 30.2 30.8 31.6 26.1 22.7 21.8\\nWizardCoder-SC 15B 35.8 41.9 39.0 39.3 33.7 27.1\\nCODELLAMA 7B 29.3 31.7 27.0 25.1 25.6 25.5\\nCODELLAMA-PYTHON 7B 29.1 35.7 30.2 29.0 27.1 27.0\\nMagicoder-CL 7B 36.4 45.9 36.5 39.5 33.4 30.6\\nMagicoderS-CL 7B 42.9 57.5 44.4 47.6 44.1 40.3\\nTable 3: Pass@1 results on DS-1000 (completion format) with temperature = 0.2, top p = 0.5, max length =\\n1024, and num samples = 40, following the same hyperparameter setting used in WizardCoder (Luo et al., 2023b). We\\nevaluate all the 7B models with their preferred prompt formats and report other results from WizardCoder.\\nModel Size\\n+ 155\\nMatplotlib\\n+ 220\\nNumPy\\n+ 291\\nPandas\\n+ 68\\nPyTorch\\n+ 106\\nSciPy\\n+ 115\\nSklearn\\n+ 45\\nTensorFlow\\n= 1000\\nOverall\\nINCODER 6.7B 28.3 4.4 3.1 4.4 2.8 2',\n",
              "  'keywords': ['wizardcoder', 'matplotlib', 'codellama', 'pass', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '.8 3.8 7.4\\nCodeGen-Mono 16B 31.7 10.9 3.4 7.0 9.0 10.8 15.2 11.7\\nCode-Cushman-001 - 40.7 21.8 7.9 12.4 11.3 18.0 12.2 18.1\\nStarCoder 15B 51.7 29.7 11.4 21.4 20.2 29.5 24.5 26.0\\nWizardCoder-SC 15B 55.2 33.6 16.7 26.2 24.2 24.9 26.7 29.2\\nCODELLAMA-PYTHON 7B 55.3 34.5 16.4 19.9 22.3 17.6 28.5 28.0\\nWizardCoder-CL 7B 53.5 34.4 15.2 25.7 21.0 24.5 28.9 28.4\\nMagicoder-CL 7B 54.6 34.8 19.0 24.7 25.0 22.6 28.9 29.9\\nMagicoderS-CL 7B 55.9 40.6 28.4 40.4 28.8 35.8 37.6 37.5\\nTable 4: Pass@1 (greedy decoding) comparison between Magicoder and DeepSeek-Coder (Guo et al., 2024) on Hu\\x02manEval (+) and MBPP (+). DeepSeek-Coder results are',\n",
              "  'keywords': ['wizardcoder', 'deepseek', 'codegen', 'mbpp', 'cushman']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' reported from EvalPlus (Liu et al., 2023b) Leaderboard.\\nModel Size Training Tokens\\nBenchmark Open-Source\\nHumanEval (+) MBPP (+) Weight Data\\nDeepSeek-Coder-Base\\n1.3B 2T - 55.4 (46.9) #\\n6.7B 2T 47.6 (39.6) 70.2 (56.6) #\\n33B 2T 51.2 (43.3) - #\\nDeepSeek-Coder Instruct\\n1.3B +2B 64.6 (58.5) 63.7 (53.1) #\\n6.7B +2B 73.8 (70.1) 72.7 (63.4) #\\n33B +2B 78.7 (72.6) 78.7 (66.7) #\\nMagicoder-DS 6.7B +90M 66.5 (60.4) 75.4 (61.9) \\nMagicoderS-DS 6.7B +240M 76.8 (70.7) 75.7 (64.4) \\n7\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nTable 5: Ablation study of using different programming languages as training data. We show the pass@1 results on\\nHumanEval+ (Liu et al., 2023b) for Python and the average pass@1 results on MultiPL-E (Cassano et al',\n",
              "  'keywords': ['deepseek', 'magicoder', 'evalplus', 'mbpp', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '., 2022) for the\\nsame set of programming languages used in Table 2 (i.e., Java, JavaScript, C++, PHP, Swift, and Rust). All the variants are\\nfinetuned with 2 epochs and evaluated through greedy-decoding.\\nModel (7B) Finetuning Data Python (HumanEval+) Others (MultiPL-E)\\nCODELLAMA-PYTHON - 34.1 29.6\\nMagicoder-CL Python (43K) 47.6 32.7\\nMagicoder-CL Others (32K) 44.5 38.3\\nMagicoder-CL Both (75K) 55.5 37.8\\nTable 6: Comparison between OSS-INSTRUCT and directly\\nfinetuning on comment-function pairs with CODELLAMA\\x02PYTHON-7B as the base model.\\nFinetuning Data HumanEval+ MultiPL-E\\nBase model w/o finetuning 34.1 29.6\\nComment-function pairs (75K) 34.1 24.1\\nOSS-INSTRUCT (75K) 55.5 37.8\\ninto semantically-consistent instruction-tuning data.\\n4.3. OSS-INSTRUCT with A Less Powerful Teacher\\nIn this section, we explore the factors contributing to the\\neffectiveness of OSS-INSTRUCT beyond just the distillation\\nof the teacher model. We propose two potential key reasons.\\nFirst, since the base model is pretrained with comprehen',\n",
              "  'keywords': ['finetuning', 'oss', 'python', 'magicoder', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\x02sive code data, the distillation process likely activates the\\nmodel’s internal capabilities, leading to improved perfor\\x02mance in coding tasks. Second, OSS-INSTRUCT uses seed\\ncode snippets to generate problem-solution pairs in one shot.\\nThese seed snippets provide valuable context, enabling the\\nmodel to create better solutions than a plain teacher model\\nlacking such seed information. These enhanced solutions\\ncan then be used to train more effective student models. To\\nverify these points, we conduct an additional experiment\\nby generating a subset of 20K OSS-INSTRUCT data using\\nMixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024), a state-of\\x02the-art, general-purpose, open-source LLM.\\nTable 7: Pass@1 on HumanEval+ and MBPP+ when fine\\x02tuning CODELLAMA-PYTHON-7B for 2 epochs on 20K\\nOSS-INSTRUCT data generated by Mixtral-8x7B-Instruct\\x02v0.1 (Jiang et al., 2024).\\nModel HumanEval+ MBPP+\\nMixtral-8x7B-Instruct-v0.1 39.6 47.4\\nCODELLAMA-PYTHON-7B 34.1 45.4\\nMagicoder-CL-Mixtral-7B 55.5 50.4\\nTable 7 indicates that Magicoder-CL-Mixtral',\n",
              "  'keywords': ['mixtral', 'magicoder', 'instruct', 'mbpp', 'coding']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '-7B not\\nonly significantly improves over the base CODELLAMA\\x02PYTHON, but is also better than Mixtral-8x7B-Instruct-v0.1\\n(i.e., the teacher model) across HumanEval+ and MBPP+.\\nThese results suggest that OSS-INSTRUCT is not simply dis\\x02tilling a teacher model, but also triggering the base model’s\\nown capability and effectively leveraging the information\\nencapsulated in seed code snippets.\\n5. Related Work\\nFoundation models for code Trained over billions of\\nlines of code, LLMs have demonstrated outstanding per\\x02formance in a wide range of software engineering tasks,\\nincluding code generation (Chen et al., 2021; Austin et al.,\\n2021), program repair (Xia & Zhang, 2022; Wei et al.,\\n2023; Xia et al., 2023b; Jiang et al., 2023b; Bouzenia et al.,\\n2024), and software testing (Xia et al., 2023a; Deng et al.,\\n2023; Yuan et al., 2023; Schafer et al. ¨ , 2023; Lemieux et al.,\\n2023). In particular, prominent base models, such as Code\\x02Gen (Nijkamp et al., 2023), CodeT5 (Wang et al., 2021),\\nStarCoder (Li et al., 2023), and CODELLAMA (Roziere `\\net al., 2023), are pre',\n",
              "  'keywords': ['code', 'instruct', 'llms', 'mixtral', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '-trained over a huge number of code\\x02base from scratch, establishing the fundamental ability of\\ngeneral code generation and understanding. More recent\\ncode LLMs, such as DeepSeek-Coder (Guo et al., 2024) and\\nStarCoder2 (Lozhkov et al., 2024), additionally organize\\nthe pretraining data at the repository level to enhance the\\nmodel’s contextual understanding capabilities. Furthermore,\\nthese base models are also finetuned (Luo et al., 2023b) or\\nprompted (Chen et al., 2023) to unlock their true potential\\nto specialize in solving domain-specific coding tasks.\\nInstruction tuning with synthetic data Instruction tun\\x02ing aims to improve pretrained LLMs by finetuning them\\nwith a mixture of instructions and corresponding re\\x02sponses (Wei et al., 2022). However, obtaining high\\x02quality instructional data is oftentimes laborious. Hence,\\nresearchers are increasingly focusing on the development\\nof methods to generate synthetic instruction data. Wang\\net al. (2023a) introduces SELF-INSTRUCT, where a founda\\x028\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\ntion LLM (GPT-3 (Brown et al., 2020)) is used to gen\\x02erate synthetic instruction-response pairs with carefully\\ncrafted prompts. The same LLM is then instruction-tuned on\\nthe synthetic data to distill such self-generated knowledge.\\nThis technique has been further extended to',\n",
              "  'keywords': ['finetuning', 'synthetic', 'starcoder2', 'llms', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' create synthetic\\ndata with different LLMs. For example, Alpaca (Taori et al.,\\n2023) and Code Alpaca (Chaudhary, 2023) apply SELF\\x02INSTRUCT to finetune LLAMA with ChatGPT-generated\\ninstructions. To improve SELF-INSTRUCT, WizardLM (Xu\\net al., 2023) and WizardCoder (Luo et al., 2023a) propose\\nEvol-Instruct and Code Evol-Instruct by guiding ChatGPT\\nwith heuristic prompts to make the synthetic data more com\\x02plex and diverse. More recently, Gunasekar et al. (2023)\\nshows that textbook-quality synthetic data alone can help\\nthe model achieve remarkable coding and reasoning capa\\x02bilities. Orthogonal to all existing methods, our proposed\\nOSS-INSTRUCT allows LLMs to get inspired from real\\x02world code snippets for better controllability, quality, and\\ncreativity in coding tasks.\\nEvaluating LLMs for code Most code benchmarks eval\\x02uate LLMs on generating single-function programs from\\nnatural language descriptions. Such benchmarks include\\nHumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),\\nAPPS (Hendrycks et al., 2021), and CodeContests (Li et al.,\\n2022). A handful of manual tests are used to assess the\\nfunctional correctness of LLM-generated solutions. How\\x02ever, insufficient tests can lead to false negatives',\n",
              "  'keywords': ['llm', 'codecontests', 'synthetic', 'instruct', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '. Conse\\x02quently, the EvalPlus framework (Liu et al., 2023b) pro\\x02duces HumanEval+ and MBPP+ by extending 80×/35×\\nmore tests. To address dataset contamination issues, re\\x02searchers propose LiveCodeBench (Jain et al., 2024), which\\ncompiles fresh coding problems not included in model\\ntraining, and EvoEval (Xia et al., 2024), which strategi\\x02cally leverages LLMs to evolve existing benchmarks into\\nnew coding tasks. Meanwhile, there are comprehensive\\nbenchmarks evaluating code generation for data science\\n(DS-1000 (Lai et al., 2022)), addressing open-source issues\\n(SWE-bench (Jimenez et al., 2023)), and repository-level\\ncode generation (CROSSCODEEVAL (Ding et al., 2023) and\\nRepoEval (Zhang et al., 2023)).\\n6. Conclusion and Future Work\\nWe propose OSS-INSTRUCT, a novel data generation\\nmethod using Large Language Models to generate diverse\\ncoding challenges from open-source code snippets. This\\napproach enables Magicoder, which significantly improves\\nthe base LLM. Despite having less than 7B parameters, it\\ncan outperform all evaluate LLMs with less than or equal to\\n16B parameters, including the 15B WizardCoder. Combin\\x02ing OSS-INSTRUCT with Evol-Instruct allows us to build\\nthe enhanced MagicoderS models. They achieve',\n",
              "  'keywords': ['livecodebench', 'evoeval', 'instruct', 'llms', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' remark\\x02able results by rivaling leading models like ChatGPT in\\nHumanEval benchmarks. We fully open source the model\\nweights, training data, and source code, to enable future\\nresearch in LLMs for code. In the near future, we will ap\\x02ply OSS-INSTRUCT to larger base models. We will also\\ncontinue advancing OSS-INSTRUCT by generating higher\\x02quality data with a strategically designed distribution of the\\nseed code snippets and with more advanced teacher LLMs\\nsuch as GPT-4.\\nAcknowledgement\\nWe thank all the reviewers for their insightful comments and\\nsuggestions for our paper. This work was partially supported\\nby NSF grant CCF-2131943, as well as Kwai Inc.\\nImpact Statement\\nThis work is motivated to boost large language models\\nin terms of their code generation and understanding ca\\x02pabilities through instruction tuning. The proposed OSS\\x02INSTRUCT method leverages the abundance of open source\\nto generate diverse and controllable instruction data. We ex\\x02pect this idea to also foster innovative software solutions tai\\x02lored to domain-specific needs, particularly in areas where\\nreal data is private and scarce, by generating extensive syn\\x02thetic data. Additionally, our method reinforces the value\\nof community-driven content and knowledge sharing by\\nincorporating open-source code as references.\\nHowever, it is essential to recognize the potential for misuse,\\nsuch as the deliberate generation of vulnerable code that can\\nbe exploited for malicious purposes. Ultimately, adhering',\n",
              "  'keywords': ['chatgpt', 'instruction', 'llms', 'source', '2131943']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\nto ethical guidelines is crucial to ensure the responsible use\\nof this technique.\\nReferences\\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Fer\\x02randis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey,\\nM., Umapathi, L. K., Anderson, C. J., Zi, Y., Poirier, J. L.,\\nSchoelkopf, H., Troshin, S., Abulkhanov, D., Romero,\\nM., Lappert, M., Toni, F. D., del R´ıo, B. G., Liu, Q.,\\nBose, S., Bhattacharyya, U., Zhuo, T. Y., Yu, I., Villegas,\\nP., Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H.,\\nContractor, D., Villa, L., Li, J., Bahdanau, D., Jernite, Y.,\\nHughes, S., Fried, D., Guha, A., de Vries, H., and von\\nWerra, L. Santacoder: don’t reach for the stars!, 2023.\\nAustin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,\\nH., Dohan, D., Jiang',\n",
              "  'keywords': ['santacoder', 'references', 'ethical', 'yu', 'werra']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ', E., Cai, C. J., Terry, M., Le, Q. V.,\\nand Sutton, C. Program synthesis with large language\\nmodels. CoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732.\\nBen Allal, L., Muennighoff, N., Kumar Umapathi,\\n9\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nL., Lipkin, B., and von Werra, L. A framework\\nfor the evaluation of code generation models.\\nhttps://github.com/bigcode-project/\\nbigcode-evaluation-harness, 2022.\\nBouzenia, I., Devanbu, P., and Pradel, M. Repairagent: An\\nautonomous, llm-based agent for program repair. arXiv\\npreprint arXiv:2403.17134, 2024.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\\nWinter, C., Hesse,',\n",
              "  'keywords': ['bigcode', 'repairagent', 'magicoder', 'synthesis', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' C., Chen, M., Sigler, E., Litwin, M.,\\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,\\nRadford, A., Sutskever, I., and Amodei, D. Language\\nmodels are few-shot learners. In Larochelle, H.,\\nRanzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),\\nAdvances in Neural Information Processing Systems,\\nvolume 33, pp. 1877–1901. Curran Associates, Inc.,\\n2020. URL https://proceedings.neurips.\\ncc/paper_files/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\\npdf.\\nCambronero, J., Gulwani, S., Le, V., Perelman, D., Rad\\x02hakrishna, A., Simon, C., and Tiwari, A. Flashfill++:\\nScaling programming by example by cutting to the\\nchase. Proc. ACM Program. Lang., 7(POPL), jan 2023.\\ndoi: 10.1145/3571226. URL https://doi.org/10.\\n1145/3571226.\\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps\\x02Costin, L., Pinck',\n",
              "  'keywords': ['scaling',\n",
              "   'models',\n",
              "   'chess',\n",
              "   '1457c0d6bfcb4967418bfb8ac142f64a',\n",
              "   'amodei']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'ney, D., Yee, M.-H., Zi, Y., Anderson,\\nC. J., Feldman, M. Q., Guha, A., Greenberg, M., and\\nJangda, A. Multipl-e: A scalable and extensible approach\\nto benchmarking neural code generation, 2022.\\nChaudhary, S. Code alpaca: An instruction-following llama\\nmodel for code generation. https://github.com/\\nsahil280114/codealpaca, 2023.\\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\\nH. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar\\x02ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\\nPlappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\\nJ., Babuschkin, I.,',\n",
              "  'keywords': ['codealpaca', 'neural', 'benchmarking', 'multipl', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' Balaji, S., Jain, S., Saunders, W.,\\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\\nV., Morikawa, E., Radford, A., Knight, M., Brundage,\\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\\nW. Evaluating large language models trained on code,\\n2021.\\nChen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large ¨\\nlanguage models to self-debug, 2023.\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., Hesse, C., and Schulman, J. Training verifiers to solve\\nmath word problems, 2021.\\nDeng, Y., Xia, C. S., Peng, H., Yang, C., and Zhang, L.\\nLarge language models are zero-shot fuzzers: Fuzzing\\ndeep-learning libraries via large language models, 2023.\\nDing, Y., Wang, Z., Ahmad, W. U., Ding, H., Tan, M.,\\nJain, N',\n",
              "  'keywords': ['verifiers', 'balaji', 'fuzzing', '2023', 'deng']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '., Ramanathan, M. K., Nallapati, R., Bhatia, P.,\\nRoth, D., and Xiang, B. Crosscodeeval: A diverse\\nand multilingual benchmark for cross-file code comple\\x02tion. In Thirty-seventh Conference on Neural Informa\\x02tion Processing Systems Datasets and Benchmarks Track,\\n2023. URL https://openreview.net/forum?\\nid=wgDcbBMSfh.\\nFeng, Y., Martins, R., Bastani, O., and Dillig, I. Program\\nsynthesis using conflict-driven learning. SIGPLAN Not.,\\n53(4):420–435, jun 2018. ISSN 0362-1340. doi: 10.\\n1145/3296979.3192382. URL https://doi.org/\\n10.1145/3296979.3192382.\\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace,\\nE., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and\\nLewis, M. Incoder: A generative model for code infilling\\nand synthesis. In The Eleventh International Confer\\x02ence on Learning Representations, 2023. URL https:\\n//openreview.net/forum?id=hQwb-lbM6EL.\\nGulwani, S., Polozov, O., and',\n",
              "  'keywords': ['crosscodeeval',\n",
              "   'generative',\n",
              "   'infilling',\n",
              "   'benchmarks',\n",
              "   '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' Singh, R. Program syn\\x02thesis. Foundations and Trends® in Programming Lan\\x02guages, 4(1-2):1–119, 2017. ISSN 2325-1107. doi:\\n10.1561/2500000010. URL http://dx.doi.org/\\n10.1561/2500000010.\\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T.,\\nGiorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P.,\\nde Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl,\\nH. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee,\\nY. T., and Li, Y. Textbooks are all you need, 2023.\\nGuo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W.,\\nChen, G., Bi, X., Wu, Y., Li, Y. K., Luo, F., Xiong, Y.,\\nand Liang, W. Deepseek-coder: When the large language\\nmodel meets programming – the rise of code intelligence,\\n2024.\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\\nA., Guo',\n",
              "  'keywords': ['deepseek', 'coder', 'textbooks', '2500000010', 'zhang']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ', E., Burns, C., Puranik, S., He, H., Song, D., and\\nSteinhardt, J. Measuring coding challenge competence\\nwith apps, 2021.\\n10\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnat\\x02ural instructions: Tuning language models with (almost)\\nno human labor. In Rogers, A., Boyd-Graber, J., and\\nOkazaki, N. (eds.), Proceedings of the 61st Annual Meet\\x02ing of the Association for Computational Linguistics (Vol\\x02ume 1: Long Papers), pp. 14409–14428, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2023.acl-long.806. URL https:\\n//aclanthology.org/2023.acl-long.806.\\nHugging Face. Hugging face: The ai community build\\x02ing the future. https://huggingface.co/, 2023.\\nAccessed: 2023-12-01.\\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and\\nBrockschmidt, M. Codesearchnet challenge: Evaluat\\x02ing the state of semantic code search, 2020.\\nJain, N., Han, K., Gu,',\n",
              "  'keywords': ['codesearchnet',\n",
              "   'magicoder',\n",
              "   'huggingface',\n",
              "   'honovich',\n",
              "   '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' A., Li, W.-D., Yan, F., Zhang, T.,\\nWang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Live\\x02codebench: Holistic and contamination free evaluation of\\nlarge language models for code, 2024.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\\nT., and Sayed, W. E. Mistral 7b, 2023a.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna,\\nE. B., Bressand, F., Lengyel, G., Bour, G., Lample, G.,\\nLavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P.,\\nSubramanian, S., Yang, S., Antoniak, S., Scao, T. L.,\\nGervet, T',\n",
              "  'keywords': ['codebench', 'contamination', 'holistic', 'lavaud', '2024']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '., Lavril, T., Wang, T., Lacroix, T., and Sayed,\\nW. E. Mixtral of experts, 2024.\\nJiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code\\nlanguage models on automated program repair, 2023b.\\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press,\\nO., and Narasimhan, K. Swe-bench: Can language mod\\x02els resolve real-world github issues?, 2023.\\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis,\\nC. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T.,\\nBahdanau, D., von Werra, L., and de Vries, H. The stack:\\n3 tb of permissively licensed source code, 2022.\\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer,\\nL., tau Yih, S. W., Fried, D., Wang, S., and Yu, T. Ds\\x021000: A natural and reliable benchmark for data science\\ncode generation, 2022.\\nLemieux, C., Inala, J. P., Lahiri, S. K., and Sen, S. Co',\n",
              "  'keywords': ['github', 'repair', 'lavril', '2023b', 'permissively']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\x02damosa: Escaping coverage plateaus in test genera\\x02tion with pre-trained large language models. In 2023\\nIEEE/ACM 45th International Conference on Software\\nEngineering (ICSE), pp. 919–931. IEEE, 2023.\\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D.,\\nMou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q.,\\nZheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O.,\\nDavaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko,\\nO., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Uma\\x02pathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang,\\nZ., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov,\\nD., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhat\\x02tacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas,\\nP., Kunakov, M., Zhdanov, F., Romero,',\n",
              "  'keywords': ['coverage', 'zebaze', 'zheltonozhskii', 'icse', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' M., Lee, T., Timor,\\nN., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J.,\\nDao, T., Mishra, M., Gu, A., Robinson, J., Anderson,\\nC. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried,\\nD., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes,\\nS., Wolf, T., Guha, A., von Werra, L., and de Vries, H.\\nStarcoder: may the source be with you!, 2023.\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser,\\nJ., Leblond, R., Eccles, T., Keeling, J., Gimeno, F.,\\nDal Lago, A., Hubert, T., Choy, P., de Masson d’Autume,\\nC., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J.,\\nGowal, S., Cherepanov, A., Molloy, J., Mankowitz,\\nD. J., Sutherland Robson, E., Kohli, P., de Freitas,\\nN., Kavukcuoglu, K., and Vinyals, O. Competition\\x02level code generation with alphac',\n",
              "  'keywords': ['starcoder', 'competition', 'jernite', 'gowal', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'ode. Science, 378\\n(6624):1092–1097, December 2022. ISSN 1095-9203.\\ndoi: 10.1126/science.abq1158. URL http://dx.doi.\\norg/10.1126/science.abq1158.\\nLiu, J., Peng, J., Wang, Y., and Zhang, L. Neuri: Di\\x02versifying dnn generation via inductive rule inference.\\nIn Proceedings of the 31st ACM Joint European Soft\\x02ware Engineering Conference and Symposium on the\\nFoundations of Software Engineering, ESEC/FSE 2023,\\npp. 657–669, New York, NY, USA, 2023a. Associa\\x02tion for Computing Machinery. ISBN 9798400703270.\\ndoi: 10.1145/3611643.3616337. URL https://doi.\\norg/10.1145/3611643.3616337.\\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code\\ngenerated by chatGPT really correct? rigorous evaluation\\nof large language models for code generation. In Thirty\\x02seventh Conference on Neural Information Processing\\nSystems, 2023b. URL https://openreview.net/\\nforum?id=1qvx610Cu7.\\nLozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poir',\n",
              "  'keywords': ['generated', 'neuri', 'chatgpt', '1qvx610cu7', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'ier,\\nJ., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T.,\\nTian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang,\\nZ., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.-D.,\\nRisdal, M., Li, J., Zhu, J., Zhuo, T. Y., Zheltonozhskii,\\nE., Dade, N. O. O., Yu, W., Krauß, L., Jain, N., Su, Y.,\\nHe, X., Dey, M., Abati, E., Chai, Y., Muennighoff, N.,\\nTang, X., Oblokulov, M., Akiki, C., Marone, M., Mou,\\nC., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A.,\\nDehaene, O., Patry, N., Xu, C., McAuley, J., Hu, H.,\\n11\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nScholak, T., Paquet, S., Robinson, J., Anderson, C. J.,\\nChapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y.,\\nFerr',\n",
              "  'keywords': ['magicoder', 'code', 'empowering', 'ier', 'zhuo']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'andis, C. M., Zhang, L., Hughes, S., Wolf, T., Guha,\\nA., von Werra, L., and de Vries, H. Starcoder 2 and the\\nstack v2: The next generation, 2024.\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C.,\\nMa, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering\\ncode large language models with evol-instruct. arXiv\\npreprint arXiv:2306.08568, 2023a.\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C.,\\nMa, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering\\ncode large language models with evol-instruct, 2023b.\\nMicrosoft. Azure openai service models. https:\\n//learn.microsoft.com/en-us/azure/\\ncognitive-services/openai/concepts/\\nmodels, 2023a.\\nMicrosoft. GitHub Copilot – Your AI pair pro\\x02grammer. https://github.com/features/\\ncopilot, 2023b.\\nMuennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B.,\\nZhuo, T. Y., Singh',\n",
              "  'keywords': ['wizardcoder', 'openai', 'evol', 'zhuo', '2023b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ', S., Tang, X., von Werra, L., and\\nLongpre, S. Octopack: Instruction tuning code large\\nlanguage models, 2023.\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H.,\\nZhou, Y., Savarese, S., and Xiong, C. Codegen: An\\nopen large language model for code with multi-turn pro\\x02gram synthesis. In The Eleventh International Confer\\x02ence on Learning Representations, 2023. URL https:\\n//openreview.net/forum?id=iaYcJKpY2B_.\\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and\\nSolar-Lezama, A. Is self-repair a silver bullet for code\\ngeneration? In The Twelfth International Conference\\non Learning Representations, 2024. URL https://\\nopenreview.net/forum?id=y0GJXRungR.\\nOpenAI. Chatgpt: Optimizing language models for dialogue.\\nhttps://openai.com/blog/chatgpt/, 2022.\\nOpenAI. Gpt-4 technical report, 2023.\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, `\\nX. E., Adi, Y., Liu, J., Remez, T.,',\n",
              "  'keywords': ['chatgpt', 'codegen', 'openai', '2022', 'tuning']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' Rapin, J., Kozhevnikov,\\nA., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C.,\\nGrattafiori, A., Xiong, W., Defossez, A., Copet, J., Azhar, ´\\nF., Touvron, H., Martin, L., Usunier, N., Scialom, T., and\\nSynnaeve, G. Code llama: Open foundation models for\\ncode, 2023.\\nSchafer, M., Nadi, S., Eghbali, A., and Tip, F. An empirical ¨\\nevaluation of using large language models for automated\\nunit test generation. IEEE Transactions on Software En\\x02gineering, 2023.\\nServices, A. W. AI Code Generator - Amazon Code\\x02Whisperer - AWS. https://aws.amazon.com/\\ncodewhisperer/, 2023.\\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\\nrates with sublinear memory cost, 2018.\\nSPARCK JONES, K. A statistical interpretation of term\\nspecificity and its application in retrieval. 28(1):11–21,\\n2023/11/30 1972. doi: 10.1108/eb026526. URL https:\\n//doi.org/10.1108/eb026526.\\nSu, H., Shi, W., Kasai',\n",
              "  'keywords': ['codewhisperer', 'eb026526', 'specificity', '2023', 'llama']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ', J., Wang, Y., Hu, Y., Ostendorf,\\nM., Yih, W.-t., Smith, N. A., Zettlemoyer, L., and Yu, T.\\nOne embedder, any task: Instruction-finetuned text em\\x02beddings. 2022. URL https://arxiv.org/abs/\\n2212.09741.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\\nStanford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca, 2023.\\ntheblackcat102. The evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/\\ntheblackcat102/evol-codealpaca-v1,\\n2023.\\nWang, X., Dillig, I., and Singh, R. Program synthesis using\\nabstraction refinement. Proc. ACM Program. Lang., 2\\n(POPL), dec 2017. doi: 10.1145/3158151. URL https:\\n//doi.org/10.1145/3158151.\\nWang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5:\\nIdentifier-',\n",
              "  'keywords': ['codealpaca', 'instruction', 'embedder', 'guestrin', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'aware unified pre-trained encoder-decoder mod\\x02els for code understanding and generation. In Moens,\\nM.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Pro\\x02ceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 8696–8708, On\\x02line and Punta Cana, Dominican Republic, November\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.emnlp-main.685. URL https://\\naclanthology.org/2021.emnlp-main.685.\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\\nKhashabi, D., and Hajishirzi, H. Self-instruct: Align\\x02ing language models with self-generated instructions. In\\nRogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Pro\\x02ceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\npp. 13484–13508, Toronto, Canada, July 2023a. Associ\\x02ation for Computational Linguistics. doi: 10.18653/v1/\\n2023.acl-long.754. URL https://aclanthology.\\norg/2023.acl-long',\n",
              "  'keywords': ['encoder', 'computational', 'emnlp', 'associ', '2023a']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '.754.\\n12\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nWang, Y., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and\\nHoi, S. C. H. Codet5+: Open code large language models\\nfor code understanding and generation, 2023b.\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\\nmodels are zero-shot learners, 2022.\\nWei, Y., Xia, C. S., and Zhang, L. Copiloting the copilots:\\nFusing large language models with completion engines\\nfor automated program repair, 2023.\\nXia, C. S. and Zhang, L. Less training, more repairing\\nplease: Revisiting automated program repair via zero\\x02shot learning, 2022.\\nXia, C. S. and Zhang, L. Keep the conversation going:\\nFixing 162 out of 337 bugs for $0.42 each using chatgpt.\\narXiv preprint arXiv:2304.00385, 2023.\\nXia, C. S., Paltenghi, M., Tian, J. L., Pradel, M., and Zhang,\\nL. Universal fuzzing via large language models, 20',\n",
              "  'keywords': ['chatgpt', 'magicoder', 'code', 'fuzzing', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '23a.\\nXia, C. S., Wei, Y., and Zhang, L. Automated program\\nrepair in the era of large pre-trained language models.\\nIn 2023 IEEE/ACM 45th International Conference on\\nSoftware Engineering (ICSE), pp. 1482–1494, 2023b. doi:\\n10.1109/ICSE48619.2023.00129.\\nXia, C. S., Deng, Y., and Zhang, L. Top leaderboard ranking\\n= top coding proficiency, always? evoeval: Evolving\\ncoding benchmarks via llm, 2024.\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\\nC., and Jiang, D. Wizardlm: Empowering large language\\nmodels to follow complex instructions. arXiv preprint\\narXiv:2304.12244, 2023.\\nYu, Y., Zhuang, Y., Zhang, J., Meng, Y., Ratner, A., Krishna,\\nR., Shen, J., and Zhang, C. Large language model as\\nattributed training data generator: A tale of diversity and\\nbias, 2023.\\nYuan, Z., Lou, Y., Liu, M., Ding, S., Wang, K., Chen, Y.,\\nand Peng, X. No more manual tests? evaluating and\\nimproving chatgpt for unit test generation. arXiv preprint\\n',\n",
              "  'keywords': ['chatgpt', 'automated', 'llm', 'zhang', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'arXiv:2305.04207, 2023.\\nZhang, F., Chen, B., Zhang, Y., Keung, J., Liu, J., Zan,\\nD., Mao, Y., Lou, J.-G., and Chen, W. Repocoder:\\nRepository-level code completion through iterative re\\x02trieval and generation, 2023.\\n13\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nYou are exceptionally skilled at crafting high-quality programming problems and \\noffering precise solutions.\\nPlease gain inspiration from the following random code snippet to create a \\nhigh-quality programming problem. Present your output in two distinct sections: \\n[Problem Description] and [Solution].\\nCode snippet for inspiration:\\n```\\n{code}\\n```\\nGuidelines for each section:\\n1. [Problem Description]: This should be **completely self-contained**, providing \\nall the contextual information one needs to understand and solve the problem. \\nAssume common programming knowledge, but ensure that any specific context, \\nvariables, or code snippets pertinent to this problem are explicitly included.\\n2. [Solution]: Offer a comprehensive, **correct** solution that accurately \\naddresses the [Problem Description] you provided.\\nFigure 4: The detailed prompt design for OSS-INSTRUCT\\nA. More Details of OSS-INSTRUCT\\nA.1. Prompt Design\\nFigure 4 illustrates the prompt template of OSS-INSTRUCT, where the first',\n",
              "  'keywords': ['repocoder', 'oss', 'iterative', 'instruct', '2023']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' section presents a high-level description of the\\ntask, the second section incorporates the code snippet, and the third section offers guidelines on the response.\\nA.2. Qualitative Examples\\nFigure 5 extends Figure 2 and shows more qualitative examples of OSS-INSTRUCT generation, including outputs that are\\ninspired by method definitions, shell scripts, library imports, class signatures, code statements, and code comments.\\nA.3. Breakdown of OSS-INSTRUCT Dataset\\nCategories To study the categories of OSS-INSTRUCT-generated data, we use INSTRUCTOR (Su et al., 2022), which is\\none of the SOTA embedding models and can generate different text embeddings according to a task instruction. Inspired by\\nOctoPack (Muennighoff et al., 2023) and the topic tags on GitHub, we manually designed 10 categories specific to coding.\\nAs shown in Figure 6, we calculate the cosine similarity between the embeddings of each sample in OSS-INSTRUCT and\\nthe 10 categories to obtain the category breakdown. Overall, OSS-INSTRUCT exhibits diversity and balance across different\\ncategories.\\nLength distribution We depict the length distribution for both generated problems and solutions in Figure 7. The x-axis\\nrepresents the number of tokens in each problem/solution, while the y-axis shows the correspondent number of samples.\\nB. Implementation Details\\nB.1. Data Generation\\nWe use gpt-3.5-turbo-1106 as the foundation model',\n",
              "  'keywords': ['embeddings', 'instruct', 'gpt', 'high', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' to do OSS-INSTRUCT due to its high cost-effectiveness. We\\nrandomly extract 1–15 lines from each selected code document from starcoderdata and let gpt-3.5-turbo-1106\\nimagine a self-contained coding problem and a correct solution. Given the numerous seed code snippets, we perform greedy\\ndecoding to maximize the consistency between the generated problems and solutions.\\n14\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD Seed: method definition\\nrender() {\\nQ Problem\\nYour task is to complete the `render` method to \\ngenerate the rendered shape as a string...\\nL Code\\nclass ShapeRenderer {\\n constructor(vertices) {\\n this.vertices = vertices;\\n }\\n render() {\\n let renderedShape = \"\";\\n for (let i = 0; i < this.vertices.length; i++) {\\n const vertex = this.vertices[i];\\n renderedShape += `(${vertex.x}, ${vertex.y})`;\\n if (i < this.vertices.length - 1) {\\n renderedShape += \" - \";\\n }\\n }\\n return renderedShape;\\n }\\n}\\nD Seed: library imports\\nimport numpy as np\\nimport gym_electric_motor as gem\\nimport matplotlib.pyplot as plt\\nQ Problem\\nCreate a reinforcement learning agent to control an \\nelectric motor using the OpenAI Gym',\n",
              "  'keywords': ['shaperenderer',\n",
              "   'generated',\n",
              "   'gym_electric_motor',\n",
              "   'code',\n",
              "   'starcoderdata']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' environment...\\nL Code\\nimport numpy as np\\nimport gym_electric_motor as gem\\n...\\nenv = gem.make(\"DcSeriesCont-v1\")\\nclass DQNAgent:\\n def __init__(self, state_dim, action_dim): ...\\n def build_model(self): ...\\n def act(self, state): ...\\n def train(self, state, action, reward, ...): ...\\n...\\nfor episode in range(episodes):\\n state = env.reset()\\n state = np.reshape(state, [1, state_dim])\\n ...\\nD Seed: class signature\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {\\n public static void main(String[] args) {\\nQ Problem\\nCreate a simple Java Spring Boot application for a \\nbanking system...\\nL Code\\nimport org.axonframework.commandhandling.CommandBus;\\nimport org.axonframework.config.Configuration;\\n...\\n@SpringBootApplication\\n@Import({ AxonConfig.class })\\npublic class AxonbankApplication {...}\\npublic class BankAccount {...}\\npublic class CreateAccountCommand {...}\\npublic class DepositFundsCommand {...}\\npublic class WithdrawFundsCommand {...}\\npublic class AccountCreatedEvent {...}\\npublic class FundsDepositedEvent {...}\\npublic class FundsWithdrawnEvent {...}\\nD Seed:',\n",
              "  'keywords': ['axonbankapplication',\n",
              "   'withdrawfundscommand',\n",
              "   '__init__',\n",
              "   'state_dim',\n",
              "   'args']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': \" code statements\\ncutoff_range = np.ptp(cutoffs)\\nif cutoff_range == 0: cutoff_range = 1\\ncutoff_colors = plt.get_cmap('plasma')(\\n (cutoffs - np.min(cutoffs)) / cutoff_range\\n)\\nQ Problem\\nImplement a function that calculates the color values \\nfor a given set of cutof values based on a specified \\ncolor map...\\nL Code\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef calculate_cutoff_colors(cutoffs, cmap_name):\\n cutoff_range = np.ptp(cutoffs)\\n if cutoff_range == 0:\\n cutoff_range = 1\\n cmap = plt.get_cmap(cmap_name)\\n normalized_cutoffs = ...\\n cutoff_colors = ...\\n return cutoff_colors\\nD Seed: shell script\\npython3 makeErrorFile.py data/test_dataset_14 14\\nQ Problem\\nCreate a Python program that generates an error file\\nbased on a given dataset...\\nL Code\\ndef generate_error_file(dataset_file, ...):\\n error_lines = []\\n with open(dataset_file, 'r') as file:\\n for line in file:\\n ...\\n with open(error_file_name, 'w') as error_file:\\n for error_line in error_lines\",\n",
              "  'keywords': ['calculate_cutoff_colors',\n",
              "   'makeerrorfile',\n",
              "   'test_dataset_14',\n",
              "   'cmap',\n",
              "   'py']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ':\\n error_file.write(error_line + \\'\\\\n\\')\\nif __name__ == \"__main__\":\\n if len(sys.argv) != 3:\\n print(\"Usage: ...\")\\n else:\\n dataset_file = sys.argv[1]\\n dataset_number = sys.argv[2]\\n generate_error_file(...)\\nD Seed: comments\\n# Set degrees\\nQ Problem\\nImplement a Python class that represents a \\ntemperature in degrees...\\nL Code\\nclass TemperatureConverter:\\n def __init__(self): ...\\n def set_celsius(self, degrees): ...\\n def set_fahrenheit(self, degrees): ...\\n def set_kelvin(self, degrees): ...\\n def get_celsius(self): ...\\n def get_fahrenheit(self): ...\\n def get_kelvin(self): ...\\n def convert_to(self, unit):\\n if unit == \\'C\\':\\n return self.get_celsius()\\n elif unit == \\'F\\':\\n return self.get_fahrenheit()\\n elif unit == \\'K\\':\\n return self.get_kelvin()\\n ...\\nFigure 5: More examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed\\nproblem requirements, implementations, and explanations are omitted for brevity.\\n15\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT',\n",
              "  'keywords': ['get_celsius', 'python', '__name__', 'magicoder', 'self']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\nFigure 6: The category constitution of OSS-INSTRUCT\\n0 100 200 300 400 500 600 700\\nNumber of Tokens\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n#Count (Thousand)\\nproblem\\nsolution\\nFigure 7: Token count distribution of OSS-INSTRUCT-generated problems and solutions\\nB.2. Data Decontamination\\nWe apply data decontamination before training our Magicoder and MagicoderS models. Following Li et al. (2023), we\\ndecontaminate both our 75K OSS-INSTRUCT dataset and the evol-codealpaca-v1 (theblackcat102, 2023) dataset,\\nan open-source reproduction of Evol-Instruct generated by GPT-4 (OpenAI, 2023), by removing exact matches from\\nHumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), DS-1000 (Lai et al., 2022), and GSM8K (Cobbe et al., 2021).\\nEventually, we filtered out 9 problems for OSS-INSTRUCT dataset and 89 for evol-codealpaca-v1.\\nB.3. Training\\nWe employ CODELLAMA-PYTHON-7B and DeepSeek-Coder-Base 6.7B as the base LLMs. To obtain Magicoder series, we\\nfirst finetune the base models on about 75K synthetic data generated through OSS-INSTRUCT using the',\n",
              "  'keywords': ['magicoder', 'codealpaca', 'instruct', '2022', 'llms']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' transformers\\nlibrary from Hugging Face (Hugging Face, 2023). We finetune the base models for 2 epochs using two NVIDIA A100-80GB\\nGPUs through the Distributed Data Parallel (DDP) module from PyTorch. We set the initial learning rate at 5e-5 with 15\\nwarmup steps and a linear scheduler. We use Adafactor (Shazeer & Stern, 2018) as our optimizer and choose a batch size of\\n512 with a sequence truncation length of 1216. To obtain MagicoderS, we continue to finetune Magicoder models with the\\nevol-codealpaca-v1 dataset, an open-source Evol-Instruct implementation containing about 110K samples. We use\\nthe same hyperparameters except for 15 warmup steps and a 1024 maximum sequence length.\\nC. More Evaluation Results\\nC.1. Evaluation on APPS for Competitive Programming\\nWe additionally evaluate Magicoder on APPS (Hendrycks et al., 2021), a benchmark suite of competitive programming\\nproblems. Following Olausson et al. (2024), we select a subset of 300 problems from the APPS test set. From Table 8, we can\\nobserve that the CODELLAMA-PYTHON-based Magicoder-CL significantly outperforms the base model and WizardCoder\\x02CL. MagicoderS-CL-7B is even better than WizardCoder-SC-15B despite having less than half the number of parameters',\n",
              "  'keywords': ['magicoders', 'codealpaca', 'pytorch', 'evaluate', '80gb']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '.\\n16\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nMeanwhile, DeepSeek-Coder-based MagicoderS-DS achieves the best result among all the evaluated baselines, substantially\\noutperforming the instruction-tuned DeepSeek-Coder-6.7B-Instruct.\\nTable 8: Pass@1 results on APPS evaluated using greedy decoding in a zero-shot setting.\\nModel Introductory (60) Interview (180) Competition (60) Overall (300)\\nWizardCoder-SC-15B 21.7 6.1 1.7 8.3\\nCODELLAMA-PYTHON-7B 3.3 2.8 0.0 2.3\\nWizardCoder-CL-7B 10.0 3.9 1.7 4.7\\nMagicoder-CL-7B 18.3 5.6 1.7 7.3\\nMagicoderS-CL-7B 23.3 6.1 1.7 8.7\\nDeepSeek-Coder-6.7B-Base 16.7 7.2 0.0 7.7\\nDeepSeek-Coder-6.7B-Instruct 23.3 9.4 0.0 10.3\\nMagicoder-DS-6.7B 20.0 8.9 1.7 9.7\\nMagicoderS-DS-6.7B 28.3 11.7 3.3 13.3\\nC.2',\n",
              "  'keywords': ['magicoders', 'deepseek', 'code', 'introductory', '7b']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '. Fill-in-the-Middle Evaluation on DS-1000\\nTable 9 shows the evaluation results of Magicoder-DS and MagicoderS-DS on DS-1000 (Lai et al., 2022) (Insertion format),\\nassessing a model’s fill-in-the-middle capability. In this experiment, we use DeepSeek-Coder as the base model and exclude\\nCODELLAMA-PYTHON-based results, as CODELLAMA-PYTHON does not support the fill-in-the-middle format. The results\\nhighlight Magicoder’s superior performance in fill-in-the-middle tasks compared to all other evaluated baselines. This\\noutstanding capability suggests that Magicoder can serve as a valuable copilot for developers.\\nTable 9: Pass@1 results on DS-1000 (Insertion format) with temperature = 0.2, top p = 0.5, max length = 1024,\\nand num samples = 40.\\nModel NumPy Pandas PyTorch SciPy Sklearn TensorFlow Overall\\nWizardCoder-SC-15B 35.1 20.4 30.4 28.9 32.3 37.8 28.6\\nDeepSeek-Coder-6.7B-Base 36.3 28.6 15.8 19.3 32.8 35.1 29.3\\nDeepSeek-Coder-6.7B-Instruct 44.1 27.3 38.2 30.8 38',\n",
              "  'keywords': ['magicoder', 'deepseek', 'codellama', 'middle', '2022']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '.4 29.6 34.6\\nMagicoder-DS-6.7B 39.7 31.2 27.4 23.7 44.6 30.2 33.9\\nMagicoderS-DS-6.7B 43.3 29.5 39.2 26.2 44.5 36.2 35.9\\nC.3. Impact of Removing Noisy Data\\nIn §2.2, we highlight the benefits of preserving certain types of noise in instruction-tuning data. To support this argument, we\\nremoved data samples with partially implemented code, such as those containing pass or NotImplemented tokens in\\nPython problems. Table 10 shows the pass@1 performance of CODELLAMA-PYTHON-7B on HumanEval+ when finetuned\\nfor 2 epochs on the raw OSS-INSTRUCT dataset compared to the same dataset with noisy samples removed. We can observe\\nthat retaining the noisy samples results in slightly better performance.\\nTable 10: Pass@1 on HumanEval+ when finetuning CODELLAMA-PYTHON-7B with and without noisy samples.\\nSetting Dataset Size HumanEval+\\nWith noisy data 75K 55.5\\nWithout noisy data 68K 54.9\\n17\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nD. Cases where OSS-INSTRUCT Improves Code Understanding\\nIn this section, we compare the base model, DeepSeek-Coder-6.7',\n",
              "  'keywords': ['magicoder', 'finetuning', 'code', 'noisy', '75k']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'B-Base, and its OSS-INSTRUCT-tuned version, Magicoder\\x02DS-6.7B, in both in-domain and out-of-domain scenarios and demonstrate how OSS-INSTRUCT helps improve the base\\nmodel’s instruction following capability. We provide the base model with an appropriate prompt prefix to make sure it can\\nproduce a plausible output format.\\nD.1. More Precise Understanding of Requirements\\nThe following example shows that after instruction tuning, the model can understand the requirement “Check whether the\\ninput is valid” more precisely. DeepSeek-Coder-6.7B-Base only considers the integer case, while Magicoder-DS-6.7B\\neffectively checks if the parameter is of the correct type.\\nPrompt: Write a function that takes an integer a and returns True if this integer is a cube of some integer number. Note:\\nyou should check whether the input is valid.\\ndef is_cube(a):\\nif a <= 0:\\nreturn False\\nx = int(a ** (1/3))\\nreturn x**3 == a\\nDeepSeek-Coder-6.7B-Base\\ndef is_cube(a):\\nif not isinstance(a, int) or a < 0:\\nreturn False # Invalid input\\n# Check if the number is a perfect cube\\nx = int(a ** (1/3))\\nreturn x**3 == a\\nMagicoder-DS-6.7',\n",
              "  'keywords': ['is_cube', 'deepseek', 'magicoder', 'precise', 'input']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'B\\nD.2. Handling Corner Cases\\nThe example below indicates that Magicoder-DS-6.7B can correctly handle the corner case, where the input string has less\\nthan or equal to 2 characters, even if this is not explicitly stated in the prompt. This is also an out-of-domain example since\\nthe Scala programming language is not included in OSS-INSTRUCT dataset.\\nPrompt: Implement a Scala function that takes a string as input and returns a new string where all the characters are\\nreversed except the first and last characters. Reply only with the function.\\ndef reverseExceptFirstLast(str: String):\\nString = {\\nval chars = str.toCharArray\\nval first = chars(0)\\nval last = chars(chars.length - 1)\\nval middle = chars.slice(1, chars.length\\n- 1).reverse\\nfirst + middle + last\\n}\\nDeepSeek-Coder-6.7B-Base\\ndef reverseExceptFirstLast(input: String):\\nString = {\\nif (input.length <= 2) {\\ninput\\n} else {\\nval reversedMiddle = input.substring(1,\\ninput.length - 1).reverse\\ninput.charAt(0) + reversedMiddle +\\ninput.charAt(input.length - 1)\\n}\\n}\\nMagicoder-DS-6.7B\\nD.3. Generalizing to Unseen Tasks\\nWe also show that after instruction tuning, Magic',\n",
              "  'keywords': ['reverseexceptfirstlast',\n",
              "   'magicoder',\n",
              "   'scala',\n",
              "   'chars',\n",
              "   'middle']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'oder-DS-6.7B can respond plausibly to unseen requests during training.\\nThis example asks the model to create an easy Python coding exercise, which the base model cannot interpret. Such kinds of\\ntasks are also not incorporated in the OSS-INSTRUCT dataset.\\n18\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nPrompt: Create an easy Python coding exercise\\ndef add_numbers(a, b):\\nreturn a + b\\nDeepSeek-Coder-6.7B-Base\\n# Given a list of integers, return the sum\\nof all the positive integers in the\\nlist.\\ndef sum_positive_integers(nums):\\nreturn sum(num for num in nums if num >\\n0)\\n# Test the function\\nnumbers = [1, -2, 3, -4, 5, -6, 7, -8, 9,\\n-10]\\nprint(sum_positive_integers(numbers)) #\\nOutput: 25\\nMagicoder-DS-6.7B\\nE. Generating Complex Programs\\nIn this section, we present the cases where the best performing MagicoderS-DS-6.7B is capable of generating real-world,\\ncomplex programs.\\nE.1. Implementing Snake Game in Python\\nPrompt: Write a snake game in Python using the turtle library (the game is created by Magicoder).\\nResponse: Here is a simple snake game in Python using the turtle library',\n",
              "  'keywords': ['magicoder',\n",
              "   'sum_positive_integers',\n",
              "   'deepseek',\n",
              "   'snake',\n",
              "   'positive']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '. This game is a basic version and does not\\ninclude many features that a full-fledged snake game would have.\\nimport turtle\\nimport time\\nimport random\\ndelay = 0.1\\n# Score\\nscore = 0\\nhigh_score = 0\\n# Set up the screen\\nwn = turtle.Screen()\\nwn.title(\"Snake Game by Magicoder\")\\nwn.bgcolor(\"green\")\\nwn.setup(width=600, height=600)\\nwn.tracer(0) # Turns off the screen updates\\n# Snake head\\nhead = turtle.Turtle()\\nhead.speed(0)\\nhead.shape(\"square\")\\nhead.color(\"black\")\\nhead.penup()\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Snake food\\nfood = turtle.Turtle()\\nfood.speed(0)\\nfood.shape(\"circle\")\\nfood.color(\"red\")\\nfood.penup()\\nfood.goto(0, 100)\\nsegments = []\\n# Pen\\npen = turtle.Turtle()\\npen.speed(0)\\npen.shape(\"square\")\\n19\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\npen.color(\"white\")\\npen.penup()\\npen.hideturtle()\\npen.goto(0, 260)\\npen.write(\"Score: 0 High Score: 0\", align=\"center\", font=(\"Courier\", 24,',\n",
              "  'keywords': ['magicoder', 'snake', 'high_score', 'penup', 'delay']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' \"normal\"))\\n# Functions\\ndef go_up():\\nif head.direction!= \"down\":\\nhead.direction = \"up\"\\ndef go_down():\\nif head.direction!= \"up\":\\nhead.direction = \"down\"\\ndef go_left():\\nif head.direction!= \"right\":\\nhead.direction = \"left\"\\ndef go_right():\\nif head.direction!= \"left\":\\nhead.direction = \"right\"\\ndef move():\\nif head.direction == \"up\":\\ny = head.ycor()\\nhead.sety(y + 20)\\nif head.direction == \"down\":\\ny = head.ycor()\\nhead.sety(y - 20)\\nif head.direction == \"left\":\\nx = head.xcor()\\nhead.setx(x - 20)\\nif head.direction == \"right\":\\nx = head.xcor()\\nhead.setx(x + 20)\\n# Keyboard bindings\\nwn.listen()\\nwn.onkeypress(go_up, \"w\")\\nwn.onkeypress(go_down, \"s\")\\nwn.onkeypress(go_left, \"a\")\\nwn.onkeypress(go_right, \"d\")\\n# Main game loop\\nwhile True:\\nwn.update()\\n# Check for a collision with the border\\nif head.xcor() > 290 or head.xcor() < -',\n",
              "  'keywords': ['onkeypress', 'go_right', 'xcor', 'true', '290']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '290 or head.ycor() > 290 or head.ycor() < -290:\\ntime.sleep(1)\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Hide the segments\\nfor segment in segments:\\nsegment.goto(1000, 1000)\\n# Clear the segments list\\nsegments.clear()\\n# Reset the score\\nscore = 0\\n# Reset the delay\\ndelay = 0.1\\n# Update the score display\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24, \"\\n20\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nnormal\"))\\n# Check for a collision with the food\\nif head.distance(food) < 20:\\n# Move the food to a random spot\\nx = random.randint(-280, 280)\\ny = random.randint(-280, 280)\\nfood.goto(x, y)\\n# Add a segment\\nnew_segment = turtle.Turtle()\\nnew_segment.speed(0)\\nnew_segment.shape(\"square\")\\nnew_segment.color(\"grey\")\\nnew_segment.penup()\\nsegments.append(new_segment)\\n# Shorten the delay\\ndelay -= 0.001\\n# Increase the score\\nscore += 10\\nif score > high_',\n",
              "  'keywords': ['magicoder', 'high_score', 'delay', '280', 'increase']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'score:\\nhigh_score = score\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24, \"\\nnormal\"))\\n# Move the end segments first in reverse order\\nfor index in range(len(segments) - 1, 0, -1):\\nx = segments[index - 1].xcor()\\ny = segments[index - 1].ycor()\\nsegments[index].goto(x, y)\\n# Move segment 0 to where the head is\\nif len(segments) > 0:\\nx = head.xcor()\\ny = head.ycor()\\nsegments[0].goto(x, y)\\nmove()\\n# Check for head collision with the body segments\\nfor segment in segments:\\nif segment.distance(head) < 20:\\ntime.sleep(1)\\nhead.goto(0, 0)\\nhead.direction = \"stop\"\\n# Hide the segments\\nfor segment in segments:\\nsegment.goto(1000, 1000)\\n# Clear the segments list\\nsegments.clear()\\n# Reset the score\\nscore = 0\\n# Reset the delay\\ndelay = 0.1\\n# Update the score display\\npen.clear()\\npen.write(\"Score: {} High Score: {}\".format(score, high_score), align=\"center\", font=(\"Courier\", 24,',\n",
              "  'keywords': ['high_score', 'delay', 'segments', 'pen', 'ycor']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' \"\\nnormal\"))\\ntime.sleep(delay)\\nwn.mainloop()\\nThis game is a simple version of the classic snake game. The snake starts with a single segment and grows in size when it\\neats food. The game ends when the snake hits the border or its own body. The score is increased by 10 points for each\\npiece of food eaten. The game speed increases as the snake grows.\\n21\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nE.2. Building Othello Game in Java\\nPrompt: Build a console-based Othello game in Java with row and column numbers shown on the board. The game\\nshould end when there are no more valid moves for either player.\\nResponse: Here is a simple console-based Othello game in Java. It does not include the graphical interface, but it does\\ninclude the row and column numbers on the board.\\nimport java.util.Scanner;\\npublic class Othello {\\nprivate static final int BOARD_SIZE = 8;\\nprivate static final char EMPTY = ’-’;\\nprivate static final char BLACK = ’B’;\\nprivate static final char WHITE = ’W’;\\nprivate static final char[][] board = new char[BOARD_SIZE][BOARD_SIZE];\\nprivate static char currentPlayer = BLACK;\\npublic static void main(String[] args) {\\ninitializeBoard();\\nprintBoard();\\nwhile (true)',\n",
              "  'keywords': ['initializeboard',\n",
              "   'othello',\n",
              "   'magicoder',\n",
              "   'graphical',\n",
              "   'snake']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' {\\nSystem.out.println(\"Player \" + currentPlayer + \", enter your move (row column):\");\\nScanner scanner = new Scanner(System.in);\\nint row = scanner.nextInt();\\nint col = scanner.nextInt();\\nif (isValidMove(row, col)) {\\nmakeMove(row, col);\\nprintBoard();\\nif (!hasValidMove()) {\\nSystem.out.println(\"Player \" + currentPlayer + \" has no valid moves. Game over!\");\\nbreak;\\n}\\ncurrentPlayer = (currentPlayer == BLACK)? WHITE : BLACK;\\n} else {\\nSystem.out.println(\"Invalid move. Try again.\");\\n}\\n}\\n}\\nprivate static void initializeBoard() {\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nboard[i][j] = EMPTY;\\n}\\n}\\nboard[3][3] = board[4][4] = BLACK;\\nboard[3][4] = board[4][3] = WHITE;\\n}\\nprivate static void printBoard() {\\nSystem.out.print(\" \");\\nfor (int i = 0; i < BOARD_SIZE; i++) {\\nSystem.out.print(i + \" \");\\n}\\nSystem.out.println();\\nfor (int i = 0; i < BOARD_SIZE; i++)',\n",
              "  'keywords': ['initializeboard', 'enter', 'isvalidmove', 'black', 'println']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' {\\nSystem.out.print(i + \" \");\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nSystem.out.print(board[i][j] + \" \");\\n}\\nSystem.out.println();\\n}\\n}\\nprivate static boolean isValidMove(int row, int col) {\\nif (board[row][col]!= EMPTY) {\\nreturn false;\\n}\\nfor (int i = -1; i <= 1; i++) {\\nfor (int j = -1; j <= 1; j++) {\\nif (i!= 0 || j!= 0) {\\nif (isValidDirection(row, col, i, j)) {\\nreturn true;\\n}\\n}\\n22\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\n}\\n}\\nreturn false;\\n}\\nprivate static boolean isValidDirection(int row, int col, int rowDir, int colDir) {\\nint newRow = row + rowDir;\\nint newCol = col + colDir;\\nwhile (newRow >= 0 && newRow < BOARD_SIZE && newCol >= 0 && newCol < BOARD_SIZE) {\\nif (board[newRow][newCol] == EMPTY) {\\nreturn false;\\n}\\nif (board[newRow][newCol] == currentPlayer) {\\nreturn true;\\n}\\nnewRow += rowDir;',\n",
              "  'keywords': ['magicoder',\n",
              "   'false',\n",
              "   'isvaliddirection',\n",
              "   'board_size',\n",
              "   'currentplayer']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '\\nnewCol += colDir;\\n}\\nreturn false;\\n}\\nprivate static void makeMove(int row, int col) {\\nboard[row][col] = currentPlayer;\\nfor (int i = -1; i <= 1; i++) {\\nfor (int j = -1; j <= 1; j++) {\\nif (i!= 0 || j!= 0) {\\nflipDirection(row, col, i, j);\\n}\\n}\\n}\\n}\\nprivate static void flipDirection(int row, int col, int rowDir, int colDir) {\\nint newRow = row + rowDir;\\nint newCol = col + colDir;\\nwhile (newRow >= 0 && newRow < BOARD_SIZE && newCol >= 0 && newCol < BOARD_SIZE) {\\nif (board[newRow][newCol] == EMPTY) {\\nreturn;\\n}\\nif (board[newRow][newCol] == currentPlayer) {\\nwhile (true) {\\nnewRow -= rowDir;\\nnewCol -= colDir;\\nif (newRow == row && newCol == col) {\\nbreak;\\n}\\nboard[newRow][newCol] = currentPlayer;\\n}\\nreturn;\\n}\\nnewRow += rowDir;\\nnewCol += colDir;\\n}\\n}\\nprivate static boolean hasValidMove() {\\nfor (int i = 0; i < BOARD_SIZE; i',\n",
              "  'keywords': ['false',\n",
              "   'currentplayer',\n",
              "   'board_size',\n",
              "   'hasvalidmove',\n",
              "   'rowdir']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '++) {\\nfor (int j = 0; j < BOARD_SIZE; j++) {\\nif (isValidMove(i, j)) {\\nreturn true;\\n}\\n}\\n}\\nreturn false;\\n}\\n}\\nThis program starts with a 8x8 board with two pieces in the center of the board, one black and one white. The player who\\ngoes first is black. The game ends when there are no more valid moves for either player. The player enters their move as\\ntwo numbers, the row and the column. If the move is valid, the game makes the move and switches the player. If the\\nmove is not valid, the player is asked to enter a valid move.\\nE.3. Writing Specific Gradio Application\\n23\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nPrompt: Write a gradio (3.48.0) application for the following use case: Take an input image and return a 45 degree\\nclockwise rotated image. You should also add text description under the output showing the rotation degree.\\nResponse: Here is a simple snake game in Python using the turtle library. This game is a basic version and does not\\ninclude many features that a full-fledged snake game would have.\\nimport gradio as gr\\nimport numpy as np\\nfrom PIL import Image\\ndef rotate_image(img):\\nimg = Image.fromarray(img)\\nrotated_img = img.rotate(-45)\\nreturn np.',\n",
              "  'keywords': ['rotate_image', 'gradio', 'numpy', 'snake', 'magicoder']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'array(rotated_img), \"45 degree clockwise rotation\"\\ndemo = gr.Interface(\\nfn=rotate_image,\\ninputs=gr.inputs.Image(type=\"numpy\"),\\noutputs=[gr.outputs.Image(type=\"numpy\"), \"text\"],\\ntitle=\"Image Rotation\",\\ndescription=\"Upload an image and get a 45 degree clockwise rotated image\",\\n)\\ndemo.launch()\\nThis script defines a function rotate_image that takes an image as input, rotates it 45 degrees clockwise using the PIL\\nlibrary, and returns the rotated image and a text description. The gr.Interface function is then used to create a Gradio\\ninterface for this function.\\nThe gr.inputs.Image(type=\"numpy\") line specifies that the input to the function should be an image, and the\\ngr.outputs.Image(type=\"numpy\") line specifies that the output should be an image. The ”text” output type is\\nused to display the text description.\\nFinally, demo.launch() is used to launch the Gradio interface.\\nE.4. Developing Machine Learning Pipeline with PyTorch\\nPrompt: Build a simple neural network in Python using Pytorch to classify handwritten digits from the MNIST dataset.\\nYou should use CNN as the model structure, train the model for 5 epochs, draw a chart of the training loss, and show the\\nfinal result.\\nResponse: Here is a simple implementation',\n",
              "  'keywords': ['rotate_image', 'numpy', 'demo', 'mnist', '45']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' of a Convolutional Neural Network (CNN) using PyTorch to classify\\nhandwritten digits from the MNIST dataset.\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\n# Device configuration\\ndevice = torch.device(’cuda’ if torch.cuda.is_available() else ’cpu’)\\n# Hyper-parameters\\nnum_epochs = 5\\nbatch_size = 100\\nlearning_rate = 0.001\\n# MNIST dataset\\ntrain_dataset = torchvision.datasets.MNIST(root=’../../data’,\\ntrain=True,\\ntransform=transforms.ToTensor(),\\ndownload=True)\\ntest_dataset = torchvision.datasets.MNIST(root=’../../data’,\\ntrain=False,\\ntransform=transforms.ToTensor())\\n# Data loader\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\\nbatch_size=batch_size,\\n24\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nshuffle=True)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\\nbatch_size=batch_size,\\nshuffle=False)\\n# Convolution',\n",
              "  'keywords': ['batch_size',\n",
              "   'convolutional',\n",
              "   'train_dataset',\n",
              "   'magicoder',\n",
              "   'mnist']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': 'al neural network (two convolutional layers)\\nclass ConvNet(nn.Module):\\ndef __init__(self):\\nsuper(ConvNet, self).__init__()\\nself.layer1 = nn.Sequential(\\nnn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\\nnn.BatchNorm2d(16),\\nnn.ReLU(),\\nnn.MaxPool2d(kernel_size=2, stride=2))\\nself.layer2 = nn.Sequential(\\nnn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\\nnn.BatchNorm2d(32),\\nnn.ReLU(),\\nnn.MaxPool2d(kernel_size=2, stride=2))\\nself.fc = nn.Linear(7*7*32, 10)\\ndef forward(self, x):\\nout = self.layer1(x)\\nout = self.layer2(out)\\nout = out.reshape(out.size(0), -1)\\nout = self.fc(out)\\nreturn out\\nmodel = ConvNet().to(device)\\n# Loss and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\n# Train the model\\ntotal',\n",
              "  'keywords': ['layer2', 'optimizer', 'convnet', '__init__', 'learning_rate']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': '_step = len(train_loader)\\nloss_list = []\\nfor epoch in range(num_epochs):\\nfor i, (images, labels) in enumerate(train_loader):\\nimages = images.to(device)\\nlabels = labels.to(device)\\n# Forward pass\\noutputs = model(images)\\nloss = criterion(outputs, labels)\\n# Backward and optimize\\noptimizer.zero_grad()\\nloss.backward()\\noptimizer.step()\\nif (i+1) %\\nprint (’Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}’\\n.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\\nloss_list.append(loss.item())\\n# Test the model\\nmodel.eval() # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\\nwith torch.no_grad():\\ncorrect = 0\\ntotal = 0\\nfor images, labels in test_loader:\\nimages = images.to(device)\\nlabels = labels.to(device)\\noutputs = model(images)\\n_, predicted = torch.max(outputs.data, 1)\\ntotal += labels.size(0)\\ncorrect += (predicted == labels).sum().item()\\nprint(’Test Accuracy of',\n",
              "  'keywords': ['num_epochs', 'optimizer', 'test_loader', 'backward', 'model']},\n",
              " {'document': '/magicoder.txt.txt',\n",
              "  'title': 'Magicoder Source Code Is All You Need',\n",
              "  'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ',\n",
              "  'url': 'https://arxiv.org/pdf/2312.02120',\n",
              "  'doc': ' the model on the 10000 test images: {} %\\n# Draw a chart of the training loss\\nplt.plot(loss_list)\\nplt.title(’Training Loss’)\\nplt.xlabel(’Epoch’)\\nplt.ylabel(’Loss’)\\nplt.show()\\n25\\nMagicoder: Empowering Code Generation with OSS-INSTRUCT\\nThis script first loads the MNIST dataset and then defines a Convolutional Neural Network (CNN) with two convolutional\\nlayers. The CNN is trained for 5 epochs using the Adam optimizer and the Cross Entropy Loss function. After training,\\nthe model is tested on the test dataset and the accuracy is printed. Finally, a chart of the training loss is drawn.\\nF. Limitations\\nWhile Magicoders prove to be effective by being instruction-tuned with OSS-INSTRUCT and Evol-Instruct generated data,\\nwe apply our techniques to models with no more than 7B parameters due to resource constraints. This limitation hinders our\\nability to observe the method’s impact on larger models. Moreover, the OSS-INSTRUCT prompt is sophisticated and may\\nrequire a relatively strong model to comprehend the correct intent. Weaker models may fail to produce plausible instruction\\ndata. Future research may explore applying OSS-INSTRUCT to models at different capacities in the context of self-training.',\n",
              "  'keywords': ['magicoder', 'instruct', 'oss', 'mnist', 'loss_list']}]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.document import Document\n"
      ],
      "metadata": {
        "id": "3ytYmtPLZhps"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_docs = []\n",
        "\n",
        "for i in range(0,len(keys)):\n",
        "  final_docs.append(Document(page_content=keys[i]['doc'],metadata={\n",
        "      'source':keys[i]['document'],\n",
        "      'title':keys[i]['title'],\n",
        "      'author':keys[i]['author'],\n",
        "      'url':keys[i]['url'],\n",
        "      'keywords':keys[i]['keywords']\n",
        "  }))"
      ],
      "metadata": {
        "id": "NabtPKmLZ9hZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyP31amnaJ9j",
        "outputId": "91ab65b1-470a-4aa0-b2b0-a2a72e8b20af"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/magicoder.txt.txt', 'title': 'Magicoder Source Code Is All You Need', 'author': 'Yuxiang Wei ,Zhe Wang ,Jiawei Liu ,Yifeng Ding ,Lingming Zhang ', 'url': 'https://arxiv.org/pdf/2312.02120', 'keywords': ['magicoders', 'instruct', 'llms', 'synthetic', '75k']}, page_content='We introduce Magicoder, a series of fully open\\x02source (code, weights, and data) Large Language\\nModels (LLMs) for code that significantly closes\\nthe gap with top code models while having no\\nmore than 7B parameters. Magicoder models are\\ntrained on 75K synthetic instruction data using\\nOSS-INSTRUCT, a novel approach to enlighten\\x02ing LLMs with open-source code snippets to gen\\x02erate diverse instruction data for code. Our main\\nmotivation is to mitigate the inherent bias of the\\nsynthetic data generated by LLMs through the\\nwealth of open-source references for the produc\\x02tion of more realistic and controllable data. The\\northogonality of OSS-INSTRUCT and other data\\ngeneration methods like Evol-Instruct further en\\x02ables us to build an enhanced MagicoderS. Both\\nMagicoder and MagicoderS substantially outper\\x02form state-of-the-art code models with similar or\\neven larger sizes on a wide range of coding bench\\x02marks. Notably, MagicoderS-CL-7B based on\\nCODELLAMA even surpasses the prominent Chat\\x02GPT on HumanEval+ (66.5 vs. 65.9 in pass@1).\\nOverall, OSS-INSTRUCT opens a new direction\\nfor crafting diverse synthetic instruction data for\\ncode using abundant open-source references.\\n1. Introduction\\nCode generation, also known as program synthesis (Gul\\x02wani et al., 2017), is a long-standing challenge')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}